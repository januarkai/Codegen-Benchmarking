{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/meter/FocusMeter.java:FocusMeter:0", "source": "\n\n    private static final String TAG = CLASSTOKEN.class.getSimpleName();\n    private static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    public CLASSTOKEN(@NonNull List<MeteringRectangle> areas, boolean skipIfPossible) {\n        super(areas, skipIfPossible);\n    }\n\n    @Override\n    protected boolean checkIsSupported(@NonNull ActionHolder holder) {\n        // Exclude OFF and EDOF as per docs. These do no support the trigger.\n        Integer afMode = holder.getBuilder(this).get(CaptureRequest.CONTROL_AF_MODE);\n        boolean result = afMode != null &&\n                (afMode == CameraCharacteristics.CONTROL_AF_MODE_AUTO\n                        || afMode == CameraCharacteristics.CONTROL_AF_MODE_CONTINUOUS_PICTURE\n                        || afMode == CameraCharacteristics.CONTROL_AF_MODE_CONTINUOUS_VIDEO\n                        || afMode == CameraCharacteristics.CONTROL_AF_MODE_MACRO);\n        LOG.i(\"checkIsSupported:\", result);\n        return result;\n    }\n\n    @Override\n    protected boolean checkShouldSkip(@NonNull ActionHolder holder) {\n        CaptureResult lastResult = holder.getLastResult(this);\n        if (lastResult != null) {\n            Integer afState = lastResult.get(CaptureResult.CONTROL_AF_STATE);\n            boolean result = afState != null &&\n                    (afState == CaptureResult.CONTROL_AF_STATE_FOCUSED_LOCKED ||\n                            afState == CaptureResult.CONTROL_AF_STATE_PASSIVE_FOCUSED);\n            LOG.i(\"checkShouldSkip:\", result);\n            return result;\n        } else {\n            LOG.i(\"checkShouldSkip: false - lastResult is null.\");\n            return false;\n        }\n    }\n\n    @Override\n    protected void onStarted(@NonNull ActionHolder holder, @NonNull List<MeteringRectangle> areas) {\n        LOG.i(\"onStarted:\", \"with areas:\", areas);\n        holder.getBuilder(this).set(CaptureRequest.CONTROL_AF_TRIGGER,\n                CaptureRequest.CONTROL_AF_TRIGGER_START);\n        int maxRegions = readCharacteristic(CameraCharacteristics.CONTROL_MAX_REGIONS_AF,\n                0);\n        if (!areas.isEmpty() && maxRegions > 0) {\n            int max = Math.min(maxRegions, areas.size());\n            holder.getBuilder(this).set(CaptureRequest.CONTROL_AF_REGIONS,\n                    areas.subList(0, max).toArray(new MeteringRectangle[]{}));\n        }\n        holder.applyBuilder(this);\n    }\n\n    @Override\n    protected void onCompleted(@NonNull ActionHolder holder) {\n        super.onCompleted(holder);\n        // Remove (but not apply) the risky parameter so it is not included in new requests.\n        // Documentation about this key says that this should be allowed.\n        holder.getBuilder(this).set(CaptureRequest.CONTROL_AF_TRIGGER, null);\n    }\n\n    @Override\n    public void onCaptureCompleted(@NonNull ActionHolder holder, @NonNull CaptureRequest request,\n                                   @NonNull TotalCaptureResult result) {\n        super.onCaptureCompleted(holder, request, result);\n        Integer afState = result.get(CaptureResult.CONTROL_AF_STATE);\n        LOG.i(\"onCaptureCompleted:\", \"afState:\", afState);\n        if (afState == null) return;\n        switch (afState) {\n            case CaptureRequest.CONTROL_AF_STATE_FOCUSED_LOCKED: {\n                setSuccessful(true);\n                setState(STATE_COMPLETED);\n                break;\n            }\n            case CaptureRequest.CONTROL_AF_STATE_NOT_FOCUSED_LOCKED: {\n                setSuccessful(false);\n                setState(STATE_COMPLETED);\n                break;\n            }\n            case CaptureRequest.CONTROL_AF_STATE_INACTIVE: break;\n            case CaptureRequest.CONTROL_AF_STATE_ACTIVE_SCAN: break;\n            default: break;\n        }\n    }\n", "target": "focus meter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/FullVideoRecorder.java:FullVideoRecorder:0", "source": "\n\n    private static final String TAG = CLASSTOKEN.class.getSimpleName();\n    protected static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    @SuppressWarnings(\"WeakerAccess\") protected MediaRecorder mMediaRecorder;\n    private CamcorderProfile mProfile;\n    private boolean mMediaRecorderPrepared;\n\n\n    CLASSTOKEN(@Nullable VideoResultListener listener) {\n        super(listener);\n    }\n\n    /**\n     * Subclasses should return an appropriate CamcorderProfile.\n     * This could be taken from the {@link CamcorderProfiles} utility class based on the\n     * stub declared size, for instance.\n     *\n     * @param stub the stub\n     * @return the profile\n     */\n    @NonNull\n    protected abstract CamcorderProfile getCamcorderProfile(@NonNull VideoResult.Stub stub);\n\n    /**\n     * Subclasses should apply a video source to the given recorder.\n     *\n     * @param stub the stub\n     * @param mediaRecorder the recorder\n     */\n    protected abstract void applyVideoSource(@NonNull VideoResult.Stub stub,\n                                             @NonNull MediaRecorder mediaRecorder);\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected final boolean prepareMediaRecorder(@NonNull VideoResult.Stub stub) {\n        if (mMediaRecorderPrepared) return true;\n        // We kind of trust the stub size at this point. It's coming from CameraOptions sizes\n        // and it's clipped to be less than CamcorderProfile's highest available profile.\n        // However, we still can't trust the developer parameters (e.g. bit rates), and even\n        // without them, the camera declared sizes can cause crashes in MediaRecorder (#467, #602).\n        // A possible solution was to prepare without checking DeviceEncoders first, and should it\n        // fail, prepare again checking them. However, when parameters are wrong, MediaRecorder\n        // fails on start() instead of prepare() (start failed -19), so this wouldn't be effective.\n        return prepareMediaRecorder(stub, true);\n    }\n\n    @SuppressWarnings(\"SameParameterValue\")\n    private boolean prepareMediaRecorder(@NonNull VideoResult.Stub stub,\n                                         boolean applyEncodersConstraints) {\n        LOG.i(\"prepareMediaRecorder:\", \"Preparing on thread\", Thread.currentThread());\n        // 1. Create reference and ask for the CamcorderProfile\n        mMediaRecorder = new MediaRecorder();\n        mProfile = getCamcorderProfile(stub);\n\n        // 2. Set the video and audio sources.\n        applyVideoSource(stub, mMediaRecorder);\n        int audioChannels = 0;\n        if (stub.audio == Audio.ON) {\n            audioChannels = mProfile.audioChannels;\n        } else if (stub.audio == Audio.MONO) {\n            audioChannels = 1;\n        } else if (stub.audio == Audio.STEREO) {\n            audioChannels = 2;\n        }\n        boolean hasAudio = audioChannels > 0;\n        if (hasAudio) {\n            mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.DEFAULT);\n        }\n\n        // 3. Set the output format. Before, change the profile data if the user\n        // has specified a specific codec.\n        if (stub.videoCodec == VideoCodec.H_264) {\n            mProfile.videoCodec = MediaRecorder.VideoEncoder.H264;\n            mProfile.fileFormat = MediaRecorder.OutputFormat.MPEG_4;\n        } else if (stub.videoCodec == VideoCodec.H_263) {\n            mProfile.videoCodec = MediaRecorder.VideoEncoder.H263;\n            mProfile.fileFormat = MediaRecorder.OutputFormat.MPEG_4; // should work\n        }\n        // Set audio codec if the user has specified a specific codec.\n        if (stub.audioCodec == AudioCodec.AAC) {\n            mProfile.audioCodec = MediaRecorder.AudioEncoder.AAC;\n        } else if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.JELLY_BEAN\n                && stub.audioCodec == AudioCodec.HE_AAC) {\n            mProfile.audioCodec = MediaRecorder.AudioEncoder.HE_AAC;\n        } else if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.JELLY_BEAN\n                && stub.audioCodec == AudioCodec.AAC_ELD) {\n            mProfile.audioCodec = MediaRecorder.AudioEncoder.AAC_ELD;\n        }\n        mMediaRecorder.setOutputFormat(mProfile.fileFormat);\n\n        // 4. Update the VideoResult stub with information from the profile, if the\n        // stub values are absent or incomplete\n        if (stub.videoFrameRate <= 0) stub.videoFrameRate = mProfile.videoFrameRate;\n        if (stub.videoBitRate <= 0) stub.videoBitRate = mProfile.videoBitRate;\n        if (stub.audioBitRate <= 0 && hasAudio) stub.audioBitRate = mProfile.audioBitRate;\n\n        // 5. Update the VideoResult stub with DeviceEncoders constraints\n        if (applyEncodersConstraints) {\n            // A. Get the audio mime type\n            // https://android.googlesource.com/platform/frameworks/av/+/master/media/libmediaplayerservice/StagefrightRecorder.cpp#1096\n            // https://github.com/MrAlex94/Waterfox-Old/blob/master/media/libstagefright/frameworks/av/media/libstagefright/MediaDefs.cpp\n            String audioType;\n            switch (mProfile.audioCodec) {\n                case MediaRecorder.AudioEncoder.AMR_NB: audioType = \"audio/3gpp\"; break;\n                case MediaRecorder.AudioEncoder.AMR_WB: audioType = \"audio/amr-wb\"; break;\n                case MediaRecorder.AudioEncoder.AAC:\n                case MediaRecorder.AudioEncoder.HE_AAC:\n                case MediaRecorder.AudioEncoder.AAC_ELD: audioType = \"audio/mp4a-latm\"; break;\n                case MediaRecorder.AudioEncoder.VORBIS: audioType = \"audio/vorbis\"; break;\n                case MediaRecorder.AudioEncoder.DEFAULT:\n                default: audioType = \"audio/3gpp\";\n            }\n            // B. Get the video mime type\n            // https://android.googlesource.com/platform/frameworks/av/+/master/media/libmediaplayerservice/StagefrightRecorder.cpp#1650\n            // https://github.com/MrAlex94/Waterfox-Old/blob/master/media/libstagefright/frameworks/av/media/libstagefright/MediaDefs.cpp\n            String videoType;\n            switch (mProfile.videoCodec) {\n                case MediaRecorder.VideoEncoder.H263: videoType = \"video/3gpp\"; break;\n                case MediaRecorder.VideoEncoder.H264: videoType = \"video/avc\"; break;\n                case MediaRecorder.VideoEncoder.MPEG_4_SP: videoType = \"video/mp4v-es\"; break;\n                case MediaRecorder.VideoEncoder.VP8: videoType = \"video/x-vnd.on2.vp8\"; break;\n                case MediaRecorder.VideoEncoder.HEVC: videoType = \"video/hevc\"; break;\n                case MediaRecorder.VideoEncoder.DEFAULT:\n                default: videoType = \"video/avc\";\n            }\n            // C. Check DeviceEncoders support\n            boolean flip = stub.rotation % 180 != 0;\n            if (flip) stub.size = stub.size.flip();\n            Size newVideoSize = null;\n            int newVideoBitRate = 0;\n            int newAudioBitRate = 0;\n            int newVideoFrameRate = 0;\n            int videoEncoderOffset = 0;\n            int audioEncoderOffset = 0;\n            boolean encodersFound = false;\n            while (!encodersFound) {\n                LOG.i(\"prepareMediaRecorder:\", \"Checking DeviceEncoders...\",\n                        \"videoOffset:\", videoEncoderOffset,\n                        \"audioOffset:\", audioEncoderOffset);\n                DeviceEncoders encoders;\n                try {\n                    encoders = new DeviceEncoders(DeviceEncoders.MODE_RESPECT_ORDER,\n                            videoType, audioType, videoEncoderOffset, audioEncoderOffset);\n                } catch (RuntimeException e) {\n                    LOG.w(\"prepareMediaRecorder:\", \"Could not respect encoders parameters.\",\n                            \"Trying again without checking encoders.\");\n                    return prepareMediaRecorder(stub, false);\n                }\n                try {\n                    newVideoSize = encoders.getSupportedVideoSize(stub.size);\n                    newVideoBitRate = encoders.getSupportedVideoBitRate(stub.videoBitRate);\n                    newVideoFrameRate = encoders.getSupportedVideoFrameRate(newVideoSize,\n                            stub.videoFrameRate);\n                    encoders.tryConfigureVideo(videoType, newVideoSize, newVideoFrameRate,\n                            newVideoBitRate);\n                    if (hasAudio) {\n                        newAudioBitRate = encoders.getSupportedAudioBitRate(stub.audioBitRate);\n                        encoders.tryConfigureAudio(audioType, newAudioBitRate,\n                                mProfile.audioSampleRate, audioChannels);\n                    }\n                    encodersFound = true;\n                } catch (DeviceEncoders.VideoException videoException) {\n                    LOG.i(\"prepareMediaRecorder:\", \"Got VideoException:\",\n                            videoException.getMessage());\n                    videoEncoderOffset++;\n                } catch (DeviceEncoders.AudioException audioException) {\n                    LOG.i(\"prepareMediaRecorder:\", \"Got AudioException:\",\n                            audioException.getMessage());\n                    audioEncoderOffset++;\n                }\n            }\n            // D. Apply results\n            stub.size = newVideoSize;\n            stub.videoBitRate = newVideoBitRate;\n            stub.audioBitRate = newAudioBitRate;\n            stub.videoFrameRate = newVideoFrameRate;\n            if (flip) stub.size = stub.size.flip();\n        }\n\n        // 6A. Configure MediaRecorder from stub and from profile (video)\n        boolean flip = stub.rotation % 180 != 0;\n        mMediaRecorder.setVideoSize(\n                flip ? stub.size.getHeight() : stub.size.getWidth(),\n                flip ? stub.size.getWidth() : stub.size.getHeight());\n        mMediaRecorder.setVideoFrameRate(stub.videoFrameRate);\n        mMediaRecorder.setVideoEncoder(mProfile.videoCodec);\n        mMediaRecorder.setVideoEncodingBitRate(stub.videoBitRate);\n\n        // 6B. Configure MediaRecorder from stub and from profile (audio)\n        if (hasAudio) {\n            mMediaRecorder.setAudioChannels(audioChannels);\n            mMediaRecorder.setAudioSamplingRate(mProfile.audioSampleRate);\n            mMediaRecorder.setAudioEncoder(mProfile.audioCodec);\n            mMediaRecorder.setAudioEncodingBitRate(stub.audioBitRate);\n        }\n\n        // 7. Set other params\n        if (stub.location != null) {\n            mMediaRecorder.setLocation(\n                    (float) stub.location.getLatitude(),\n                    (float) stub.location.getLongitude());\n        }\n\n        if (stub.file != null) {\n            mMediaRecorder.setOutputFile(stub.file.getAbsolutePath());\n        } else if (stub.fileDescriptor != null) {\n            mMediaRecorder.setOutputFile(stub.fileDescriptor);\n        } else {\n            throw new IllegalStateException(\"file and fileDescriptor are both null.\");\n        }\n\n        mMediaRecorder.setOrientationHint(stub.rotation);\n        // When using MEDIA_RECORDER_INFO_MAX_FILESIZE_REACHED, the recorder might have stopped\n        // before calling it. But this creates issues on Camera2 Legacy devices - they need a\n        // callback BEFORE the recorder stops (see Camera2Engine). For this reason, we increase\n        // the max size and use MEDIA_RECORDER_INFO_MAX_FILESIZE_APPROACHING instead.\n        // Would do this with max duration as well but there's no such callback.\n        mMediaRecorder.setMaxFileSize(stub.maxSize <= 0 ? stub.maxSize\n                : Math.round(stub.maxSize / 0.9D));\n        LOG.i(\"prepareMediaRecorder:\", \"Increased max size from\", stub.maxSize, \"to\",\n                Math.round(stub.maxSize / 0.9D));\n        mMediaRecorder.setMaxDuration(stub.maxDuration);\n        mMediaRecorder.setOnInfoListener(new MediaRecorder.OnInfoListener() {\n            @Override\n            public void onInfo(MediaRecorder mediaRecorder, int what, int extra) {\n                LOG.i(\"OnInfoListener:\", \"Received info\", what, extra,\n                        \"Thread: \", Thread.currentThread());\n                boolean shouldStop = false;\n                switch (what) {\n                    case MediaRecorder.MEDIA_RECORDER_INFO_MAX_DURATION_REACHED:\n                        mResult.endReason = VideoResult.REASON_MAX_DURATION_REACHED;\n                        shouldStop = true;\n                        break;\n                    case MediaRecorder.MEDIA_RECORDER_INFO_MAX_FILESIZE_APPROACHING:\n                    case MediaRecorder.MEDIA_RECORDER_INFO_MAX_FILESIZE_REACHED:\n                        // On rare occasions APPROACHING is not called. Make sure we listen to\n                        // REACHED as well.\n                        mResult.endReason = VideoResult.REASON_MAX_SIZE_REACHED;\n                        shouldStop = true;\n                        break;\n                }\n                if (shouldStop) {\n                    LOG.i(\"OnInfoListener:\", \"Stopping\");\n                    stop(false);\n                }\n            }\n        });\n        mMediaRecorder.setOnErrorListener(new MediaRecorder.OnErrorListener() {\n            @Override\n            public void onError(MediaRecorder mr, int what, int extra) {\n                LOG.e(\"OnErrorListener: got error\", what, extra, \". Stopping.\");\n                mResult = null;\n                mError = new RuntimeException(\"MediaRecorder error: \" + what + \" \" + extra);\n                LOG.i(\"OnErrorListener:\", \"Stopping\");\n                stop(false);\n            }\n        });\n\n        // 8. Prepare the Recorder\n        try {\n            mMediaRecorder.prepare();\n            mMediaRecorderPrepared = true;\n            mError = null;\n            return true;\n        } catch (Exception e) {\n            LOG.w(\"prepareMediaRecorder:\", \"Error while preparing media recorder.\", e);\n            mMediaRecorderPrepared = false;\n            mError = e;\n            return false;\n        }\n    }\n\n    @Override\n    protected void onStart() {\n        if (!prepareMediaRecorder(mResult)) {\n            mResult = null;\n            stop(false);\n            return;\n        }\n\n        try {\n            mMediaRecorder.start();\n            dispatchVideoRecordingStart();\n        } catch (Exception e) {\n            LOG.w(\"start:\", \"Error while starting media recorder.\", e);\n            mResult = null;\n            mError = e;\n            stop(false);\n        }\n    }\n\n    @Override\n    protected void onStop(boolean isCameraShutdown) {\n        if (mMediaRecorder != null) {\n            dispatchVideoRecordingEnd();\n            try {\n                LOG.i(\"stop:\", \"Stopping MediaRecorder...\");\n                // TODO HANGS (rare, emulator only)\n                mMediaRecorder.stop();\n                LOG.i(\"stop:\", \"Stopped MediaRecorder.\");\n            } catch (Exception e) {\n                // This can happen if stopVideo() is called right after takeVideo()\n                // (in which case we don't care). Or when prepare()/start() have failed for\n                // some reason and we are not allowed to call stop.\n                // Make sure we don't override the error if one exists already.\n                mResult = null;\n                if (mError == null) {\n                    LOG.w(\"stop:\", \"Error while closing media recorder.\", e);\n                    mError = e;\n                }\n            }\n            try {\n                LOG.i(\"stop:\", \"Releasing MediaRecorder...\");\n                mMediaRecorder.release();\n                LOG.i(\"stop:\", \"Released MediaRecorder.\");\n            } catch (Exception e) {\n                mResult = null;\n                if (mError == null) {\n                    LOG.w(\"stop:\", \"Error while releasing media recorder.\", e);\n                    mError = e;\n                }\n            }\n        }\n        mProfile = null;\n        mMediaRecorder = null;\n        mMediaRecorderPrepared = false;\n        dispatchResult();\n    }\n\n", "target": "full video recorder"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filter/MultiFilter.java:State:1", "source": "\n        @VisibleForTesting boolean isProgramCreated = false;\n        @VisibleForTesting boolean isFramebufferCreated = false;\n        private boolean sizeChanged = false;\n        @VisibleForTesting Size size = null;\n        private int programHandle = -1;\n        private GlFramebuffer outputFramebuffer = null;\n        private GlTexture outputTexture = null;\n    ", "target": "state"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/internal/FpsRangeValidator.java:FpsRangeValidator:0", "source": "\n\n    private final static CameraLogger LOG = CameraLogger.create(\"CLASSTOKEN\");\n    private final static Map<String, List<Range<Integer>>> sIssues = new HashMap<>();\n\n    static {\n        sIssues.put(\"Google Pixel 4\", Arrays.asList(new Range<>(15, 60)));\n        sIssues.put(\"Google Pixel 4a\", Arrays.asList(new Range<>(15, 60)));\n        sIssues.put(\"Google Pixel 4 XL\", Arrays.asList(new Range<>(15, 60)));\n    }\n\n    public static boolean validate(Range<Integer> range) {\n        LOG.i(\"Build.MODEL:\", Build.MODEL, \"Build.BRAND:\", Build.BRAND, \"Build.MANUFACTURER:\", Build.MANUFACTURER);\n        String descriptor = Build.MANUFACTURER + \" \" + Build.MODEL;\n        List<Range<Integer>> ranges = sIssues.get(descriptor);\n        if (ranges != null && ranges.contains(range)) {\n            LOG.i(\"Dropping range:\", range);\n            return false;\n        }\n        return true;\n    }\n", "target": "fps range validator"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filters/HueFilter.java:HueFilter:0", "source": "\n\n    private final static String FRAGMENT_SHADER = \"#extension GL_OES_EGL_image_external : require\\n\"\n            + \"precision mediump float;\\n\"\n            + \"varying vec2 \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\";\\n\"\n            + \"uniform samplerExternalOES sTexture;\\n\"\n            + \"uniform float hue;\\n\"\n            + \"void main() {\\n\"\n            + \"  vec4 kRGBToYPrime = vec4 (0.299, 0.587, 0.114, 0.0);\\n\"\n            + \"  vec4 kRGBToI = vec4 (0.595716, -0.274453, -0.321263, 0.0);\\n\"\n            + \"  vec4 kRGBToQ = vec4 (0.211456, -0.522591, 0.31135, 0.0);\\n\"\n            + \"  vec4 kYIQToR = vec4 (1.0, 0.9563, 0.6210, 0.0);\\n\"\n            + \"  vec4 kYIQToG = vec4 (1.0, -0.2721, -0.6474, 0.0);\\n\"\n            + \"  vec4 kYIQToB = vec4 (1.0, -1.1070, 1.7046, 0.0);\\n\"\n            + \"  vec4 color = texture2D(sTexture, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\");\\n\"\n            + \"  float YPrime = dot(color, kRGBToYPrime);\\n\"\n            + \"  float I = dot(color, kRGBToI);\\n\"\n            + \"  float Q = dot(color, kRGBToQ);\\n\"\n            + \"  float chroma = sqrt (I * I + Q * Q);\\n\"\n            + \"  Q = chroma * sin (hue);\\n\"\n            + \"  I = chroma * cos (hue);\\n\"\n            + \"  vec4 yIQ = vec4 (YPrime, I, Q, 0.0);\\n\"\n            + \"  color.r = dot (yIQ, kYIQToR);\\n\"\n            + \"  color.g = dot (yIQ, kYIQToG);\\n\"\n            + \"  color.b = dot (yIQ, kYIQToB);\\n\"\n            + \"  gl_FragColor = color;\\n\"\n            + \"}\\n\";\n\n    private float hue = 0.0f;\n    private int hueLocation = -1;\n\n    public CLASSTOKEN() { }\n\n    /**\n     * Sets the hue value in degrees. See the values chart:\n     * https://cloud.githubusercontent.com/assets/2201511/21810115/b99ac22a-d74a-11e6-9f6c-ef74d15c88c7.jpg\n     *\n     * @param hue hue degrees\n     */\n    @SuppressWarnings({\"unused\", \"WeakerAccess\"})\n    public void setHue(float hue) {\n        this.hue = hue % 360;\n    }\n\n    /**\n     * Returns the current hue value.\n     *\n     * @see #setHue(float)\n     * @return hue\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public float getHue() {\n        return hue;\n    }\n\n    @Override\n    public void setParameter1(float value) {\n        setHue(value * 360F);\n    }\n\n    @Override\n    public float getParameter1() {\n        return getHue() / 360F;\n    }\n\n    @NonNull\n    @Override\n    public String getFragmentShader() {\n        return FRAGMENT_SHADER;\n    }\n\n    @Override\n    public void onCreate(int programHandle) {\n        super.onCreate(programHandle);\n        hueLocation = GLES20.glGetUniformLocation(programHandle, \"hue\");\n        Egloo.checkGlProgramLocation(hueLocation, \"hue\");\n    }\n\n    @Override\n    public void onDestroy() {\n        super.onDestroy();\n        hueLocation = -1;\n    }\n\n    @Override\n    protected void onPreDraw(long timestampUs, @NonNull float[] transformMatrix) {\n        super.onPreDraw(timestampUs, transformMatrix);\n        // map it on 360 degree circle\n        float shaderHue = ((hue - 45) / 45f + 0.5f) * -1;\n        GLES20.glUniform1f(hueLocation, shaderHue);\n        Egloo.checkGlError(\"glUniform1f\");\n    }\n", "target": "hue filter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/encoding/AudioNoise.java:AudioNoise:0", "source": "\n\n    private final static int FRAMES = 1; // After testing, it looks like this is the best setup\n    private final static Random RANDOM = new Random();\n\n    private final ByteBuffer mNoiseBuffer;\n\n    CLASSTOKEN(@NonNull AudioConfig config) {\n        //noinspection ConstantConditions\n        if (config.sampleSizePerChannel != 2) {\n            throw new IllegalArgumentException(\"CLASSTOKEN expects 2bytes-1short samples.\");\n        }\n        mNoiseBuffer = ByteBuffer\n                .allocateDirect(config.frameSize() * FRAMES)\n                .order(ByteOrder.nativeOrder());\n        double i = 0;\n        double frequency = config.frameSize() / 2D; // each X samples, the signal repeats\n        double step = Math.PI / frequency; // the increase in radians\n        double max = 10; // might choose this from 0 to Short.MAX_VALUE\n        while (mNoiseBuffer.hasRemaining()) {\n            short noise = (short) (Math.sin(++i * step) * max);\n            mNoiseBuffer.put((byte) noise);\n            mNoiseBuffer.put((byte) (noise >> 8));\n        }\n        mNoiseBuffer.rewind();\n    }\n\n    void fill(@NonNull ByteBuffer outBuffer) {\n        mNoiseBuffer.clear();\n        if (mNoiseBuffer.capacity() == outBuffer.remaining()) {\n            mNoiseBuffer.position(0); // Happens if FRAMES = 1.\n        } else {\n            mNoiseBuffer.position(RANDOM.nextInt(mNoiseBuffer.capacity()\n                    - outBuffer.remaining()));\n        }\n        mNoiseBuffer.limit(mNoiseBuffer.position() + outBuffer.remaining());\n        outBuffer.put(mNoiseBuffer);\n    }\n", "target": "audio noise"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/picture/SnapshotGlPictureRecorder.java:SnapshotGlPictureRecorder:0", "source": "\n\n    private RendererCameraPreview mPreview;\n    private AspectRatio mOutputRatio;\n\n    private Overlay mOverlay;\n    private boolean mHasOverlay;\n    private OverlayDrawer mOverlayDrawer;\n    private GlTextureDrawer mTextureDrawer;\n\n    public CLASSTOKEN(\n            @NonNull PictureResult.Stub stub,\n            @Nullable PictureResultListener listener,\n            @NonNull RendererCameraPreview preview,\n            @NonNull AspectRatio outputRatio,\n            @Nullable Overlay overlay) {\n        super(stub, listener);\n        mPreview = preview;\n        mOutputRatio = outputRatio;\n        mOverlay = overlay;\n        mHasOverlay = mOverlay != null && mOverlay.drawsOn(Overlay.Target.PICTURE_SNAPSHOT);\n    }\n\n    @TargetApi(Build.VERSION_CODES.KITKAT)\n    @Override\n    public void take() {\n        mPreview.addRendererFrameCallback(new RendererFrameCallback() {\n\n            @RendererThread\n            public void onRendererTextureCreated(int textureId) {\n                CLASSTOKEN.this.onRendererTextureCreated(textureId);\n            }\n\n            @RendererThread\n            @Override\n            public void onRendererFilterChanged(@NonNull Filter filter) {\n                CLASSTOKEN.this.onRendererFilterChanged(filter);\n            }\n\n            @RendererThread\n            @Override\n            public void onRendererFrame(@NonNull SurfaceTexture surfaceTexture,\n                                        int rotation, float scaleX, float scaleY) {\n                mPreview.removeRendererFrameCallback(this);\n                CLASSTOKEN.this.onRendererFrame(surfaceTexture,\n                        rotation, scaleX, scaleY);\n            }\n\n        });\n    }\n\n    @SuppressWarnings(\"WeakerAccess\")\n    @RendererThread\n    @TargetApi(Build.VERSION_CODES.KITKAT)\n    protected void onRendererTextureCreated(int textureId) {\n        mTextureDrawer = new GlTextureDrawer(textureId);\n        // Need to crop the size.\n        Rect crop = CropHelper.computeCrop(mResult.size, mOutputRatio);\n        mResult.size = new Size(crop.width(), crop.height());\n        if (mHasOverlay) {\n            mOverlayDrawer = new OverlayDrawer(mOverlay, mResult.size);\n        }\n    }\n\n    @SuppressWarnings(\"WeakerAccess\")\n    @RendererThread\n    @TargetApi(Build.VERSION_CODES.KITKAT)\n    protected void onRendererFilterChanged(@NonNull Filter filter) {\n        mTextureDrawer.setFilter(filter.copy());\n    }\n\n    @SuppressWarnings(\"WeakerAccess\")\n    @RendererThread\n    @TargetApi(Build.VERSION_CODES.KITKAT)\n    protected void onRendererFrame(@SuppressWarnings(\"unused\") @NonNull final SurfaceTexture surfaceTexture,\n                                 final int rotation,\n                                 final float scaleX,\n                                 final float scaleY) {\n        // Get egl context from the RendererThread, which is the one in which we have created\n        // the textureId and the overlayTextureId, managed by the GlSurfaceView.\n        // Next operations can then be performed on different threads using this handle.\n        final EGLContext eglContext = EGL14.eglGetCurrentContext();\n        WorkerHandler.execute(new Runnable() {\n            @Override\n            public void run() {\n                takeFrame(surfaceTexture, rotation, scaleX, scaleY, eglContext);\n\n            }\n        });\n    }\n\n    /**\n     * The tricky part here is the EGL surface creation.\n     *\n     * We don't have a real output window for the EGL surface - we will use glReadPixels()\n     * and never call swapBuffers(), so what we draw is never published.\n     *\n     * 1. One option is to use a pbuffer EGL surface. This works, we just have to pass\n     *    the correct width and height. However, it is significantly slower than the current\n     *    solution.\n     *\n     * 2. Another option is to create the EGL surface out of a ImageReader.getSurface()\n     *    and use the reader to create a JPEG. In this case, we would have to publish\n     *    the frame with swapBuffers(). However, currently ImageReader does not support\n     *    all formats, it's risky. This is an example error that we get:\n     *    \"RGBA override BLOB format buffer should have height == width\"\n     *\n     * The third option, which we are using, is to create the EGL surface using whatever\n     * {@link Surface} or {@link SurfaceTexture} we have at hand. Since we never call\n     * swapBuffers(), the frame will not actually be rendered. This is the fastest.\n     *\n     * @param scaleX frame scale x in {@link Reference#VIEW}\n     * @param scaleY frame scale y in {@link Reference#VIEW}\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    @WorkerThread\n    @TargetApi(Build.VERSION_CODES.KITKAT)\n    protected void takeFrame(@NonNull SurfaceTexture surfaceTexture,\n                             int rotation,\n                             float scaleX,\n                             float scaleY,\n                             @NonNull EGLContext eglContext) {\n\n        // 0. EGL window will need an output.\n        // We create a fake one as explained in javadocs.\n        final int fakeOutputTextureId = 9999;\n        SurfaceTexture fakeOutputSurface = new SurfaceTexture(fakeOutputTextureId);\n        fakeOutputSurface.setDefaultBufferSize(mResult.size.getWidth(), mResult.size.getHeight());\n\n        // 1. Create an EGL surface\n        final EglCore core = new EglCore(eglContext, EglCore.FLAG_RECORDABLE);\n        final EglSurface eglSurface = new EglWindowSurface(core, fakeOutputSurface);\n        eglSurface.makeCurrent();\n        final float[] transform = mTextureDrawer.getTextureTransform();\n\n        // 2. Apply preview transformations\n        surfaceTexture.getTransformMatrix(transform);\n        float scaleTranslX = (1F - scaleX) / 2F;\n        float scaleTranslY = (1F - scaleY) / 2F;\n        Matrix.translateM(transform, 0, scaleTranslX, scaleTranslY, 0);\n        Matrix.scaleM(transform, 0, scaleX, scaleY, 1);\n\n        // 3. Apply rotation and flip\n        // If this doesn't work, rotate \"rotation\" before scaling, like GlCameraPreview does.\n        Matrix.translateM(transform, 0, 0.5F, 0.5F, 0); // Go back to 0,0\n        Matrix.rotateM(transform, 0, rotation + mResult.rotation, 0, 0, 1); // Rotate to OUTPUT\n        Matrix.scaleM(transform, 0, 1, -1, 1); // Vertical flip because we'll use glReadPixels\n        Matrix.translateM(transform, 0, -0.5F, -0.5F, 0); // Go back to old position\n\n        // 4. Do pretty much the same for overlays\n        if (mHasOverlay) {\n            // 1. First we must draw on the texture and get latest image\n            mOverlayDrawer.draw(Overlay.Target.PICTURE_SNAPSHOT);\n\n            // 2. Then we can apply the transformations\n            Matrix.translateM(mOverlayDrawer.getTransform(), 0, 0.5F, 0.5F, 0);\n            Matrix.rotateM(mOverlayDrawer.getTransform(), 0, mResult.rotation, 0, 0, 1);\n            Matrix.scaleM(mOverlayDrawer.getTransform(), 0, 1, -1, 1); // Vertical flip because we'll use glReadPixels\n            Matrix.translateM(mOverlayDrawer.getTransform(), 0, -0.5F, -0.5F, 0);\n        }\n        mResult.rotation = 0;\n\n        // 5. Draw and save\n        long timestampUs = surfaceTexture.getTimestamp() / 1000L;\n        LOG.i(\"takeFrame:\", \"timestampUs:\", timestampUs);\n        mTextureDrawer.draw(timestampUs);\n        if (mHasOverlay) mOverlayDrawer.render(timestampUs);\n        mResult.data = eglSurface.toByteArray(Bitmap.CompressFormat.JPEG);\n\n        // 6. Cleanup\n        eglSurface.release();\n        mTextureDrawer.release();\n        fakeOutputSurface.release();\n        if (mHasOverlay) mOverlayDrawer.release();\n        core.release();\n        dispatchResult();\n    }\n\n    @Override\n    protected void dispatchResult() {\n        mOutputRatio = null;\n        super.dispatchResult();\n    }\n", "target": "snapshot gl picture recorder"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/lock/FocusLock.java:FocusLock:0", "source": "\n\n    private final static String TAG = CLASSTOKEN.class.getSimpleName();\n    private final static CameraLogger LOG = CameraLogger.create(TAG);\n\n    @Override\n    protected boolean checkIsSupported(@NonNull ActionHolder holder) {\n        // We'll lock by changing the AF mode to AUTO.\n        // In that mode, AF won't change unless someone starts a trigger operation.\n        int[] modes = readCharacteristic(CameraCharacteristics.CONTROL_AF_AVAILABLE_MODES,\n                new int[]{});\n        for (int mode : modes) {\n            if (mode == CameraCharacteristics.CONTROL_AF_MODE_AUTO) {\n                return true;\n            }\n        }\n        return false;\n    }\n\n    @Override\n    protected boolean checkShouldSkip(@NonNull ActionHolder holder) {\n        CaptureResult lastResult = holder.getLastResult(this);\n        if (lastResult != null) {\n            Integer afState = lastResult.get(CaptureResult.CONTROL_AF_STATE);\n            boolean afStateOk = afState != null &&\n                    (afState == CaptureResult.CONTROL_AF_STATE_FOCUSED_LOCKED\n                            || afState == CaptureResult.CONTROL_AF_STATE_NOT_FOCUSED_LOCKED\n                            || afState == CaptureResult.CONTROL_AF_STATE_INACTIVE\n                            || afState == CaptureResult.CONTROL_AF_STATE_PASSIVE_FOCUSED\n                            || afState == CaptureResult.CONTROL_AF_STATE_PASSIVE_UNFOCUSED);\n            Integer afMode = lastResult.get(CaptureResult.CONTROL_AF_MODE);\n            boolean afModeOk = afMode != null && afMode == CaptureResult.CONTROL_AF_MODE_AUTO;\n            boolean result = afStateOk && afModeOk;\n            LOG.i(\"checkShouldSkip:\", result);\n            return result;\n        } else {\n            LOG.i(\"checkShouldSkip: false - lastResult is null.\");\n            return false;\n        }\n    }\n\n    @Override\n    protected void onStarted(@NonNull ActionHolder holder) {\n        holder.getBuilder(this).set(CaptureRequest.CONTROL_AF_MODE,\n                CaptureRequest.CONTROL_AF_MODE_AUTO);\n        holder.getBuilder(this).set(CaptureRequest.CONTROL_AF_TRIGGER,\n                CaptureRequest.CONTROL_AF_TRIGGER_CANCEL);\n        holder.applyBuilder(this);\n    }\n\n    @Override\n    public void onCaptureCompleted(@NonNull ActionHolder holder,\n                                   @NonNull CaptureRequest request,\n                                   @NonNull TotalCaptureResult result) {\n        super.onCaptureCompleted(holder, request, result);\n        Integer afState = result.get(CaptureResult.CONTROL_AF_STATE);\n        Integer afMode = result.get(CaptureResult.CONTROL_AF_MODE);\n        LOG.i(\"onCapture:\", \"afState:\", afState, \"afMode:\", afMode);\n        if (afState == null || afMode == null) return;\n        if (afMode != CaptureResult.CONTROL_AF_MODE_AUTO) return;\n        switch (afState) {\n            case CaptureRequest.CONTROL_AF_STATE_FOCUSED_LOCKED:\n            case CaptureRequest.CONTROL_AF_STATE_NOT_FOCUSED_LOCKED:\n            case CaptureRequest.CONTROL_AF_STATE_INACTIVE:\n            case CaptureRequest.CONTROL_AF_STATE_PASSIVE_FOCUSED:\n            case CaptureRequest.CONTROL_AF_STATE_PASSIVE_UNFOCUSED: {\n                setState(STATE_COMPLETED);\n                break;\n            }\n            case CaptureRequest.CONTROL_AF_STATE_ACTIVE_SCAN:\n            case CaptureRequest.CONTROL_AF_STATE_PASSIVE_SCAN: {\n                // Wait...\n                break;\n            }\n        }\n    }\n", "target": "focus lock"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/markers/DefaultAutoFocusMarker.java:DefaultAutoFocusMarker:0", "source": "\n\n    @VisibleForTesting View mContainer;\n    @VisibleForTesting View mFill;\n\n    @Nullable\n    @Override\n    public View onAttach(@NonNull Context context, @NonNull ViewGroup container) {\n        View view = LayoutInflater.from(context).inflate(R.layout.cameraview_layout_focus_marker,\n                container, false);\n        mContainer = view.findViewById(R.id.focusMarkerContainer);\n        mFill = view.findViewById(R.id.focusMarkerFill);\n        return view;\n    }\n\n    @Override\n    public void onAutoFocusStart(@NonNull AutoFocusTrigger trigger, @NonNull PointF point) {\n        if (trigger == AutoFocusTrigger.METHOD) return;\n        mContainer.clearAnimation();\n        mFill.clearAnimation();\n        mContainer.setScaleX(1.36f);\n        mContainer.setScaleY(1.36f);\n        mContainer.setAlpha(1f);\n        mFill.setScaleX(0);\n        mFill.setScaleY(0);\n        mFill.setAlpha(1f);\n        animate(mContainer, 1, 1, 300, 0, null);\n        animate(mFill, 1, 1, 300, 0, null);\n    }\n\n    @Override\n    public void onAutoFocusEnd(@NonNull AutoFocusTrigger trigger,\n                               boolean successful,\n                               @NonNull PointF point) {\n        if (trigger == AutoFocusTrigger.METHOD) return;\n        if (successful) {\n            animate(mContainer, 1, 0, 500, 0, null);\n            animate(mFill, 1, 0, 500, 0, null);\n        } else {\n            animate(mFill, 0, 0, 500, 0, null);\n            animate(mContainer, 1.36f, 1, 500, 0,\n                    new AnimatorListenerAdapter() {\n                @Override\n                public void onAnimationEnd(Animator animation) {\n                    super.onAnimationEnd(animation);\n                    animate(mContainer, 1.36f, 0, 200, 1000,\n                            null);\n                }\n            });\n        }\n    }\n\n    private static void animate(@NonNull View view, float scale, float alpha, long duration,\n                                long delay, @Nullable Animator.AnimatorListener listener) {\n        view.animate()\n                .scaleX(scale)\n                .scaleY(scale)\n                .alpha(alpha)\n                .setDuration(duration)\n                .setStartDelay(delay)\n                .setListener(listener)\n                .start();\n    }\n", "target": "default auto focus marker"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/gesture/ScrollGestureFinder.java:ScrollGestureFinder:0", "source": "\n\n    private static final String TAG = CLASSTOKEN.class.getSimpleName();\n    private static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    private GestureDetector mDetector;\n    private boolean mNotify;\n    private float mFactor;\n\n    public CLASSTOKEN(final @NonNull Controller controller) {\n        super(controller, 2);\n        mDetector = new GestureDetector(controller.getContext(),\n                new GestureDetector.SimpleOnGestureListener() {\n\n            @Override\n            public boolean onScroll(MotionEvent e1,\n                                    MotionEvent e2,\n                                    float distanceX,\n                                    float distanceY) {\n                boolean horizontal;\n                LOG.i(\"onScroll:\", \"distanceX=\"+distanceX, \"distanceY=\"+distanceY);\n                if (e1 == null || e2 == null) return false; // Got some crashes about this.\n                if (e1.getX() != getPoint(0).x || e1.getY() != getPoint(0).y) {\n                    // First step. We choose now if it's a vertical or horizontal scroll, and\n                    // stick to it for the whole gesture.\n                    horizontal = Math.abs(distanceX) >= Math.abs(distanceY);\n                    setGesture(horizontal ? Gesture.SCROLL_HORIZONTAL : Gesture.SCROLL_VERTICAL);\n                    getPoint(0).set(e1.getX(), e1.getY());\n                } else {\n                    // Not the first step. We already defined the type.\n                    horizontal = getGesture() == Gesture.SCROLL_HORIZONTAL;\n                }\n                getPoint(1).set(e2.getX(), e2.getY());\n                mFactor = horizontal ? (distanceX / controller.getWidth())\n                        : (distanceY / controller.getHeight());\n                mFactor = horizontal ? -mFactor : mFactor; // When vertical, up = positive\n                mNotify = true;\n                return true;\n            }\n        });\n\n        mDetector.setIsLongpressEnabled(false); // Looks important.\n    }\n\n    @Override\n    protected boolean handleTouchEvent(@NonNull MotionEvent event) {\n        // Reset the mNotify flag on a new gesture.\n        // This is to ensure that the mNotify flag stays on until the\n        // previous gesture ends.\n        if (event.getAction() == MotionEvent.ACTION_DOWN) {\n            mNotify = false;\n        }\n\n        // Let's see if we detect something.\n        mDetector.onTouchEvent(event);\n\n        // Keep notifying CameraView as long as the gesture goes.\n        if (mNotify) LOG.i(\"Notifying a gesture of type\", getGesture().name());\n        return mNotify;\n    }\n\n    @Override\n    public float getValue(float currValue, float minValue, float maxValue) {\n        float delta = getFactor(); // -1 ... 1\n\n        // ^ This works well if minValue = 0, maxValue = 1.\n        // Account for the different range:\n        delta *= (maxValue - minValue); // -(max-min) ... (max-min)\n        delta *= 2; // Add some sensitivity.\n\n        return currValue + delta;\n    }\n\n    /* for tests */ protected float getFactor() {\n        return mFactor;\n    }\n", "target": "scroll gesture finder"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filters/InvertColorsFilter.java:InvertColorsFilter:0", "source": "\n\n    private final static String FRAGMENT_SHADER = \"#extension GL_OES_EGL_image_external : require\\n\"\n            + \"precision mediump float;\\n\"\n            + \"varying vec2 \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\";\\n\"\n            + \"uniform samplerExternalOES sTexture;\\n\"\n            + \"void main() {\\n\"\n            + \"  vec4 color = texture2D(sTexture, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\");\\n\"\n            + \"  float colorR = (1.0 - color.r) / 1.0;\\n\"\n            + \"  float colorG = (1.0 - color.g) / 1.0;\\n\"\n            + \"  float colorB = (1.0 - color.b) / 1.0;\\n\"\n            + \"  gl_FragColor = vec4(colorR, colorG, colorB, color.a);\\n\"\n            + \"}\\n\";\n\n    public CLASSTOKEN() { }\n\n    @NonNull\n    @Override\n    public String getFragmentShader() {\n        return FRAGMENT_SHADER;\n    }\n", "target": "invert colors filter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/size/SizeSelectors.java:OrSelector:3", "source": "\n\n        private SizeSelector[] values;\n\n        private CLASSTOKEN(@NonNull SizeSelector... values) {\n            this.values = values;\n        }\n\n        @Override\n        @NonNull\n        public List<Size> select(@NonNull List<Size> source) {\n            List<Size> temp = null;\n            for (SizeSelector selector : values) {\n                temp = selector.select(source);\n                if (!temp.isEmpty()) {\n                    break;\n                }\n            }\n            return temp == null ? new ArrayList<Size>() : temp;\n        }\n\n    ", "target": "or selector"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/action/TimeoutAction.java:TimeoutAction:0", "source": "\n\n    private long startMillis;\n    private long timeoutMillis;\n    private BaseAction action;\n\n    CLASSTOKEN(long timeoutMillis, @NonNull BaseAction action) {\n        this.timeoutMillis = timeoutMillis;\n        this.action = action;\n    }\n\n    @NonNull\n    @Override\n    public BaseAction getAction() {\n        return action;\n    }\n\n    @Override\n    protected void onStart(@NonNull ActionHolder holder) {\n        startMillis = System.currentTimeMillis();\n        super.onStart(holder);\n    }\n\n    @Override\n    public void onCaptureCompleted(@NonNull ActionHolder holder,\n                                   @NonNull CaptureRequest request,\n                                   @NonNull TotalCaptureResult result) {\n        super.onCaptureCompleted(holder, request, result);\n        if (!isCompleted()) {\n            if (System.currentTimeMillis() > startMillis + timeoutMillis) {\n                // This will set our state to COMPLETED and stop requests.\n                getAction().abort(holder);\n            }\n        }\n    }\n", "target": "timeout action"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/internal/GlTextureDrawer.java:GlTextureDrawer:0", "source": "\n\n    private final static String TAG = CLASSTOKEN.class.getSimpleName();\n    private final static CameraLogger LOG = CameraLogger.create(TAG);\n\n    private final static int TEXTURE_TARGET = GLES11Ext.GL_TEXTURE_EXTERNAL_OES;\n    private final static int TEXTURE_UNIT = GLES20.GL_TEXTURE0;\n\n    private final GlTexture mTexture;\n    private float[] mTextureTransform = Egloo.IDENTITY_MATRIX.clone();\n\n    @NonNull\n    private Filter mFilter = new NoFilter();\n    private Filter mPendingFilter = null;\n    private int mProgramHandle = -1;\n\n    @SuppressWarnings(\"unused\")\n    public CLASSTOKEN() {\n        this(new GlTexture(TEXTURE_UNIT, TEXTURE_TARGET));\n    }\n\n    @SuppressWarnings(\"unused\")\n    public CLASSTOKEN(int textureId) {\n        this(new GlTexture(TEXTURE_UNIT, TEXTURE_TARGET, textureId));\n    }\n\n    @SuppressWarnings(\"WeakerAccess\")\n    public CLASSTOKEN(@NonNull GlTexture texture) {\n        mTexture = texture;\n    }\n\n    public void setFilter(@NonNull Filter filter) {\n        mPendingFilter = filter;\n    }\n\n    @NonNull\n    public GlTexture getTexture() {\n        return mTexture;\n    }\n\n    @NonNull\n    public float[] getTextureTransform() {\n        return mTextureTransform;\n    }\n\n    public void setTextureTransform(@NonNull float[] textureTransform) {\n        mTextureTransform = textureTransform;\n    }\n\n    public void draw(final long timestampUs) {\n        if (mPendingFilter != null) {\n            release();\n            mFilter = mPendingFilter;\n            mPendingFilter = null;\n\n        }\n\n        if (mProgramHandle == -1) {\n            mProgramHandle = GlProgram.create(\n                    mFilter.getVertexShader(),\n                    mFilter.getFragmentShader());\n            mFilter.onCreate(mProgramHandle);\n            Egloo.checkGlError(\"program creation\");\n        }\n\n        GLES20.glUseProgram(mProgramHandle);\n        Egloo.checkGlError(\"glUseProgram(handle)\");\n        mTexture.bind();\n        mFilter.draw(timestampUs, mTextureTransform);\n        mTexture.unbind();\n        GLES20.glUseProgram(0);\n        Egloo.checkGlError(\"glUseProgram(0)\");\n    }\n\n    public void release() {\n        if (mProgramHandle == -1) return;\n        mFilter.onDestroy();\n        GLES20.glDeleteProgram(mProgramHandle);\n        mProgramHandle = -1;\n    }\n", "target": "gl texture drawer"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/action/ActionWrapper.java:ActionWrapper:0", "source": "\n\n    /**\n     * Should return the wrapped action.\n     * @return the wrapped action\n     */\n    @NonNull\n    public abstract BaseAction getAction();\n\n    @Override\n    protected void onStart(@NonNull ActionHolder holder) {\n        super.onStart(holder);\n        getAction().addCallback(new ActionCallback() {\n            @Override\n            public void onActionStateChanged(@NonNull Action action, int state) {\n                setState(state);\n                if (state == STATE_COMPLETED) {\n                    action.removeCallback(this);\n                }\n            }\n        });\n        getAction().onStart(holder);\n    }\n\n    @Override\n    protected void onAbort(@NonNull ActionHolder holder) {\n        super.onAbort(holder);\n        getAction().onAbort(holder);\n    }\n\n    @Override\n    public void onCaptureStarted(@NonNull ActionHolder holder, @NonNull CaptureRequest request) {\n        super.onCaptureStarted(holder, request);\n        getAction().onCaptureStarted(holder, request);\n    }\n\n    @Override\n    public void onCaptureProgressed(@NonNull ActionHolder holder,\n                                    @NonNull CaptureRequest request,\n                                    @NonNull CaptureResult result) {\n        super.onCaptureProgressed(holder, request, result);\n        getAction().onCaptureProgressed(holder, request, result);\n    }\n\n    @Override\n    public void onCaptureCompleted(@NonNull ActionHolder holder,\n                                   @NonNull CaptureRequest request,\n                                   @NonNull TotalCaptureResult result) {\n        super.onCaptureCompleted(holder, request, result);\n        getAction().onCaptureCompleted(holder, request, result);\n    }\n", "target": "action wrapper"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filters/CrossProcessFilter.java:CrossProcessFilter:0", "source": "\n\n    private final static String FRAGMENT_SHADER = \"#extension GL_OES_EGL_image_external : require\\n\"\n            + \"precision mediump float;\\n\"\n            + \"uniform samplerExternalOES sTexture;\\n\"\n            + \"varying vec2 \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\";\\n\"\n            + \"void main() {\\n\"\n            + \"  vec4 color = texture2D(sTexture, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\");\\n\"\n            + \"  vec3 ncolor = vec3(0.0, 0.0, 0.0);\\n\"\n            + \"  float value;\\n\"\n            + \"  if (color.r < 0.5) {\\n\"\n            + \"    value = color.r;\\n\"\n            + \"  } else {\\n\"\n            + \"    value = 1.0 - color.r;\\n\"\n            + \"  }\\n\"\n            + \"  float red = 4.0 * value * value * value;\\n\"\n            + \"  if (color.r < 0.5) {\\n\"\n            + \"    ncolor.r = red;\\n\"\n            + \"  } else {\\n\"\n            + \"    ncolor.r = 1.0 - red;\\n\"\n            + \"  }\\n\"\n            + \"  if (color.g < 0.5) {\\n\"\n            + \"    value = color.g;\\n\"\n            + \"  } else {\\n\"\n            + \"    value = 1.0 - color.g;\\n\"\n            + \"  }\\n\"\n            + \"  float green = 2.0 * value * value;\\n\"\n            + \"  if (color.g < 0.5) {\\n\"\n            + \"    ncolor.g = green;\\n\"\n            + \"  } else {\\n\"\n            + \"    ncolor.g = 1.0 - green;\\n\"\n            + \"  }\\n\"\n            + \"  ncolor.b = color.b * 0.5 + 0.25;\\n\"\n            + \"  gl_FragColor = vec4(ncolor.rgb, color.a);\\n\"\n            + \"}\\n\";\n\n    public CLASSTOKEN() { }\n\n    @NonNull\n    @Override\n    public String getFragmentShader() {\n        return FRAGMENT_SHADER;\n    }\n", "target": "cross process filter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/CameraException.java:CameraException:0", "source": "\n\n    /**\n     * Unknown error. No further info available.\n     */\n    public static final int REASON_UNKNOWN = 0;\n\n    /**\n     * We failed to connect to the camera service.\n     * The camera might be in use by another app.\n     */\n    public static final int REASON_FAILED_TO_CONNECT = 1;\n\n    /**\n     * Failed to start the camera preview.\n     * Again, the camera might be in use by another app.\n     */\n    public static final int REASON_FAILED_TO_START_PREVIEW = 2;\n\n    /**\n     * Camera was forced to disconnect.\n     * In Camera1, this is thrown when android.hardware.Camera.CAMERA_ERROR_EVICTED\n     * is caught.\n     */\n    public static final int REASON_DISCONNECTED = 3;\n\n    /**\n     * Could not take a picture or a picture snapshot,\n     * for some not specified reason.\n     */\n    public static final int REASON_PICTURE_FAILED = 4;\n\n    /**\n     * Could not take a video or a video snapshot,\n     * for some not specified reason.\n     */\n    public static final int REASON_VIDEO_FAILED = 5;\n\n    /**\n     * Indicates that we could not find a camera for the current {@link Facing}\n     * value.\n     * This can be solved by changing the facing value and starting again.\n     */\n    public static final int REASON_NO_CAMERA = 6;\n\n    private int reason = REASON_UNKNOWN;\n\n    @SuppressWarnings(\"WeakerAccess\")\n    public CLASSTOKEN(Throwable cause) {\n        super(cause);\n    }\n\n    public CLASSTOKEN(Throwable cause, int reason) {\n        super(cause);\n        this.reason = reason;\n    }\n\n    public CLASSTOKEN(int reason) {\n        super();\n        this.reason = reason;\n    }\n\n    public int getReason() {\n        return reason;\n    }\n\n    /**\n     * Whether this error is unrecoverable. If this function returns true,\n     * the Camera has been closed (or will be soon) and it is likely showing a black preview.\n     * This is the right moment to show an error dialog to the user.\n     *\n     * @return true if this error is unrecoverable\n     */\n    @SuppressWarnings(\"unused\")\n    public boolean isUnrecoverable() {\n        switch (getReason()) {\n            case REASON_FAILED_TO_CONNECT: return true;\n            case REASON_FAILED_TO_START_PREVIEW: return true;\n            case REASON_DISCONNECTED: return true;\n            default: return false;\n        }\n    }\n", "target": "camera exception"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/gesture/GestureFinder.java:GestureFinder:0", "source": "\n\n    public interface Controller {\n        @NonNull Context getContext();\n        int getWidth();\n        int getHeight();\n    }\n\n    // The number of possible values between minValue and maxValue, for the getValue method.\n    // We could make this non-static (e.g. larger granularity for exposure correction).\n    private final static int GRANULARITY = 50;\n\n    private boolean mActive;\n    @VisibleForTesting Gesture mType;\n    private PointF[] mPoints;\n    private Controller mController;\n\n    CLASSTOKEN(@NonNull Controller controller, int points) {\n        mController = controller;\n        mPoints = new PointF[points];\n        for (int i = 0; i < points; i++) {\n            mPoints[i] = new PointF(0, 0);\n        }\n    }\n\n    /**\n     * Makes this instance active, which means, listening to events.\n     * @param active whether this should be active or not\n     */\n    public void setActive(boolean active) {\n        mActive = active;\n    }\n\n    /**\n     * Whether this instance is active, which means, it is listening\n     * to events and identifying new gestures.\n     * @return true if active\n     */\n    public boolean isActive() {\n        return mActive;\n    }\n\n    /**\n     * Called when new events are available.\n     * If true is returned, users will call {@link #getGesture()}, {@link #getPoints()}\n     * and maybe {@link #getValue(float, float, float)} to know more about the gesture.\n     *\n     * @param event the new event\n     * @return true if a gesture was detected\n     */\n    public final boolean onTouchEvent(@NonNull MotionEvent event) {\n        if (!mActive) return false;\n        return handleTouchEvent(event);\n    }\n\n    /**\n     * Called when new events are available.\n     * If true is returned, users will call {@link #getGesture()}, {@link #getPoints()}\n     * and maybe {@link #getValue(float, float, float)} to know more about the gesture.\n     *\n     * @param event the new event\n     * @return true if a gesture was detected\n     */\n    protected abstract boolean handleTouchEvent(@NonNull MotionEvent event);\n\n    /**\n     * Returns the gesture that this instance is currently detecting.\n     * This is mutable - for instance, a scroll layout can detect both\n     * horizontal and vertical scroll gestures.\n     *\n     * @return the current gesture\n     */\n    @NonNull\n    public final Gesture getGesture() {\n        return mType;\n    }\n\n    /**\n     * Sets the currently detected gesture.\n     * @see #getGesture()\n     *\n     * @param gesture the current gesture\n     */\n    protected final void setGesture(Gesture gesture) {\n        mType = gesture;\n    }\n\n    /**\n     * Returns an array of points that identify the currently\n     * detected gesture. If no gesture was detected, this returns\n     * an array of points with x and y set to 0.\n     *\n     * @return array of gesture points\n     */\n    @NonNull\n    public final PointF[] getPoints() {\n        return mPoints;\n    }\n\n    /**\n     * Utility function to access an item in the\n     * {@link #getPoints()} array.\n     *\n     * @param which the array position\n     * @return the point\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    @NonNull\n    protected final PointF getPoint(int which) {\n        return mPoints[which];\n    }\n\n    /**\n     * For {@link GestureType#CONTINUOUS} gestures, returns the float value at the current\n     * gesture state. This means, for example, scaling the old value with a pinch factor,\n     * taking into account the minimum and maximum values.\n     *\n     * @param currValue the last value\n     * @param minValue the min possible value\n     * @param maxValue the max possible value\n     * @return the new continuous value\n     */\n    public final float computeValue(float currValue, float minValue, float maxValue) {\n        return capValue(currValue, getValue(currValue, minValue, maxValue), minValue, maxValue);\n    }\n\n    /**\n     * For {@link GestureType#CONTINUOUS} gestures, returns the float value at the current\n     * gesture state. This means, for example, scaling the old value with a pinch factor,\n     * taking into account the minimum and maximum values.\n     *\n     * @param currValue the last value\n     * @param minValue the min possible value\n     * @param maxValue the max possible value\n     * @return the new continuous value\n     */\n    protected abstract float getValue(float currValue, float minValue, float maxValue);\n\n    /**\n     * Checks for newValue to be between minValue and maxValue,\n     * and checks that it is 'far enough' from the oldValue, in order\n     * to reduce useless updates.\n     */\n    private static float capValue(float oldValue, float newValue, float minValue, float maxValue) {\n        if (newValue < minValue) newValue = minValue;\n        if (newValue > maxValue) newValue = maxValue;\n\n        float distance = (maxValue - minValue) / (float) GRANULARITY;\n        float half = distance / 2;\n        if (newValue >= oldValue - half && newValue <= oldValue + half) {\n            // Too close! Return the oldValue.\n            return oldValue;\n        }\n        return newValue;\n    }\n\n    /**\n     * Returns the controller for this finder.\n     * @return the controller\n     */\n    @NonNull\n    protected Controller getController() {\n        return mController;\n    }\n", "target": "gesture finder"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/frame/Frame.java:Frame:0", "source": "\n\n    private final static String TAG = CLASSTOKEN.class.getSimpleName();\n    private final static CameraLogger LOG = CameraLogger.create(TAG);\n\n    private final FrameManager mManager;\n    private final Class<?> mDataClass;\n\n    private Object mData = null;\n    private long mTime = -1;\n    private long mLastTime = -1;\n    private int mUserRotation = 0;\n    private int mViewRotation = 0;\n    private Size mSize = null;\n    private int mFormat = -1;\n\n    CLASSTOKEN(@NonNull FrameManager manager) {\n        mManager = manager;\n        mDataClass = manager.getFrameDataClass();\n    }\n\n    void setContent(@NonNull Object data, long time, int userRotation, int viewRotation,\n                    @NonNull Size size, int format) {\n        mData = data;\n        mTime = time;\n        mLastTime = time;\n        mUserRotation = userRotation;\n        mViewRotation = viewRotation;\n        mSize = size;\n        mFormat = format;\n    }\n\n    @SuppressWarnings(\"BooleanMethodIsAlwaysInverted\")\n    private boolean hasContent() {\n        return mData != null;\n    }\n\n    private void ensureHasContent() {\n        if (!hasContent()) {\n            LOG.e(\"CLASSTOKEN is dead! time:\", mTime, \"lastTime:\", mLastTime);\n            throw new RuntimeException(\"You should not access a released frame. \" +\n                    \"If this frame was passed to a FrameProcessor, you can only use its contents \" +\n                    \"synchronously, for the duration of the process() method.\");\n        }\n    }\n\n\n    @Override\n    public boolean equals(Object obj) {\n        // We want a super fast implementation here, do not compare arrays.\n        return obj instanceof CLASSTOKEN && ((CLASSTOKEN) obj).mTime == mTime;\n    }\n\n    /**\n     * Clones the frame, returning a frozen content that will not be overwritten.\n     * This can be kept or safely passed to other threads.\n     * Using freeze without clearing with {@link #release()} can result in memory leaks.\n     *\n     * @return a frozen CLASSTOKEN\n     */\n    @SuppressLint(\"NewApi\")\n    @NonNull\n    public CLASSTOKEN freeze() {\n        ensureHasContent();\n        CLASSTOKEN other = new CLASSTOKEN(mManager);\n        //noinspection unchecked\n        Object data = mManager.cloneFrameData(getData());\n        other.setContent(data, mTime, mUserRotation, mViewRotation, mSize, mFormat);\n        return other;\n    }\n\n    /**\n     * Disposes the contents of this frame. Can be useful for frozen frames\n     * that are not useful anymore.\n     */\n    public void release() {\n        if (!hasContent()) return;\n        LOG.v(\"CLASSTOKEN with time\", mTime, \"is being released.\");\n        Object data = mData;\n        mData = null;\n        mUserRotation = 0;\n        mViewRotation = 0;\n        mTime = -1;\n        mSize = null;\n        mFormat = -1;\n        // After the manager is notified, this frame instance can be taken by\n        // someone else, possibly from another thread. So this should be the\n        // last call in this method. If we null data after, we can have issues.\n        //noinspection unchecked\n        mManager.onFrameReleased(this, data);\n    }\n\n    /**\n     * Returns the frame data.\n     * @return the frame data\n     */\n    @SuppressWarnings(\"unchecked\")\n    @NonNull\n    public <T> T getData() {\n        ensureHasContent();\n        return (T) mData;\n    }\n\n    /**\n     * Returns the class returned by {@link #getData()}.\n     * This class depends on the engine that produced this frame.\n     * - {@link Engine#CAMERA1} will produce byte[] arrays\n     * - {@link Engine#CAMERA2} will produce {@link android.media.Image}s\n     * @return the data class\n     */\n    @NonNull\n    public Class<?> getDataClass() {\n        return mDataClass;\n    }\n\n    /**\n     * Returns the milliseconds epoch for this frame,\n     * in the {@link System#currentTimeMillis()} reference.\n     *\n     * @return time data\n     */\n    public long getTime() {\n        ensureHasContent();\n        return mTime;\n    }\n\n    /**\n     * @deprecated use {@link #getRotationToUser()} instead\n     */\n    @Deprecated\n    public int getRotation() {\n        return getRotationToUser();\n    }\n\n    /**\n     * Returns the clock-wise rotation that should be applied on the data\n     * array, such that the resulting frame matches what the user is seeing\n     * on screen. Knowing this can help in the processing phase.\n     *\n     * @return clock-wise rotation\n     */\n    public int getRotationToUser() {\n        ensureHasContent();\n        return mUserRotation;\n    }\n\n    /**\n     * Returns the clock-wise rotation that should be applied on the data\n     * array, such that the resulting frame matches the View / Activity orientation.\n     * Knowing this can help in the drawing / rendering phase.\n     *\n     * @return clock-wise rotation\n     */\n    public int getRotationToView() {\n        ensureHasContent();\n        return mViewRotation;\n    }\n\n    /**\n     * Returns the frame size.\n     *\n     * @return frame size\n     */\n    @NonNull\n    public Size getSize() {\n        ensureHasContent();\n        return mSize;\n    }\n\n    /**\n     * Returns the data format, in one of the\n     * {@link android.graphics.ImageFormat} constants.\n     * This will always be {@link android.graphics.ImageFormat#NV21} for now.\n     *\n     * @return the data format\n     * @see android.graphics.ImageFormat\n     */\n    public int getFormat() {\n        ensureHasContent();\n        return mFormat;\n    }\n", "target": "frame"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/encoding/InputBufferPool.java:InputBufferPool:0", "source": "\n\n    CLASSTOKEN() {\n        super(Integer.MAX_VALUE, new Factory<InputBuffer>() {\n            @Override\n            public InputBuffer create() {\n                return new InputBuffer();\n            }\n        });\n    }\n", "target": "input buffer pool"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/action/LogAction.java:LogAction:0", "source": "\n\n    private final static CameraLogger LOG\n            = CameraLogger.create(CameraEngine.class.getSimpleName());\n\n    private String lastLog;\n\n    @Override\n    public void onCaptureCompleted(@NonNull ActionHolder holder,\n                                   @NonNull CaptureRequest request,\n                                   @NonNull TotalCaptureResult result) {\n        super.onCaptureCompleted(holder, request, result);\n        Integer aeMode = result.get(CaptureResult.CONTROL_AE_MODE);\n        Integer aeState = result.get(CaptureResult.CONTROL_AE_STATE);\n        Integer afState = result.get(CaptureResult.CONTROL_AF_STATE);\n        Boolean aeLock = result.get(CaptureResult.CONTROL_AE_LOCK);\n        Integer aeTriggerState = result.get(CaptureResult.CONTROL_AE_PRECAPTURE_TRIGGER);\n        Integer afTriggerState = result.get(CaptureResult.CONTROL_AF_TRIGGER);\n        String log = \"aeMode: \" + aeMode + \" aeLock: \" + aeLock +\n                \" aeState: \" + aeState + \" aeTriggerState: \" + aeTriggerState +\n                \" afState: \" + afState + \" afTriggerState: \" + afTriggerState;\n        if (!log.equals(lastLog)) {\n            lastLog = log;\n            LOG.i(log);\n        }\n    }\n\n    @Override\n    protected void onCompleted(@NonNull ActionHolder holder) {\n        super.onCompleted(holder);\n        setState(0); // set another state.\n        start(holder); // restart.\n    }\n", "target": "log action"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filters/PosterizeFilter.java:PosterizeFilter:0", "source": "\n\n    private final static String FRAGMENT_SHADER = \"#extension GL_OES_EGL_image_external : require\\n\"\n            + \"precision mediump float;\\n\"\n            + \"uniform samplerExternalOES sTexture;\\n\"\n            + \"varying vec2 \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\";\\n\" + \"void main() {\\n\"\n            + \"  vec4 color = texture2D(sTexture, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\");\\n\"\n            + \"  vec3 pcolor;\\n\"\n            + \"  pcolor.r = (color.r >= 0.5) ? 0.75 : 0.25;\\n\"\n            + \"  pcolor.g = (color.g >= 0.5) ? 0.75 : 0.25;\\n\"\n            + \"  pcolor.b = (color.b >= 0.5) ? 0.75 : 0.25;\\n\"\n            + \"  gl_FragColor = vec4(pcolor, color.a);\\n\"\n            + \"}\\n\";\n\n    public CLASSTOKEN() { }\n\n    @NonNull\n    @Override\n    public String getFragmentShader() {\n        return FRAGMENT_SHADER;\n    }\n", "target": "posterize filter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/meter/ExposureReset.java:ExposureReset:0", "source": "\n\n    private static final String TAG = CLASSTOKEN.class.getSimpleName();\n    private static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    private static final int STATE_WAITING_LOCK = 0;\n\n    @SuppressWarnings(\"WeakerAccess\")\n    public CLASSTOKEN() {\n        super(true);\n    }\n\n    @Override\n    protected void onStarted(@NonNull ActionHolder holder, @Nullable MeteringRectangle area) {\n        int maxRegions = readCharacteristic(CameraCharacteristics.CONTROL_MAX_REGIONS_AE,\n                0);\n        if (area != null && maxRegions > 0) {\n            holder.getBuilder(this).set(CaptureRequest.CONTROL_AE_REGIONS,\n                    new MeteringRectangle[]{area});\n        }\n\n        // NOTE: precapture might not be supported, in which case I think it will be ignored.\n        CaptureResult lastResult = holder.getLastResult(this);\n        Integer trigger = lastResult == null ? null\n                : lastResult.get(CaptureResult.CONTROL_AE_PRECAPTURE_TRIGGER);\n        LOG.i(\"onStarted:\", \"last precapture trigger is\", trigger);\n        if (trigger != null && trigger == CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_START) {\n            LOG.i(\"onStarted:\", \"canceling precapture.\");\n            int newTrigger = Build.VERSION.SDK_INT >= 23\n                    ? CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_CANCEL\n                    : CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_IDLE;\n            holder.getBuilder(this).set(CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER,\n                    newTrigger);\n        }\n\n        // Documentation about CONTROL_AE_PRECAPTURE_TRIGGER says that, if it was started but not\n        // followed by a CAPTURE_INTENT_STILL_PICTURE request, the internal AE routine might remain\n        // locked unless we unlock manually.\n        // This is often the case for us, since the snapshot picture recorder does not use the\n        // intent and anyway we use the precapture sequence for touch metering as well.\n        // To reset docs suggest the use of CONTROL_AE_PRECAPTURE_TRIGGER_CANCEL, which we do above,\n        // or the technique used below: locking then unlocking. This proved to be the ONLY method\n        // to unlock reliably, unlike the cancel trigger (which we'll run anyway).\n        holder.getBuilder(this).set(CaptureRequest.CONTROL_AE_LOCK, true);\n        holder.applyBuilder(this);\n        setState(STATE_WAITING_LOCK);\n    }\n\n    @Override\n    public void onCaptureCompleted(@NonNull ActionHolder holder, @NonNull CaptureRequest request,\n                                   @NonNull TotalCaptureResult result) {\n        super.onCaptureCompleted(holder, request, result);\n        if (getState() == STATE_WAITING_LOCK) {\n            holder.getBuilder(this).set(CaptureRequest.CONTROL_AE_LOCK, false);\n            holder.applyBuilder(this);\n            setState(STATE_COMPLETED);\n        }\n    }\n", "target": "exposure reset"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/CameraView.java:CameraView:0", "source": "\n\n    private final static String TAG = CLASSTOKEN.class.getSimpleName();\n    private static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    public final static int PERMISSION_REQUEST_CODE = 16;\n\n    final static long DEFAULT_AUTOFOCUS_RESET_DELAY_MILLIS = 3000;\n    final static boolean DEFAULT_PLAY_SOUNDS = true;\n    final static boolean DEFAULT_USE_DEVICE_ORIENTATION = true;\n    final static boolean DEFAULT_PICTURE_METERING = true;\n    final static boolean DEFAULT_PICTURE_SNAPSHOT_METERING = false;\n    final static boolean DEFAULT_REQUEST_PERMISSIONS = true;\n    final static int DEFAULT_FRAME_PROCESSING_POOL_SIZE = 2;\n    final static int DEFAULT_FRAME_PROCESSING_EXECUTORS = 1;\n\n    // Self managed parameters\n    private boolean mPlaySounds;\n    private boolean mUseDeviceOrientation;\n    private boolean mRequestPermissions;\n    private HashMap<Gesture, GestureAction> mGestureMap = new HashMap<>(4);\n    private Preview mPreview;\n    private Engine mEngine;\n    private Filter mPendingFilter;\n    private int mFrameProcessingExecutors;\n    private int mActiveGestures;\n\n    // Components\n    private Handler mUiHandler;\n    private Executor mFrameProcessingExecutor;\n    @VisibleForTesting CameraCallbacks mCameraCallbacks;\n    private CameraPreview mCameraPreview;\n    private OrientationHelper mOrientationHelper;\n    private CameraEngine mCameraEngine;\n    private Size mLastPreviewStreamSize;\n    private MediaActionSound mSound;\n    private AutoFocusMarker mAutoFocusMarker;\n    @VisibleForTesting List<CameraListener> mListeners = new CopyOnWriteArrayList<>();\n    @VisibleForTesting List<FrameProcessor> mFrameProcessors = new CopyOnWriteArrayList<>();\n    private Lifecycle mLifecycle;\n\n    // Gestures\n    @VisibleForTesting PinchGestureFinder mPinchGestureFinder;\n    @VisibleForTesting TapGestureFinder mTapGestureFinder;\n    @VisibleForTesting ScrollGestureFinder mScrollGestureFinder;\n\n    // Views\n    @VisibleForTesting GridLinesLayout mGridLinesLayout;\n    @VisibleForTesting MarkerLayout mMarkerLayout;\n    private boolean mKeepScreenOn;\n    @SuppressWarnings({\"FieldCanBeLocal\", \"unused\"})\n    private boolean mExperimental;\n    private boolean mInEditor;\n\n    // Overlays\n    @VisibleForTesting OverlayLayout mOverlayLayout;\n\n    public CLASSTOKEN(@NonNull Context context) {\n        super(context, null);\n        initialize(context, null);\n    }\n\n    public CLASSTOKEN(@NonNull Context context, @Nullable AttributeSet attrs) {\n        super(context, attrs);\n        initialize(context, attrs);\n    }\n\n    //region Init\n\n    @SuppressWarnings(\"WrongConstant\")\n    private void initialize(@NonNull Context context, @Nullable AttributeSet attrs) {\n        mInEditor = isInEditMode();\n        if (mInEditor) return;\n\n        setWillNotDraw(false);\n        TypedArray a = context.getTheme().obtainStyledAttributes(attrs, R.styleable.CLASSTOKEN,\n                0, 0);\n        ControlParser controls = new ControlParser(context, a);\n\n        // Self managed\n        boolean playSounds = a.getBoolean(R.styleable.CameraView_cameraPlaySounds,\n                DEFAULT_PLAY_SOUNDS);\n        boolean useDeviceOrientation = a.getBoolean(\n                R.styleable.CameraView_cameraUseDeviceOrientation, DEFAULT_USE_DEVICE_ORIENTATION);\n        mExperimental = a.getBoolean(R.styleable.CameraView_cameraExperimental, false);\n        mRequestPermissions = a.getBoolean(R.styleable.CameraView_cameraRequestPermissions,\n                DEFAULT_REQUEST_PERMISSIONS);\n        mPreview = controls.getPreview();\n        mEngine = controls.getEngine();\n\n        // Camera engine params\n        int gridColor = a.getColor(R.styleable.CameraView_cameraGridColor,\n                GridLinesLayout.DEFAULT_COLOR);\n        long videoMaxSize = (long) a.getFloat(R.styleable.CameraView_cameraVideoMaxSize,\n                0);\n        int videoMaxDuration = a.getInteger(R.styleable.CameraView_cameraVideoMaxDuration,\n                0);\n        int videoBitRate = a.getInteger(R.styleable.CameraView_cameraVideoBitRate, 0);\n        int audioBitRate = a.getInteger(R.styleable.CameraView_cameraAudioBitRate, 0);\n        float videoFrameRate = a.getFloat(R.styleable.CameraView_cameraPreviewFrameRate, 0);\n        boolean videoFrameRateExact = a.getBoolean(R.styleable.CameraView_cameraPreviewFrameRateExact, false);\n        long autoFocusResetDelay = (long) a.getInteger(\n                R.styleable.CameraView_cameraAutoFocusResetDelay,\n                (int) DEFAULT_AUTOFOCUS_RESET_DELAY_MILLIS);\n        boolean pictureMetering = a.getBoolean(R.styleable.CameraView_cameraPictureMetering,\n                DEFAULT_PICTURE_METERING);\n        boolean pictureSnapshotMetering = a.getBoolean(\n                R.styleable.CameraView_cameraPictureSnapshotMetering,\n                DEFAULT_PICTURE_SNAPSHOT_METERING);\n        int snapshotMaxWidth = a.getInteger(R.styleable.CameraView_cameraSnapshotMaxWidth, 0);\n        int snapshotMaxHeight = a.getInteger(R.styleable.CameraView_cameraSnapshotMaxHeight, 0);\n        int frameMaxWidth = a.getInteger(R.styleable.CameraView_cameraFrameProcessingMaxWidth, 0);\n        int frameMaxHeight = a.getInteger(R.styleable.CameraView_cameraFrameProcessingMaxHeight, 0);\n        int frameFormat = a.getInteger(R.styleable.CameraView_cameraFrameProcessingFormat, 0);\n        int framePoolSize = a.getInteger(R.styleable.CameraView_cameraFrameProcessingPoolSize,\n                DEFAULT_FRAME_PROCESSING_POOL_SIZE);\n        int frameExecutors = a.getInteger(R.styleable.CameraView_cameraFrameProcessingExecutors,\n                DEFAULT_FRAME_PROCESSING_EXECUTORS);\n\n        boolean drawHardwareOverlays = a.getBoolean(R.styleable.CameraView_cameraDrawHardwareOverlays, false);\n\n        // Size selectors and gestures\n        SizeSelectorParser sizeSelectors = new SizeSelectorParser(a);\n        GestureParser gestures = new GestureParser(a);\n        MarkerParser markers = new MarkerParser(a);\n        FilterParser filters = new FilterParser(a);\n\n        a.recycle();\n\n        // Components\n        mCameraCallbacks = new CameraCallbacks();\n        mUiHandler = new Handler(Looper.getMainLooper());\n\n        // Gestures\n        mPinchGestureFinder = new PinchGestureFinder(mCameraCallbacks);\n        mTapGestureFinder = new TapGestureFinder(mCameraCallbacks);\n        mScrollGestureFinder = new ScrollGestureFinder(mCameraCallbacks);\n\n        // Views\n        mGridLinesLayout = new GridLinesLayout(context);\n        mOverlayLayout = new OverlayLayout(context);\n        mMarkerLayout = new MarkerLayout(context);\n        addView(mGridLinesLayout);\n        addView(mMarkerLayout);\n        addView(mOverlayLayout);\n\n        // Create the engine\n        doInstantiateEngine();\n\n        // Apply self managed\n        setPlaySounds(playSounds);\n        setUseDeviceOrientation(useDeviceOrientation);\n        setGrid(controls.getGrid());\n        setGridColor(gridColor);\n        setDrawHardwareOverlays(drawHardwareOverlays);\n\n        // Apply camera engine params\n        // Adding new ones? See setEngine().\n        setFacing(controls.getFacing());\n        setFlash(controls.getFlash());\n        setMode(controls.getMode());\n        setWhiteBalance(controls.getWhiteBalance());\n        setHdr(controls.getHdr());\n        setAudio(controls.getAudio());\n        setAudioBitRate(audioBitRate);\n        setAudioCodec(controls.getAudioCodec());\n        setPictureSize(sizeSelectors.getPictureSizeSelector());\n        setPictureMetering(pictureMetering);\n        setPictureSnapshotMetering(pictureSnapshotMetering);\n        setPictureFormat(controls.getPictureFormat());\n        setVideoSize(sizeSelectors.getVideoSizeSelector());\n        setVideoCodec(controls.getVideoCodec());\n        setVideoMaxSize(videoMaxSize);\n        setVideoMaxDuration(videoMaxDuration);\n        setVideoBitRate(videoBitRate);\n        setAutoFocusResetDelay(autoFocusResetDelay);\n        setPreviewFrameRateExact(videoFrameRateExact);\n        setPreviewFrameRate(videoFrameRate);\n        setSnapshotMaxWidth(snapshotMaxWidth);\n        setSnapshotMaxHeight(snapshotMaxHeight);\n        setFrameProcessingMaxWidth(frameMaxWidth);\n        setFrameProcessingMaxHeight(frameMaxHeight);\n        setFrameProcessingFormat(frameFormat);\n        setFrameProcessingPoolSize(framePoolSize);\n        setFrameProcessingExecutors(frameExecutors);\n\n        // Apply gestures\n        mapGesture(Gesture.TAP, gestures.getTapAction());\n        mapGesture(Gesture.LONG_TAP, gestures.getLongTapAction());\n        mapGesture(Gesture.PINCH, gestures.getPinchAction());\n        mapGesture(Gesture.SCROLL_HORIZONTAL, gestures.getHorizontalScrollAction());\n        mapGesture(Gesture.SCROLL_VERTICAL, gestures.getVerticalScrollAction());\n\n        // Apply markers\n        setAutoFocusMarker(markers.getAutoFocusMarker());\n\n        // Apply filters\n        setFilter(filters.getFilter());\n\n        // Create the orientation helper\n        mOrientationHelper = new OrientationHelper(context, mCameraCallbacks);\n    }\n\n    /**\n     * Engine is instantiated on creation and anytime\n     * {@link #setEngine(Engine)} is called.\n     */\n    private void doInstantiateEngine() {\n        LOG.w(\"doInstantiateEngine:\", \"instantiating. engine:\", mEngine);\n        mCameraEngine = instantiateCameraEngine(mEngine, mCameraCallbacks);\n        LOG.w(\"doInstantiateEngine:\", \"instantiated. engine:\",\n                mCameraEngine.getClass().getSimpleName());\n        mCameraEngine.setOverlay(mOverlayLayout);\n    }\n\n    /**\n     * Preview is instantiated {@link #onAttachedToWindow()}, because\n     * we want to know if we're hardware accelerated or not.\n     * However, in tests, we might want to create the preview right after constructor.\n     */\n    @VisibleForTesting\n    void doInstantiatePreview() {\n        LOG.w(\"doInstantiateEngine:\", \"instantiating. preview:\", mPreview);\n        mCameraPreview = instantiatePreview(mPreview, getContext(), this);\n        LOG.w(\"doInstantiateEngine:\", \"instantiated. preview:\",\n                mCameraPreview.getClass().getSimpleName());\n        mCameraEngine.setPreview(mCameraPreview);\n        if (mPendingFilter != null) {\n            setFilter(mPendingFilter);\n            mPendingFilter = null;\n        }\n    }\n\n    /**\n     * Instantiates the camera engine.\n     *\n     * @param engine the engine preference\n     * @param callback the engine callback\n     * @return the engine\n     */\n    @NonNull\n    protected CameraEngine instantiateCameraEngine(@NonNull Engine engine,\n                                                   @NonNull CameraEngine.Callback callback) {\n        if (mExperimental\n                && engine == Engine.CAMERA2\n                && Build.VERSION.SDK_INT >= Build.VERSION_CODES.LOLLIPOP) {\n            return new Camera2Engine(callback);\n        } else {\n            mEngine = Engine.CAMERA1;\n            return new Camera1Engine(callback);\n        }\n    }\n\n    /**\n     * Instantiates the camera preview.\n     *\n     * @param preview current preview value\n     * @param context a context\n     * @param container the container\n     * @return the preview\n     */\n    @NonNull\n    protected CameraPreview instantiatePreview(@NonNull Preview preview,\n                                               @NonNull Context context,\n                                               @NonNull ViewGroup container) {\n        switch (preview) {\n            case SURFACE:\n                return new SurfaceCameraPreview(context, container);\n            case TEXTURE: {\n                if (isHardwareAccelerated()) {\n                    // TextureView is not supported without hardware acceleration.\n                    return new TextureCameraPreview(context, container);\n                }\n            }\n            case GL_SURFACE: default: {\n                mPreview = Preview.GL_SURFACE;\n                return new GlCameraPreview(context, container);\n            }\n        }\n    }\n\n    @Override\n    protected void onAttachedToWindow() {\n        super.onAttachedToWindow();\n        if (mInEditor) return;\n        if (mCameraPreview == null) {\n            // isHardwareAccelerated will return the real value only after we are\n            // attached. That's why we instantiate the preview here.\n            doInstantiatePreview();\n        }\n    }\n\n    @Override\n    protected void onDetachedFromWindow() {\n        mLastPreviewStreamSize = null;\n        super.onDetachedFromWindow();\n    }\n\n    //endregion\n\n    //region Measuring behavior\n\n    private String ms(int mode) {\n        switch (mode) {\n            case AT_MOST: return \"AT_MOST\";\n            case EXACTLY: return \"EXACTLY\";\n            case UNSPECIFIED: return \"UNSPECIFIED\";\n        }\n        return null;\n    }\n\n    /**\n     * Measuring is basically controlled by layout params width and height.\n     * The basic semantics are:\n     *\n     * - MATCH_PARENT: CLASSTOKEN should completely fill this dimension, even if this might mean\n     *                 not respecting the preview aspect ratio.\n     * - WRAP_CONTENT: CLASSTOKEN should try to adapt this dimension to respect the preview\n     *                 aspect ratio.\n     *\n     * When both dimensions are MATCH_PARENT, CLASSTOKEN will fill its\n     * parent no matter the preview. Thanks to what happens in {@link CameraPreview}, this acts like\n     * a CENTER CROP scale type.\n     *\n     * When both dimensions are WRAP_CONTENT, CLASSTOKEN will take the biggest dimensions that\n     * fit the preview aspect ratio. This acts like a CENTER INSIDE scale type.\n     */\n    @Override\n    protected void onMeasure(int widthMeasureSpec, int heightMeasureSpec) {\n        if (mInEditor) {\n            final int width = MeasureSpec.getSize(widthMeasureSpec);\n            final int height = MeasureSpec.getSize(heightMeasureSpec);\n            super.onMeasure(MeasureSpec.makeMeasureSpec(width, EXACTLY),\n                    MeasureSpec.makeMeasureSpec(height, EXACTLY));\n            return;\n        }\n\n        mLastPreviewStreamSize = mCameraEngine.getPreviewStreamSize(Reference.VIEW);\n        if (mLastPreviewStreamSize == null) {\n            LOG.w(\"onMeasure:\", \"surface is not ready. Calling default behavior.\");\n            super.onMeasure(widthMeasureSpec, heightMeasureSpec);\n            return;\n        }\n\n        // Let's which dimensions need to be adapted.\n        int widthMode = MeasureSpec.getMode(widthMeasureSpec);\n        int heightMode = MeasureSpec.getMode(heightMeasureSpec);\n        final int widthValue = MeasureSpec.getSize(widthMeasureSpec);\n        final int heightValue = MeasureSpec.getSize(heightMeasureSpec);\n        final float previewWidth = mLastPreviewStreamSize.getWidth();\n        final float previewHeight = mLastPreviewStreamSize.getHeight();\n\n        // Pre-process specs\n        final ViewGroup.LayoutParams lp = getLayoutParams();\n        if (!mCameraPreview.supportsCropping()) {\n            // We can't allow EXACTLY constraints in this case.\n            if (widthMode == EXACTLY) widthMode = AT_MOST;\n            if (heightMode == EXACTLY) heightMode = AT_MOST;\n        } else {\n            // If MATCH_PARENT is interpreted as AT_MOST, transform to EXACTLY\n            // to be consistent with our semantics (and our docs).\n            if (widthMode == AT_MOST && lp.width == MATCH_PARENT) widthMode = EXACTLY;\n            if (heightMode == AT_MOST && lp.height == MATCH_PARENT) heightMode = EXACTLY;\n        }\n\n        LOG.i(\"onMeasure:\", \"requested dimensions are (\"\n                + widthValue + \"[\" + ms(widthMode) + \"]x\"\n                + heightValue + \"[\" + ms(heightMode) + \"])\");\n        LOG.i(\"onMeasure:\",  \"previewSize is\", \"(\"\n                + previewWidth + \"x\" + previewHeight + \")\");\n\n        // (1) If we have fixed dimensions (either 300dp or MATCH_PARENT), there's nothing we\n        // should do, other than respect it. The preview will eventually be cropped at the sides\n        // (by Preview scaling) except the case in which these fixed dimensions manage to fit\n        // exactly the preview aspect ratio.\n        if (widthMode == EXACTLY && heightMode == EXACTLY) {\n            LOG.i(\"onMeasure:\", \"both are MATCH_PARENT or fixed value. We adapt.\",\n                    \"This means CROP_CENTER.\", \"(\" + widthValue + \"x\" + heightValue + \")\");\n            super.onMeasure(widthMeasureSpec, heightMeasureSpec);\n            return;\n        }\n\n        // (2) If both dimensions are free, with no limits, then our size will be exactly the\n        // preview size. This can happen rarely, for example in 2d scrollable containers.\n        if (widthMode == UNSPECIFIED && heightMode == UNSPECIFIED) {\n            LOG.i(\"onMeasure:\", \"both are completely free.\",\n                    \"We respect that and extend to the whole preview size.\",\n                    \"(\" + previewWidth + \"x\" + previewHeight + \")\");\n            super.onMeasure(\n                    MeasureSpec.makeMeasureSpec((int) previewWidth, EXACTLY),\n                    MeasureSpec.makeMeasureSpec((int) previewHeight, EXACTLY));\n            return;\n        }\n\n        // It's sure now that at least one dimension can be determined (either because EXACTLY\n        // or AT_MOST). This starts to seem a pleasant situation.\n\n        // (3) If one of the dimension is completely free (e.g. in a scrollable container),\n        // take the other and fit the ratio.\n        // One of the two might be AT_MOST, but we use the value anyway.\n        float ratio = previewHeight / previewWidth;\n        if (widthMode == UNSPECIFIED || heightMode == UNSPECIFIED) {\n            boolean freeWidth = widthMode == UNSPECIFIED;\n            int height, width;\n            if (freeWidth) {\n                height = heightValue;\n                width = Math.round(height / ratio);\n            } else {\n                width = widthValue;\n                height = Math.round(width * ratio);\n            }\n            LOG.i(\"onMeasure:\", \"one dimension was free, we adapted it to fit the ratio.\",\n                    \"(\" + width + \"x\" + height + \")\");\n            super.onMeasure(MeasureSpec.makeMeasureSpec(width, EXACTLY),\n                    MeasureSpec.makeMeasureSpec(height, EXACTLY));\n            return;\n        }\n\n        // (4) At this point both dimensions are either AT_MOST-AT_MOST, EXACTLY-AT_MOST or\n        // AT_MOST-EXACTLY. Let's manage this sanely. If only one is EXACTLY, we can TRY to fit\n        // the aspect ratio, but it is not guaranteed to succeed. It depends on the AT_MOST\n        // value of the other dimensions.\n        if (widthMode == EXACTLY || heightMode == EXACTLY) {\n            boolean freeWidth = widthMode == AT_MOST;\n            int height, width;\n            if (freeWidth) {\n                height = heightValue;\n                width = Math.min(Math.round(height / ratio), widthValue);\n            } else {\n                width = widthValue;\n                height = Math.min(Math.round(width * ratio), heightValue);\n            }\n            LOG.i(\"onMeasure:\", \"one dimension was EXACTLY, another AT_MOST.\",\n                    \"We have TRIED to fit the aspect ratio, but it's not guaranteed.\",\n                    \"(\" + width + \"x\" + height + \")\");\n            super.onMeasure(MeasureSpec.makeMeasureSpec(width, EXACTLY),\n                    MeasureSpec.makeMeasureSpec(height, EXACTLY));\n            return;\n        }\n\n        // (5) Last case, AT_MOST and AT_MOST. Here we can SURELY fit the aspect ratio by\n        // filling one dimension and adapting the other.\n        int height, width;\n        float atMostRatio = (float) heightValue / (float) widthValue;\n        if (atMostRatio >= ratio) {\n            // We must reduce height.\n            width = widthValue;\n            height = Math.round(width * ratio);\n        } else {\n            height = heightValue;\n            width = Math.round(height / ratio);\n        }\n        LOG.i(\"onMeasure:\", \"both dimension were AT_MOST.\",\n                \"We fit the preview aspect ratio.\",\n                \"(\" + width + \"x\" + height + \")\");\n        super.onMeasure(MeasureSpec.makeMeasureSpec(width, EXACTLY),\n                MeasureSpec.makeMeasureSpec(height, EXACTLY));\n    }\n\n    //endregion\n\n    //region Gesture APIs\n\n    /**\n     * Maps a {@link Gesture} to a certain gesture action.\n     * For example, you can assign zoom control to the pinch gesture by just calling:\n     * <code>\n     *     cameraView.mapGesture(Gesture.PINCH, GestureAction.ZOOM);\n     * </code>\n     *\n     * Not all actions can be assigned to a certain gesture. For example, zoom control can't be\n     * assigned to the Gesture.TAP gesture. Look at {@link Gesture} to know more.\n     * This method returns false if they are not assignable.\n     *\n     * @param gesture which gesture to map\n     * @param action which action should be assigned\n     * @return true if this action could be assigned to this gesture\n     */\n    public boolean mapGesture(@NonNull Gesture gesture, @NonNull GestureAction action) {\n        GestureAction none = GestureAction.NONE;\n        if (gesture.isAssignableTo(action)) {\n            mGestureMap.put(gesture, action);\n            switch (gesture) {\n                case PINCH:\n                    mPinchGestureFinder.setActive(mGestureMap.get(Gesture.PINCH) != none);\n                    break;\n                case TAP:\n                // case DOUBLE_TAP:\n                case LONG_TAP:\n                    mTapGestureFinder.setActive(\n                            mGestureMap.get(Gesture.TAP) != none ||\n                            // mGestureMap.get(Gesture.DOUBLE_TAP) != none ||\n                            mGestureMap.get(Gesture.LONG_TAP) != none);\n                    break;\n                case SCROLL_HORIZONTAL:\n                case SCROLL_VERTICAL:\n                    mScrollGestureFinder.setActive(\n                            mGestureMap.get(Gesture.SCROLL_HORIZONTAL) != none ||\n                            mGestureMap.get(Gesture.SCROLL_VERTICAL) != none);\n                    break;\n            }\n\n            mActiveGestures = 0;\n            for(GestureAction act : mGestureMap.values()) {\n                mActiveGestures += act == GestureAction.NONE ? 0 : 1;\n            }\n\n            return true;\n        }\n        mapGesture(gesture, none);\n        return false;\n    }\n\n    /**\n     * Clears any action mapped to the given gesture.\n     * @param gesture which gesture to clear\n     */\n    public void clearGesture(@NonNull Gesture gesture) {\n        mapGesture(gesture, GestureAction.NONE);\n    }\n\n    /**\n     * Returns the action currently mapped to the given gesture.\n     *\n     * @param gesture which gesture to inspect\n     * @return mapped action\n     */\n    @NonNull\n    public GestureAction getGestureAction(@NonNull Gesture gesture) {\n        //noinspection ConstantConditions\n        return mGestureMap.get(gesture);\n    }\n\n    @Override\n    public boolean onInterceptTouchEvent(MotionEvent ev) {\n        // Steal our own events if gestures are enabled\n        return mActiveGestures > 0;\n    }\n\n    @SuppressLint(\"ClickableViewAccessibility\")\n    @Override\n    public boolean onTouchEvent(MotionEvent event) {\n        if (!isOpened()) return true;\n\n        // Pass to our own GestureLayouts\n        CameraOptions options = mCameraEngine.getCameraOptions(); // Non null\n        if (options == null) throw new IllegalStateException(\"Options should not be null here.\");\n        if (mPinchGestureFinder.onTouchEvent(event)) {\n            LOG.i(\"onTouchEvent\", \"pinch!\");\n            onGesture(mPinchGestureFinder, options);\n        } else if (mScrollGestureFinder.onTouchEvent(event)) {\n            LOG.i(\"onTouchEvent\", \"scroll!\");\n            onGesture(mScrollGestureFinder, options);\n        } else if (mTapGestureFinder.onTouchEvent(event)) {\n            LOG.i(\"onTouchEvent\", \"tap!\");\n            onGesture(mTapGestureFinder, options);\n        }\n\n        return true;\n    }\n\n    // Some gesture layout detected a gesture. It's not known at this moment:\n    // (1) if it was mapped to some action (we check here)\n    // (2) if it's supported by the camera (CameraEngine checks)\n    private void onGesture(@NonNull GestureFinder source, @NonNull CameraOptions options) {\n        Gesture gesture = source.getGesture();\n        GestureAction action = mGestureMap.get(gesture);\n        PointF[] points = source.getPoints();\n        float oldValue, newValue;\n        //noinspection ConstantConditions\n        switch (action) {\n\n            case TAKE_PICTURE_SNAPSHOT:\n                takePictureSnapshot();\n                break;\n\n            case TAKE_PICTURE:\n                takePicture();\n                break;\n\n            case AUTO_FOCUS:\n                Size size = new Size(getWidth(), getHeight());\n                MeteringRegions regions = MeteringRegions.fromPoint(size, points[0]);\n                mCameraEngine.startAutoFocus(gesture, regions, points[0]);\n                break;\n\n            case ZOOM:\n                oldValue = mCameraEngine.getZoomValue();\n                newValue = source.computeValue(oldValue, 0, 1);\n                if (newValue != oldValue) {\n                    mCameraEngine.setZoom(newValue, points, true);\n                }\n                break;\n\n            case EXPOSURE_CORRECTION:\n                oldValue = mCameraEngine.getExposureCorrectionValue();\n                float minValue = options.getExposureCorrectionMinValue();\n                float maxValue = options.getExposureCorrectionMaxValue();\n                newValue = source.computeValue(oldValue, minValue, maxValue);\n                if (newValue != oldValue) {\n                    float[] bounds = new float[]{minValue, maxValue};\n                    mCameraEngine.setExposureCorrection(newValue, bounds, points, true);\n                }\n                break;\n\n            case FILTER_CONTROL_1:\n                if (getFilter() instanceof OneParameterFilter) {\n                    OneParameterFilter filter = (OneParameterFilter) getFilter();\n                    oldValue = filter.getParameter1();\n                    newValue = source.computeValue(oldValue, 0, 1);\n                    if (newValue != oldValue) {\n                        filter.setParameter1(newValue);\n                    }\n                }\n                break;\n\n            case FILTER_CONTROL_2:\n                if (getFilter() instanceof TwoParameterFilter) {\n                    TwoParameterFilter filter = (TwoParameterFilter) getFilter();\n                    oldValue = filter.getParameter2();\n                    newValue = source.computeValue(oldValue, 0, 1);\n                    if (newValue != oldValue) {\n                        filter.setParameter2(newValue);\n                    }\n                }\n                break;\n        }\n    }\n\n    //endregion\n\n    //region Lifecycle APIs\n\n    /**\n     * Sets permissions flag if you want enable auto check permissions or disable it.\n     * @param requestPermissions - true: auto check permissions enabled, false: auto check permissions disabled.\n     */\n    @SuppressWarnings(\"unused\")\n    public void setRequestPermissions(boolean requestPermissions) {\n        mRequestPermissions = requestPermissions;\n    }\n\n    /**\n     * Returns whether the camera engine has started.\n     * @return whether the camera has started\n     */\n    public boolean isOpened() {\n        return mCameraEngine.getState().isAtLeast(CameraState.ENGINE)\n                && mCameraEngine.getTargetState().isAtLeast(CameraState.ENGINE);\n    }\n\n    private boolean isClosed() {\n        return mCameraEngine.getState() == CameraState.OFF\n                && !mCameraEngine.isChangingState();\n    }\n\n    /**\n     * Sets the lifecycle owner for this view. This means you don't need\n     * to call {@link #open()}, {@link #close()} or {@link #destroy()} at all.\n     *\n     * If you want that lifecycle stopped controlling the state of the camera,\n     * pass null in this method.\n     *\n     * @param owner the owner activity or fragment\n     */\n    public void setLifecycleOwner(@Nullable LifecycleOwner owner) {\n        if (owner == null) {\n            clearLifecycleObserver();\n        } else {\n            clearLifecycleObserver();\n            mLifecycle = owner.getLifecycle();\n            mLifecycle.addObserver(this);\n        }\n    }\n\n    private void clearLifecycleObserver() {\n        if (mLifecycle != null) {\n            mLifecycle.removeObserver(this);\n            mLifecycle = null;\n        }\n    }\n\n    /**\n     * Starts the camera preview, if not started already.\n     * This should be called onResume(), or when you are ready with permissions.\n     */\n    @OnLifecycleEvent(Lifecycle.Event.ON_RESUME)\n    public void open() {\n        if (mInEditor) return;\n        if (mCameraPreview != null) mCameraPreview.onResume();\n        if (checkPermissions(getAudio())) {\n            // Update display orientation for current CameraEngine\n            mOrientationHelper.enable();\n            mCameraEngine.getAngles().setDisplayOffset(mOrientationHelper.getLastDisplayOffset());\n            mCameraEngine.start();\n        }\n    }\n\n    /**\n     * Checks that we have appropriate permissions.\n     * This means checking that we have audio permissions if audio = Audio.ON.\n     * @param audio the audio setting to be checked\n     * @return true if we can go on, false otherwise.\n     */\n    @SuppressWarnings(\"ConstantConditions\")\n    @SuppressLint(\"NewApi\")\n    protected boolean checkPermissions(@NonNull Audio audio) {\n        checkPermissionsManifestOrThrow(audio);\n        // Manifest is OK at this point. Let's check runtime permissions.\n        if (Build.VERSION.SDK_INT < Build.VERSION_CODES.M) return true;\n\n        Context c = getContext();\n        boolean needsCamera = true;\n        boolean needsAudio = audio == Audio.ON || audio == Audio.MONO || audio == Audio.STEREO;\n\n        needsCamera = needsCamera && c.checkSelfPermission(Manifest.permission.CAMERA)\n                != PackageManager.PERMISSION_GRANTED;\n        needsAudio = needsAudio && c.checkSelfPermission(Manifest.permission.RECORD_AUDIO)\n                != PackageManager.PERMISSION_GRANTED;\n\n        if (!needsCamera && !needsAudio) {\n            return true;\n        } else if (mRequestPermissions) {\n            requestPermissions(needsCamera, needsAudio);\n            return false;\n        } else {\n            return false;\n        }\n    }\n\n    /**\n     * If audio is on we will ask for RECORD_AUDIO permission.\n     * If the developer did not add this to its manifest, throw and fire warnings.\n     */\n    private void checkPermissionsManifestOrThrow(@NonNull Audio audio) {\n        if (audio == Audio.ON || audio == Audio.MONO || audio == Audio.STEREO) {\n            try {\n                PackageManager manager = getContext().getPackageManager();\n                PackageInfo info = manager.getPackageInfo(getContext().getPackageName(),\n                        PackageManager.GET_PERMISSIONS);\n                for (String requestedPermission : info.requestedPermissions) {\n                    if (requestedPermission.equals(Manifest.permission.RECORD_AUDIO)) {\n                        return;\n                    }\n                }\n                String message = LOG.e(\"Permission error: when audio is enabled (Audio.ON)\" +\n                        \" the RECORD_AUDIO permission should be added to the app manifest file.\");\n                throw new IllegalStateException(message);\n            } catch (PackageManager.NameNotFoundException e) {\n                // Not possible.\n            }\n        }\n    }\n\n    /**\n     * Stops the current preview, if any was started.\n     * This should be called onPause().\n     */\n    @OnLifecycleEvent(Lifecycle.Event.ON_PAUSE)\n    public void close() {\n        if (mInEditor) return;\n        mOrientationHelper.disable();\n        mCameraEngine.stop(false);\n        if (mCameraPreview != null) mCameraPreview.onPause();\n    }\n\n    /**\n     * Destroys this instance, releasing immediately\n     * the camera resource.\n     */\n    @OnLifecycleEvent(Lifecycle.Event.ON_DESTROY)\n    public void destroy() {\n        if (mInEditor) return;\n        clearCameraListeners();\n        clearFrameProcessors();\n        mCameraEngine.destroy(true);\n        if (mCameraPreview != null) mCameraPreview.onDestroy();\n    }\n\n    //endregion\n\n    //region Public APIs for controls\n\n    /**\n     * Sets the experimental flag which occasionally can enable\n     * new, unstable beta features.\n     * @param experimental true to enable new features\n     */\n    public void setExperimental(boolean experimental) {\n        mExperimental = experimental;\n    }\n\n    /**\n     * Shorthand for the appropriate set* method.\n     * For example, if control is a {@link Grid}, this calls {@link #setGrid(Grid)}.\n     *\n     * @param control desired value\n     */\n    public void set(@NonNull Control control) {\n        if (control instanceof Audio) {\n            setAudio((Audio) control);\n        } else if (control instanceof Facing) {\n            setFacing((Facing) control);\n        } else if (control instanceof Flash) {\n            setFlash((Flash) control);\n        } else if (control instanceof Grid) {\n            setGrid((Grid) control);\n        } else if (control instanceof Hdr) {\n            setHdr((Hdr) control);\n        } else if (control instanceof Mode) {\n            setMode((Mode) control);\n        } else if (control instanceof WhiteBalance) {\n            setWhiteBalance((WhiteBalance) control);\n        } else if (control instanceof VideoCodec) {\n            setVideoCodec((VideoCodec) control);\n        } else if (control instanceof AudioCodec) {\n            setAudioCodec((AudioCodec) control);\n        } else if (control instanceof Preview) {\n            setPreview((Preview) control);\n        } else if (control instanceof Engine) {\n            setEngine((Engine) control);\n        } else if (control instanceof PictureFormat) {\n            setPictureFormat((PictureFormat) control);\n        }\n    }\n\n    /**\n     * Shorthand for the appropriate get* method.\n     * For example, if control class is a {@link Grid}, this calls {@link #getGrid()}.\n     *\n     * @param controlClass desired value class\n     * @param <T> the class type\n     * @return the control\n     */\n    @SuppressWarnings(\"unchecked\")\n    @NonNull\n    public <T extends Control> T get(@NonNull Class<T> controlClass) {\n        if (controlClass == Audio.class) {\n            return (T) getAudio();\n        } else if (controlClass == Facing.class) {\n            return (T) getFacing();\n        } else if (controlClass == Flash.class) {\n            return (T) getFlash();\n        } else if (controlClass == Grid.class) {\n            return (T) getGrid();\n        } else if (controlClass == Hdr.class) {\n            return (T) getHdr();\n        } else if (controlClass == Mode.class) {\n            return (T) getMode();\n        } else if (controlClass == WhiteBalance.class) {\n            return (T) getWhiteBalance();\n        } else if (controlClass == VideoCodec.class) {\n            return (T) getVideoCodec();\n        } else if (controlClass == AudioCodec.class) {\n            return (T) getAudioCodec();\n        } else if (controlClass == Preview.class) {\n            return (T) getPreview();\n        } else if (controlClass == Engine.class) {\n            return (T) getEngine();\n        } else if (controlClass == PictureFormat.class) {\n            return (T) getPictureFormat();\n        } else {\n            throw new IllegalArgumentException(\"Unknown control class: \" + controlClass);\n        }\n    }\n\n    /**\n     * Controls the preview engine. Should only be called\n     * if this CLASSTOKEN was never added to any window\n     * (like if you created it programmatically).\n     * Otherwise, it has no effect.\n     *\n     * @see Preview#SURFACE\n     * @see Preview#TEXTURE\n     * @see Preview#GL_SURFACE\n     *\n     * @param preview desired preview engine\n     */\n    public void setPreview(@NonNull Preview preview) {\n        boolean isNew = preview != mPreview;\n        if (isNew) {\n            mPreview = preview;\n            boolean isAttachedToWindow = getWindowToken() != null;\n            if (!isAttachedToWindow && mCameraPreview != null) {\n                // Null the preview: will create another when re-attaching.\n                mCameraPreview.onDestroy();\n                mCameraPreview = null;\n            }\n        }\n    }\n\n    /**\n     * Returns the current preview control.\n     *\n     * @see #setPreview(Preview)\n     * @return the current preview control\n     */\n    @NonNull\n    public Preview getPreview() {\n        return mPreview;\n    }\n\n    /**\n     * Controls the core engine. Should only be called\n     * if this CLASSTOKEN is closed (open() was never called).\n     * Otherwise, it has no effect.\n     *\n     * @see Engine#CAMERA1\n     * @see Engine#CAMERA2\n     *\n     * @param engine desired engine\n     */\n    public void setEngine(@NonNull Engine engine) {\n        if (!isClosed()) return;\n        mEngine = engine;\n        CameraEngine oldEngine = mCameraEngine;\n        doInstantiateEngine();\n        if (mCameraPreview != null) mCameraEngine.setPreview(mCameraPreview);\n\n        // Set again all parameters\n        setFacing(oldEngine.getFacing());\n        setFlash(oldEngine.getFlash());\n        setMode(oldEngine.getMode());\n        setWhiteBalance(oldEngine.getWhiteBalance());\n        setHdr(oldEngine.getHdr());\n        setAudio(oldEngine.getAudio());\n        setAudioBitRate(oldEngine.getAudioBitRate());\n        setAudioCodec(oldEngine.getAudioCodec());\n        setPictureSize(oldEngine.getPictureSizeSelector());\n        setPictureFormat(oldEngine.getPictureFormat());\n        setVideoSize(oldEngine.getVideoSizeSelector());\n        setVideoCodec(oldEngine.getVideoCodec());\n        setVideoMaxSize(oldEngine.getVideoMaxSize());\n        setVideoMaxDuration(oldEngine.getVideoMaxDuration());\n        setVideoBitRate(oldEngine.getVideoBitRate());\n        setAutoFocusResetDelay(oldEngine.getAutoFocusResetDelay());\n        setPreviewFrameRate(oldEngine.getPreviewFrameRate());\n        setPreviewFrameRateExact(oldEngine.getPreviewFrameRateExact());\n        setSnapshotMaxWidth(oldEngine.getSnapshotMaxWidth());\n        setSnapshotMaxHeight(oldEngine.getSnapshotMaxHeight());\n        setFrameProcessingMaxWidth(oldEngine.getFrameProcessingMaxWidth());\n        setFrameProcessingMaxHeight(oldEngine.getFrameProcessingMaxHeight());\n        setFrameProcessingFormat(0 /* this is very engine specific, so do not pass */);\n        setFrameProcessingPoolSize(oldEngine.getFrameProcessingPoolSize());\n        mCameraEngine.setHasFrameProcessors(!mFrameProcessors.isEmpty());\n    }\n\n    /**\n     * Returns the current engine control.\n     *\n     * @see #setEngine(Engine)\n     * @return the current engine control\n     */\n    @NonNull\n    public Engine getEngine() {\n        return mEngine;\n    }\n\n    /**\n     * Returns a {@link CameraOptions} instance holding supported options for this camera\n     * session. This might change over time. It's better to hold a reference from\n     * {@link CameraListener#onCameraOpened(CameraOptions)}.\n     *\n     * @return an options map, or null if camera was not opened\n     */\n    @Nullable\n    public CameraOptions getCameraOptions() {\n        return mCameraEngine.getCameraOptions();\n    }\n\n    /**\n     * Sets exposure adjustment, in EV stops. A positive value will mean brighter picture.\n     *\n     * If camera is not opened, this will have no effect.\n     * If {@link CameraOptions#isExposureCorrectionSupported()} is false, this will have no effect.\n     * The provided value should be between the bounds returned by {@link CameraOptions}, or it will\n     * be capped.\n     *\n     * @see CameraOptions#getExposureCorrectionMinValue()\n     * @see CameraOptions#getExposureCorrectionMaxValue()\n     *\n     * @param EVvalue exposure correction value.\n     */\n    public void setExposureCorrection(float EVvalue) {\n        CameraOptions options = getCameraOptions();\n        if (options != null) {\n            float min = options.getExposureCorrectionMinValue();\n            float max = options.getExposureCorrectionMaxValue();\n            if (EVvalue < min) EVvalue = min;\n            if (EVvalue > max) EVvalue = max;\n            float[] bounds = new float[]{min, max};\n            mCameraEngine.setExposureCorrection(EVvalue, bounds, null, false);\n        }\n    }\n\n    /**\n     * Returns the current exposure correction value, typically 0\n     * at start-up.\n     * @return the current exposure correction value\n     */\n    public float getExposureCorrection() {\n        return mCameraEngine.getExposureCorrectionValue();\n    }\n\n    /**\n     * Sets a zoom value. This is not guaranteed to be supported by the current device,\n     * but you can take a look at {@link CameraOptions#isZoomSupported()}.\n     * This will have no effect if called before the camera is opened.\n     *\n     * Zoom value should be between 0 and 1, where 1 will be the maximum available zoom.\n     * If it's not, it will be capped.\n     *\n     * @param zoom value in [0,1]\n     */\n    public void setZoom(float zoom) {\n        if (zoom < 0) zoom = 0;\n        if (zoom > 1) zoom = 1;\n        mCameraEngine.setZoom(zoom, null, false);\n    }\n\n    /**\n     * Returns the current zoom value, something between 0 and 1.\n     * @return the current zoom value\n     */\n    public float getZoom() {\n        return mCameraEngine.getZoomValue();\n    }\n\n    /**\n     * Controls the grids to be drawn over the current layout.\n     *\n     * @see Grid#OFF\n     * @see Grid#DRAW_3X3\n     * @see Grid#DRAW_4X4\n     * @see Grid#DRAW_PHI\n     *\n     * @param gridMode desired grid mode\n     */\n    public void setGrid(@NonNull Grid gridMode) {\n        mGridLinesLayout.setGridMode(gridMode);\n    }\n\n    /**\n     * Gets the current grid mode.\n     * @return the current grid mode\n     */\n    @NonNull\n    public Grid getGrid() {\n        return mGridLinesLayout.getGridMode();\n    }\n\n    /**\n     * Controls the color of the grid lines that will be drawn\n     * over the current layout.\n     *\n     * @param color a resolved color\n     */\n    public void setGridColor(@ColorInt int color) {\n        mGridLinesLayout.setGridColor(color);\n    }\n\n    /**\n     * Returns the current grid color.\n     * @return the current grid color\n     */\n    public int getGridColor() {\n        return mGridLinesLayout.getGridColor();\n    }\n\n    /**\n     * Controls the grids to be drawn over the current layout.\n     *\n     * @see Hdr#OFF\n     * @see Hdr#ON\n     *\n     * @param hdr desired hdr value\n     */\n    public void setHdr(@NonNull Hdr hdr) {\n        mCameraEngine.setHdr(hdr);\n    }\n\n    /**\n     * Gets the current hdr value.\n     * @return the current hdr value\n     */\n    @NonNull\n    public Hdr getHdr() {\n        return mCameraEngine.getHdr();\n    }\n\n    /**\n     * Set location coordinates to be found later in the EXIF header\n     *\n     * @param latitude current latitude\n     * @param longitude current longitude\n     */\n    public void setLocation(double latitude, double longitude) {\n        Location location = new Location(\"Unknown\");\n        location.setTime(System.currentTimeMillis());\n        location.setAltitude(0);\n        location.setLatitude(latitude);\n        location.setLongitude(longitude);\n        mCameraEngine.setLocation(location);\n    }\n\n    /**\n     * Set location values to be found later in the EXIF header\n     *\n     * @param location current location\n     */\n    public void setLocation(@Nullable Location location) {\n        mCameraEngine.setLocation(location);\n    }\n\n    /**\n     * Retrieves the location previously applied with setLocation().\n     *\n     * @return the current location, if any.\n     */\n    @Nullable\n    public Location getLocation() {\n        return mCameraEngine.getLocation();\n    }\n\n    /**\n     * Sets desired white balance to current camera session.\n     *\n     * @see WhiteBalance#AUTO\n     * @see WhiteBalance#INCANDESCENT\n     * @see WhiteBalance#FLUORESCENT\n     * @see WhiteBalance#DAYLIGHT\n     * @see WhiteBalance#CLOUDY\n     *\n     * @param whiteBalance desired white balance behavior.\n     */\n    public void setWhiteBalance(@NonNull WhiteBalance whiteBalance) {\n        mCameraEngine.setWhiteBalance(whiteBalance);\n    }\n\n    /**\n     * Returns the current white balance behavior.\n     * @return white balance value.\n     */\n    @NonNull\n    public WhiteBalance getWhiteBalance() {\n        return mCameraEngine.getWhiteBalance();\n    }\n\n    /**\n     * Sets which camera sensor should be used.\n     *\n     * @see Facing#FRONT\n     * @see Facing#BACK\n     *\n     * @param facing a facing value.\n     */\n    public void setFacing(@NonNull Facing facing) {\n        mCameraEngine.setFacing(facing);\n    }\n\n    /**\n     * Gets the facing camera currently being used.\n     * @return a facing value.\n     */\n    @NonNull\n    public Facing getFacing() {\n        return mCameraEngine.getFacing();\n    }\n\n    /**\n     * Toggles the facing value between {@link Facing#BACK}\n     * and {@link Facing#FRONT}.\n     *\n     * @return the new facing value\n     */\n    public Facing toggleFacing() {\n        Facing facing = mCameraEngine.getFacing();\n        switch (facing) {\n            case BACK:\n                setFacing(Facing.FRONT);\n                break;\n\n            case FRONT:\n                setFacing(Facing.BACK);\n                break;\n        }\n\n        return mCameraEngine.getFacing();\n    }\n\n    /**\n     * Sets the flash mode.\n     *\n     * @see Flash#OFF\n     * @see Flash#ON\n     * @see Flash#AUTO\n     * @see Flash#TORCH\n\n     * @param flash desired flash mode.\n     */\n    public void setFlash(@NonNull Flash flash) {\n        mCameraEngine.setFlash(flash);\n    }\n\n    /**\n     * Gets the current flash mode.\n     * @return a flash mode\n     */\n    @NonNull\n    public Flash getFlash() {\n        return mCameraEngine.getFlash();\n    }\n\n    /**\n     * Controls the audio mode.\n     *\n     * @see Audio#OFF\n     * @see Audio#ON\n     * @see Audio#MONO\n     * @see Audio#STEREO\n     *\n     * @param audio desired audio value\n     */\n    public void setAudio(@NonNull Audio audio) {\n\n        if (audio == getAudio() || isClosed()) {\n            // Check did took place, or will happen on start().\n            mCameraEngine.setAudio(audio);\n\n        } else if (checkPermissions(audio)) {\n            // Camera is running. Pass.\n            mCameraEngine.setAudio(audio);\n\n        } else {\n            // This means that the audio permission is being asked.\n            // Stop the camera so it can be restarted by the developer onPermissionResult.\n            // Developer must also set the audio value again...\n            // Not ideal but good for now.\n            close();\n        }\n    }\n\n    /**\n     * Gets the current audio value.\n     * @return the current audio value\n     */\n    @NonNull\n    public Audio getAudio() {\n        return mCameraEngine.getAudio();\n    }\n\n    /**\n     * Sets an {@link AutoFocusMarker} to be notified of metering start, end and fail events\n     * so that it can draw elements on screen.\n     *\n     * @param autoFocusMarker the marker, or null\n     */\n    public void setAutoFocusMarker(@Nullable AutoFocusMarker autoFocusMarker) {\n        mAutoFocusMarker = autoFocusMarker;\n        mMarkerLayout.onMarker(MarkerLayout.TYPE_AUTOFOCUS, autoFocusMarker);\n    }\n\n    /**\n     * Sets the current delay in milliseconds to reset the focus after a metering event.\n     *\n     * @param delayMillis desired delay (in milliseconds). If the delay\n     *                    is less than or equal to 0 or equal to Long.MAX_VALUE,\n     *                    the values will not be reset.\n     */\n    public void setAutoFocusResetDelay(long delayMillis) {\n        mCameraEngine.setAutoFocusResetDelay(delayMillis);\n    }\n\n    /**\n     * Returns the current delay in milliseconds to reset the focus after a metering event.\n     *\n     * @return the current reset delay in milliseconds\n     */\n    @SuppressWarnings(\"unused\")\n    public long getAutoFocusResetDelay() { return mCameraEngine.getAutoFocusResetDelay(); }\n\n    /**\n     * Starts a 3A touch metering process at the given coordinates, with respect\n     * to the view width and height.\n     *\n     * @param x should be between 0 and getWidth()\n     * @param y should be between 0 and getHeight()\n     */\n    public void startAutoFocus(float x, float y) {\n        if (x < 0 || x > getWidth()) {\n            throw new IllegalArgumentException(\"x should be >= 0 and <= getWidth()\");\n        }\n        if (y < 0 || y > getHeight()) {\n            throw new IllegalArgumentException(\"y should be >= 0 and <= getHeight()\");\n        }\n        Size size = new Size(getWidth(), getHeight());\n        PointF point = new PointF(x, y);\n        MeteringRegions regions = MeteringRegions.fromPoint(size, point);\n        mCameraEngine.startAutoFocus(null, regions, point);\n    }\n\n    /**\n     * Starts a 3A touch metering process at the given coordinates, with respect\n     * to the view width and height.\n     *\n     * @param region should be between 0 and getWidth() / getHeight()\n     */\n    public void startAutoFocus(@NonNull RectF region) {\n        RectF full = new RectF(0, 0, getWidth(), getHeight());\n        if (!full.contains(region)) {\n            throw new IllegalArgumentException(\"Region is out of view bounds! \" + region);\n        }\n        Size size = new Size(getWidth(), getHeight());\n        MeteringRegions regions = MeteringRegions.fromArea(size, region);\n        mCameraEngine.startAutoFocus(null, regions,\n                new PointF(region.centerX(), region.centerY()));\n    }\n\n    /**\n     * <strong>ADVANCED FEATURE</strong> - sets a size selector for the preview stream.\n     * The {@link SizeSelector} will be invoked with the list of available sizes, and the first\n     * acceptable size will be accepted and passed to the internal engine and surface.\n     *\n     * This is typically NOT NEEDED. The default size selector is already smart enough to respect\n     * the picture/video output aspect ratio, and be bigger than the surface so that there is no\n     * upscaling. If all you want is set an aspect ratio, use {@link #setPictureSize(SizeSelector)}\n     * and {@link #setVideoSize(SizeSelector)}.\n     *\n     * When stream size changes, the {@link CLASSTOKEN} is remeasured so any WRAP_CONTENT dimension\n     * is recomputed accordingly.\n     *\n     * See the {@link SizeSelectors} class for handy utilities for creating selectors.\n     *\n     * @param selector a size selector\n     */\n    public void setPreviewStreamSize(@NonNull SizeSelector selector) {\n        mCameraEngine.setPreviewStreamSizeSelector(selector);\n    }\n\n    /**\n     * Set the current session type to either picture or video.\n     *\n     * @see Mode#PICTURE\n     * @see Mode#VIDEO\n     *\n     * @param mode desired session type.\n     */\n    public void setMode(@NonNull Mode mode) {\n        mCameraEngine.setMode(mode);\n    }\n\n    /**\n     * Gets the current mode.\n     * @return the current mode\n     */\n    @NonNull\n    public Mode getMode() {\n        return mCameraEngine.getMode();\n    }\n\n    /**\n     * Sets a capture size selector for picture mode.\n     * The {@link SizeSelector} will be invoked with the list of available sizes, and the first\n     * acceptable size will be accepted and passed to the internal engine.\n     * See the {@link SizeSelectors} class for handy utilities for creating selectors.\n     *\n     * @param selector a size selector\n     */\n    public void setPictureSize(@NonNull SizeSelector selector) {\n        mCameraEngine.setPictureSizeSelector(selector);\n    }\n\n    /**\n     * Whether the engine should perform a metering sequence before taking pictures requested\n     * with {@link #takePicture()}. A metering sequence includes adjusting focus, exposure\n     * and white balance to ensure a good quality of the result.\n     *\n     * When this parameter is true, the quality of the picture increases, but the latency\n     * increases as well. Defaults to true.\n     *\n     * This is a CAMERA2 only API. On CAMERA1, picture metering is always enabled.\n     *\n     * @see #setPictureSnapshotMetering(boolean)\n     * @param enable true to enable\n     */\n    public void setPictureMetering(boolean enable) {\n        mCameraEngine.setPictureMetering(enable);\n    }\n\n    /**\n     * Whether the engine should perform a metering sequence before taking pictures requested\n     * with {@link #takePicture()}. See {@link #setPictureMetering(boolean)}.\n     *\n     * @see #setPictureMetering(boolean)\n     * @return true if picture metering is enabled\n     */\n    public boolean getPictureMetering() {\n        return mCameraEngine.getPictureMetering();\n    }\n\n    /**\n     * Whether the engine should perform a metering sequence before taking pictures requested\n     * with {@link #takePictureSnapshot()}. A metering sequence includes adjusting focus,\n     * exposure and white balance to ensure a good quality of the result.\n     *\n     * When this parameter is true, the quality of the picture increases, but the latency\n     * increases as well. To keep snapshots fast, this defaults to false.\n     *\n     * This is a CAMERA2 only API. On CAMERA1, picture snapshot metering is always disabled.\n     *\n     * @see #setPictureMetering(boolean)\n     * @param enable true to enable\n     */\n    public void setPictureSnapshotMetering(boolean enable) {\n        mCameraEngine.setPictureSnapshotMetering(enable);\n    }\n\n    /**\n     * Whether the engine should perform a metering sequence before taking pictures requested\n     * with {@link #takePictureSnapshot()}. See {@link #setPictureSnapshotMetering(boolean)}.\n     *\n     * @see #setPictureSnapshotMetering(boolean)\n     * @return true if picture metering is enabled\n     */\n    public boolean getPictureSnapshotMetering() {\n        return mCameraEngine.getPictureSnapshotMetering();\n    }\n\n    /**\n     * Sets the format for pictures taken with {@link #takePicture()}. This format does not apply\n     * to picture snapshots taken with {@link #takePictureSnapshot()}.\n     * The {@link PictureFormat#JPEG} is always supported - for other values, please check\n     * the {@link CameraOptions#getSupportedPictureFormats()} value.\n     *\n     * @param pictureFormat new format\n     */\n    public void setPictureFormat(@NonNull PictureFormat pictureFormat) {\n        mCameraEngine.setPictureFormat(pictureFormat);\n    }\n\n    /**\n     * Returns the current picture format.\n     * @see #setPictureFormat(PictureFormat)\n     * @return the picture format\n     */\n    @NonNull\n    public PictureFormat getPictureFormat() {\n        return mCameraEngine.getPictureFormat();\n    }\n\n\n    /**\n     * Sets a capture size selector for video mode.\n     * The {@link SizeSelector} will be invoked with the list of available sizes, and the first\n     * acceptable size will be accepted and passed to the internal engine.\n     * See the {@link SizeSelectors} class for handy utilities for creating selectors.\n     *\n     * @param selector a size selector\n     */\n    public void setVideoSize(@NonNull SizeSelector selector) {\n        mCameraEngine.setVideoSizeSelector(selector);\n    }\n\n    /**\n     * Sets the bit rate in bits per second for video capturing.\n     * Will be used by both {@link #takeVideo(File)} and {@link #takeVideoSnapshot(File)}.\n     *\n     * @param bitRate desired bit rate\n     */\n    public void setVideoBitRate(int bitRate) {\n        mCameraEngine.setVideoBitRate(bitRate);\n    }\n\n    /**\n     * Returns the current video bit rate.\n     * @return current bit rate\n     */\n    @SuppressWarnings(\"unused\")\n    public int getVideoBitRate() {\n        return mCameraEngine.getVideoBitRate();\n    }\n\n    /**\n     * A flag to control the behavior when calling {@link #setPreviewFrameRate(float)}.\n     *\n     * If the value is set to true, {@link #setPreviewFrameRate(float)} will choose the preview\n     * frame range as close to the desired new frame rate as possible. Which mean it may choose a\n     * narrow range around the desired frame rate. Note: This option will give you as exact fps as\n     * you want but the sensor will have less freedom when adapting the exposure to the environment,\n     * which may lead to dark preview.\n     *\n     * If the value is set to false, {@link #setPreviewFrameRate(float)} will choose as broad range\n     * as it can.\n     *\n     * @param videoFrameRateExact whether want a more exact preview frame range\n     *\n     * @see #setPreviewFrameRate(float)\n     */\n    public void setPreviewFrameRateExact(boolean videoFrameRateExact) {\n        mCameraEngine.setPreviewFrameRateExact(videoFrameRateExact);\n    }\n\n    /**\n     * Returns whether we want to set preview fps as exact as we set through\n     * {@link #setPreviewFrameRate(float)}.\n     *\n     * @see #setPreviewFrameRateExact(boolean)\n     * @see #setPreviewFrameRate(float)\n     * @return current option\n     */\n    public boolean getPreviewFrameRateExact() {\n        return mCameraEngine.getPreviewFrameRateExact();\n    }\n\n    /**\n     * Sets the preview frame rate in frames per second.\n     * This rate will be used, for example, by the frame processor and in video\n     * snapshot taken through {@link #takeVideo(File)}.\n     *\n     * A value of 0F will restore the rate to a default value.\n     *\n     * @param frameRate desired frame rate\n     */\n    public void setPreviewFrameRate(float frameRate) {\n        mCameraEngine.setPreviewFrameRate(frameRate);\n    }\n\n    /**\n     * Returns the current preview frame rate.\n     * This can return 0F if no frame rate was set.\n     *\n     * @see #setPreviewFrameRate(float)\n     * @return current frame rate\n     */\n    public float getPreviewFrameRate() {\n        return mCameraEngine.getPreviewFrameRate();\n    }\n\n    /**\n     * Sets the bit rate in bits per second for audio capturing.\n     * Will be used by both {@link #takeVideo(File)} and {@link #takeVideoSnapshot(File)}.\n     *\n     * @param bitRate desired bit rate\n     */\n    public void setAudioBitRate(int bitRate) {\n        mCameraEngine.setAudioBitRate(bitRate);\n    }\n\n    /**\n     * Returns the current audio bit rate.\n     * @return current bit rate\n     */\n    @SuppressWarnings(\"unused\")\n    public int getAudioBitRate() {\n        return mCameraEngine.getAudioBitRate();\n    }\n\n    /**\n     * Sets the encoder for audio recordings.\n     * Defaults to {@link AudioCodec#DEVICE_DEFAULT}.\n     *\n     * @see AudioCodec#DEVICE_DEFAULT\n     * @see AudioCodec#AAC\n     * @see AudioCodec#HE_AAC\n     * @see AudioCodec#AAC_ELD\n     *\n     * @param codec requested audio codec\n     */\n    public void setAudioCodec(@NonNull AudioCodec codec) {\n        mCameraEngine.setAudioCodec(codec);\n    }\n\n    /**\n     * Gets the current encoder for audio recordings.\n     * @return the current audio codec\n     */\n    @NonNull\n    public AudioCodec getAudioCodec() {\n        return mCameraEngine.getAudioCodec();\n    }\n\n    /**\n     * Adds a {@link CameraListener} instance to be notified of all\n     * interesting events that happen during the camera lifecycle.\n     *\n     * @param cameraListener a listener for events.\n     */\n    public void addCameraListener(@NonNull CameraListener cameraListener) {\n        mListeners.add(cameraListener);\n    }\n\n    /**\n     * Remove a {@link CameraListener} that was previously registered.\n     *\n     * @param cameraListener a listener for events.\n     */\n    public void removeCameraListener(@NonNull CameraListener cameraListener) {\n        mListeners.remove(cameraListener);\n    }\n\n    /**\n     * Clears the list of {@link CameraListener} that are registered\n     * to camera events.\n     */\n    public void clearCameraListeners() {\n        mListeners.clear();\n    }\n\n    /**\n     * Asks the camera to capture an image of the current scene.\n     * This will trigger {@link CameraListener#onPictureTaken(PictureResult)} if a listener\n     * was registered.\n     *\n     * @see #takePictureSnapshot()\n     */\n    public void takePicture() {\n        PictureResult.Stub stub = new PictureResult.Stub();\n        mCameraEngine.takePicture(stub);\n    }\n\n    /**\n     * Asks the camera to capture a snapshot of the current preview.\n     * This eventually triggers {@link CameraListener#onPictureTaken(PictureResult)} if a listener\n     * was registered.\n     *\n     * The difference with {@link #takePicture()} is that this capture is faster, so it might be\n     * better on slower cameras, though the result can be generally blurry or low quality.\n     *\n     * @see #takePicture()\n     */\n    public void takePictureSnapshot() {\n        PictureResult.Stub stub = new PictureResult.Stub();\n        mCameraEngine.takePictureSnapshot(stub);\n    }\n\n    /**\n     * Starts recording a video. Video will be written to the given file,\n     * so callers should ensure they have appropriate permissions to write to the file.\n     *\n     * @param file a file where the video will be saved\n     */\n    public void takeVideo(@NonNull File file) {\n        takeVideo(file, null);\n    }\n\n    /**\n     * Starts recording a video. Video will be written to the given file,\n     * so callers should ensure they have appropriate permissions to write to the file.\n     *\n     * @param fileDescriptor a file descriptor where the video will be saved\n     */\n    public void takeVideo(@NonNull FileDescriptor fileDescriptor) {\n        takeVideo(null, fileDescriptor);\n    }\n\n    private void takeVideo(@Nullable File file, @Nullable FileDescriptor fileDescriptor) {\n        VideoResult.Stub stub = new VideoResult.Stub();\n        if (file != null) {\n            mCameraEngine.takeVideo(stub, file, null);\n        } else if (fileDescriptor != null) {\n            mCameraEngine.takeVideo(stub, null, fileDescriptor);\n        } else {\n            throw new IllegalStateException(\"file and fileDescriptor are both null.\");\n        }\n        mUiHandler.post(new Runnable() {\n            @Override\n            public void run() {\n                mKeepScreenOn = getKeepScreenOn();\n                if (!mKeepScreenOn) setKeepScreenOn(true);\n            }\n        });\n    }\n\n    /**\n     * Starts recording a fast, low quality video snapshot. Video will be written to the given file,\n     * so callers should ensure they have appropriate permissions to write to the file.\n     *\n     * Throws an exception if API level is below 18, or if the preview being used is not\n     * {@link Preview#GL_SURFACE}.\n     *\n     * @param file a file where the video will be saved\n     */\n    public void takeVideoSnapshot(@NonNull File file) {\n        VideoResult.Stub stub = new VideoResult.Stub();\n        mCameraEngine.takeVideoSnapshot(stub, file);\n        mUiHandler.post(new Runnable() {\n            @Override\n            public void run() {\n                mKeepScreenOn = getKeepScreenOn();\n                if (!mKeepScreenOn) setKeepScreenOn(true);\n            }\n        });\n    }\n\n    /**\n     * Starts recording a video. Video will be written to the given file,\n     * so callers should ensure they have appropriate permissions to write to the file.\n     * Recording will be automatically stopped after the given duration, overriding\n     * temporarily any duration limit set by {@link #setVideoMaxDuration(int)}.\n     *\n     * @param file a file where the video will be saved\n     * @param durationMillis recording max duration\n     */\n    public void takeVideo(@NonNull File file, int durationMillis) {\n        takeVideo(file, null, durationMillis);\n    }\n\n    /**\n     * Starts recording a video. Video will be written to the given file,\n     * so callers should ensure they have appropriate permissions to write to the file.\n     * Recording will be automatically stopped after the given duration, overriding\n     * temporarily any duration limit set by {@link #setVideoMaxDuration(int)}.\n     *\n     * @param fileDescriptor a file descriptor where the video will be saved\n     * @param durationMillis recording max duration\n     */\n    @SuppressWarnings(\"unused\")\n    public void takeVideo(@NonNull FileDescriptor fileDescriptor, int durationMillis) {\n        takeVideo(null, fileDescriptor, durationMillis);\n    }\n\n    private void takeVideo(@Nullable File file, @Nullable FileDescriptor fileDescriptor,\n                           int durationMillis) {\n        final int old = getVideoMaxDuration();\n        addCameraListener(new CameraListener() {\n            @Override\n            public void onVideoTaken(@NonNull VideoResult result) {\n                setVideoMaxDuration(old);\n                removeCameraListener(this);\n            }\n\n            @Override\n            public void onCameraError(@NonNull CameraException exception) {\n                super.onCameraError(exception);\n                if (exception.getReason() == CameraException.REASON_VIDEO_FAILED) {\n                    setVideoMaxDuration(old);\n                    removeCameraListener(this);\n                }\n            }\n        });\n        setVideoMaxDuration(durationMillis);\n        takeVideo(file, fileDescriptor);\n    }\n\n    /**\n     * Starts recording a fast, low quality video snapshot. Video will be written to the given file,\n     * so callers should ensure they have appropriate permissions to write to the file.\n     * Recording will be automatically stopped after the given duration, overriding\n     * temporarily any duration limit set by {@link #setVideoMaxDuration(int)}.\n     *\n     * Throws an exception if API level is below 18, or if the preview being used is not\n     * {@link Preview#GL_SURFACE}.\n     *\n     * @param file a file where the video will be saved\n     * @param durationMillis recording max duration\n     *\n     */\n    public void takeVideoSnapshot(@NonNull File file, int durationMillis) {\n        final int old = getVideoMaxDuration();\n        addCameraListener(new CameraListener() {\n            @Override\n            public void onVideoTaken(@NonNull VideoResult result) {\n                setVideoMaxDuration(old);\n                removeCameraListener(this);\n            }\n\n            @Override\n            public void onCameraError(@NonNull CameraException exception) {\n                super.onCameraError(exception);\n                if (exception.getReason() == CameraException.REASON_VIDEO_FAILED) {\n                    setVideoMaxDuration(old);\n                    removeCameraListener(this);\n                }\n            }\n        });\n        setVideoMaxDuration(durationMillis);\n        takeVideoSnapshot(file);\n    }\n\n    // TODO: pauseVideo and resumeVideo? There is mediarecorder.pause(), but API 24...\n\n    /**\n     * Stops capturing video or video snapshots being recorded, if there was any.\n     * This will fire {@link CameraListener#onVideoTaken(VideoResult)}.\n     */\n    public void stopVideo() {\n        mCameraEngine.stopVideo();\n        mUiHandler.post(new Runnable() {\n            @Override\n            public void run() {\n                if (getKeepScreenOn() != mKeepScreenOn) setKeepScreenOn(mKeepScreenOn);\n            }\n        });\n    }\n\n    /**\n     * Sets the max width for snapshots taken with {@link #takePictureSnapshot()} or\n     * {@link #takeVideoSnapshot(File)}. If the snapshot width exceeds this value, the snapshot\n     * will be scaled down to match this constraint.\n     *\n     * @param maxWidth max width for snapshots\n     */\n    public void setSnapshotMaxWidth(int maxWidth) {\n        mCameraEngine.setSnapshotMaxWidth(maxWidth);\n    }\n\n    /**\n     * Sets the max height for snapshots taken with {@link #takePictureSnapshot()} or\n     * {@link #takeVideoSnapshot(File)}. If the snapshot height exceeds this value, the snapshot\n     * will be scaled down to match this constraint.\n     *\n     * @param maxHeight max height for snapshots\n     */\n    public void setSnapshotMaxHeight(int maxHeight) {\n        mCameraEngine.setSnapshotMaxHeight(maxHeight);\n    }\n\n    /**\n     * The max width for snapshots.\n     * @see #setSnapshotMaxWidth(int)\n     * @return max width\n     */\n    public int getSnapshotMaxWidth() {\n        return mCameraEngine.getSnapshotMaxWidth();\n    }\n\n    /**\n     * The max height for snapshots.\n     * @see #setSnapshotMaxHeight(int)\n     * @return max height\n     */\n    public int getSnapshotMaxHeight() {\n        return mCameraEngine.getSnapshotMaxHeight();\n    }\n\n    /**\n     * Returns the size used for snapshots, or null if it hasn't been computed\n     * (for example if the surface is not ready). This is the preview size, rotated to match\n     * the output orientation, and cropped to the visible part.\n     *\n     * This also includes the {@link #setSnapshotMaxWidth(int)} and\n     * {@link #setSnapshotMaxHeight(int)} constraints.\n     *\n     * This does NOT include any constraints specific to video encoding, which are\n     * device specific and depend on the capabilities of the device codec.\n     *\n     * @return the size of snapshots\n     */\n    @Nullable\n    public Size getSnapshotSize() {\n        if (getWidth() == 0 || getHeight() == 0) return null;\n\n        // Get the preview size and crop according to the current view size.\n        // It's better to do calculations in the REF_VIEW reference, and then flip if needed.\n        Size preview = mCameraEngine.getUncroppedSnapshotSize(Reference.VIEW);\n        if (preview == null) return null; // Should never happen.\n        AspectRatio viewRatio = AspectRatio.of(getWidth(), getHeight());\n        Rect crop = CropHelper.computeCrop(preview, viewRatio);\n        Size cropSize = new Size(crop.width(), crop.height());\n        if (mCameraEngine.getAngles().flip(Reference.VIEW, Reference.OUTPUT)) {\n            return cropSize.flip();\n        } else {\n            return cropSize;\n        }\n    }\n\n    /**\n     * Returns the size used for pictures taken with {@link #takePicture()},\n     * or null if it hasn't been computed (for example if the surface is not ready),\n     * or null if we are in video mode.\n     *\n     * The size is rotated to match the output orientation.\n     *\n     * @return the size of pictures\n     */\n    @Nullable\n    public Size getPictureSize() {\n        return mCameraEngine.getPictureSize(Reference.OUTPUT);\n    }\n\n    /**\n     * Returns the size used for videos taken with {@link #takeVideo(File)},\n     * or null if it hasn't been computed (for example if the surface is not ready),\n     * or null if we are in picture mode.\n     *\n     * The size is rotated to match the output orientation.\n     *\n     * @return the size of videos\n     */\n    @Nullable\n    public Size getVideoSize() {\n        return mCameraEngine.getVideoSize(Reference.OUTPUT);\n    }\n\n    // If we end up here, we're in M.\n    @TargetApi(Build.VERSION_CODES.M)\n    private void requestPermissions(boolean requestCamera, boolean requestAudio) {\n        Activity activity = null;\n        Context context = getContext();\n        while (context instanceof ContextWrapper) {\n            if (context instanceof Activity) {\n                activity = (Activity) context;\n            }\n            context = ((ContextWrapper) context).getBaseContext();\n        }\n\n        List<String> permissions = new ArrayList<>();\n        if (requestCamera) permissions.add(Manifest.permission.CAMERA);\n        if (requestAudio) permissions.add(Manifest.permission.RECORD_AUDIO);\n        if (activity != null) {\n            activity.requestPermissions(permissions.toArray(new String[0]),\n                    PERMISSION_REQUEST_CODE);\n        }\n    }\n\n    @SuppressLint(\"NewApi\")\n    private void playSound(int soundType) {\n        if (mPlaySounds) {\n            if (mSound == null) mSound = new MediaActionSound();\n            mSound.play(soundType);\n        }\n    }\n\n    /**\n     * Controls whether CLASSTOKEN should play sound effects on certain\n     * events (picture taken, focus complete). Note that:\n     * - On API level {@literal <} 16, this flag is always false\n     * - Camera1 will always play the shutter sound when taking pictures\n     *\n     * @param playSounds whether to play sound effects\n     */\n    public void setPlaySounds(boolean playSounds) {\n        mPlaySounds = playSounds && Build.VERSION.SDK_INT >= 16;\n        mCameraEngine.setPlaySounds(playSounds);\n    }\n\n    /**\n     * Gets the current sound effect behavior.\n     *\n     * @see #setPlaySounds(boolean)\n     * @return whether sound effects are supported\n     */\n    public boolean getPlaySounds() {\n        return mPlaySounds;\n    }\n\n    /**\n     * Controls whether picture and video output should consider the current device orientation.\n     * For example, when true, if the user rotates the device before taking a picture, the picture\n     * will be rotated as well.\n     *\n     * @param useDeviceOrientation true to consider device orientation for outputs\n     */\n    public void setUseDeviceOrientation(boolean useDeviceOrientation) {\n        mUseDeviceOrientation = useDeviceOrientation;\n    }\n\n    /**\n     * Gets the current behavior for considering the device orientation when returning picture\n     * or video outputs.\n     *\n     * @see #setUseDeviceOrientation(boolean)\n     * @return whether we are using the device orientation for outputs\n     */\n    public boolean getUseDeviceOrientation() {\n        return mUseDeviceOrientation;\n    }\n\n    /**\n     * Sets the encoder for video recordings.\n     * Defaults to {@link VideoCodec#DEVICE_DEFAULT}.\n     *\n     * @see VideoCodec#DEVICE_DEFAULT\n     * @see VideoCodec#H_263\n     * @see VideoCodec#H_264\n     *\n     * @param codec requested video codec\n     */\n    public void setVideoCodec(@NonNull VideoCodec codec) {\n        mCameraEngine.setVideoCodec(codec);\n    }\n\n    /**\n     * Gets the current encoder for video recordings.\n     * @return the current video codec\n     */\n    @NonNull\n    public VideoCodec getVideoCodec() {\n        return mCameraEngine.getVideoCodec();\n    }\n\n    /**\n     * Sets the maximum size in bytes for recorded video files.\n     * Once this size is reached, the recording will automatically stop.\n     * Defaults to unlimited size. Use 0 or negatives to disable.\n     *\n     * @param videoMaxSizeInBytes The maximum video size in bytes\n     */\n    public void setVideoMaxSize(long videoMaxSizeInBytes) {\n        mCameraEngine.setVideoMaxSize(videoMaxSizeInBytes);\n    }\n\n    /**\n     * Returns the maximum size in bytes for recorded video files, or 0\n     * if no size was set.\n     *\n     * @see #setVideoMaxSize(long)\n     * @return the maximum size in bytes\n     */\n    public long getVideoMaxSize() {\n        return mCameraEngine.getVideoMaxSize();\n    }\n\n    /**\n     * Sets the maximum duration in milliseconds for video recordings.\n     * Once this duration is reached, the recording will automatically stop.\n     * Defaults to unlimited duration. Use 0 or negatives to disable.\n     *\n     * @param videoMaxDurationMillis The maximum video duration in milliseconds\n     */\n    public void setVideoMaxDuration(int videoMaxDurationMillis) {\n        mCameraEngine.setVideoMaxDuration(videoMaxDurationMillis);\n    }\n\n    /**\n     * Returns the maximum duration in milliseconds for video recordings, or 0\n     * if no limit was set.\n     *\n     * @see #setVideoMaxDuration(int)\n     * @return the maximum duration in milliseconds\n     */\n    public int getVideoMaxDuration() {\n        return mCameraEngine.getVideoMaxDuration();\n    }\n\n    /**\n     * Returns true if the camera is currently recording a video\n     * @return boolean indicating if the camera is recording a video\n     */\n    public boolean isTakingVideo() {\n        return mCameraEngine.isTakingVideo();\n    }\n\n    /**\n     * Returns true if the camera is currently capturing a picture\n     * @return boolean indicating if the camera is capturing a picture\n     */\n    public boolean isTakingPicture() {\n        return mCameraEngine.isTakingPicture();\n    }\n\n    /**\n     * Sets the overlay layout hardware canvas capture mode to allow hardware\n     * accelerated views to be captured in snapshots\n     *\n     * @param on true if enabled\n     */\n    public void setDrawHardwareOverlays(boolean on) {\n        mOverlayLayout.setHardwareCanvasEnabled(on);\n    }\n\n    /**\n     * Returns true if the overlay layout is set to capture the hardware canvas\n     * of child views\n     *\n     * @return boolean indicating hardware canvas capture is enabled\n     */\n    public boolean getDrawHardwareOverlays() {\n        return mOverlayLayout.getHardwareCanvasEnabled();\n    }\n    //endregion\n\n    //region Callbacks and dispatching\n\n    @VisibleForTesting\n    class CameraCallbacks implements\n            CameraEngine.Callback,\n            OrientationHelper.Callback,\n            GestureFinder.Controller {\n\n        private final String TAG = CameraCallbacks.class.getSimpleName();\n        private final CameraLogger LOG = CameraLogger.create(TAG);\n\n        @NonNull\n        @Override\n        public Context getContext() {\n            return CLASSTOKEN.this.getContext();\n        }\n\n        @Override\n        public int getWidth() {\n            return CLASSTOKEN.this.getWidth();\n        }\n\n        @Override\n        public int getHeight() {\n            return CLASSTOKEN.this.getHeight();\n        }\n\n        @Override\n        public void dispatchOnCameraOpened(@NonNull final CameraOptions options) {\n            LOG.i(\"dispatchOnCameraOpened\", options);\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    for (CameraListener listener : mListeners) {\n                        listener.onCameraOpened(options);\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void dispatchOnCameraClosed() {\n            LOG.i(\"dispatchOnCameraClosed\");\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    for (CameraListener listener : mListeners) {\n                        listener.onCameraClosed();\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void onCameraPreviewStreamSizeChanged() {\n            // Camera preview size has changed.\n            // Request a layout pass for onMeasure() to do its stuff.\n            // Potentially this will change CLASSTOKEN size, which changes Surface size,\n            // which triggers a new Preview size. But hopefully it will converge.\n            Size previewSize = mCameraEngine.getPreviewStreamSize(Reference.VIEW);\n            if (previewSize == null) {\n                throw new RuntimeException(\"Preview stream size should not be null here.\");\n            } else if (previewSize.equals(mLastPreviewStreamSize)) {\n                LOG.i(\"onCameraPreviewStreamSizeChanged:\",\n                        \"swallowing because the preview size has not changed.\", previewSize);\n            } else {\n                LOG.i(\"onCameraPreviewStreamSizeChanged: posting a requestLayout call.\",\n                        \"Preview stream size:\", previewSize);\n                mUiHandler.post(new Runnable() {\n                    @Override\n                    public void run() {\n                        requestLayout();\n                    }\n                });\n            }\n        }\n\n        @Override\n        public void dispatchOnPictureShutter(boolean shouldPlaySound) {\n            if (shouldPlaySound && mPlaySounds) {\n                playSound(MediaActionSound.SHUTTER_CLICK);\n            }\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    for (CameraListener listener : mListeners) {\n                        listener.onPictureShutter();\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void dispatchOnPictureTaken(@NonNull final PictureResult.Stub stub) {\n            LOG.i(\"dispatchOnPictureTaken\", stub);\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    PictureResult result = new PictureResult(stub);\n                    for (CameraListener listener : mListeners) {\n                        listener.onPictureTaken(result);\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void dispatchOnVideoTaken(@NonNull final VideoResult.Stub stub) {\n            LOG.i(\"dispatchOnVideoTaken\", stub);\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    VideoResult result = new VideoResult(stub);\n                    for (CameraListener listener : mListeners) {\n                        listener.onVideoTaken(result);\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void dispatchOnFocusStart(@Nullable final Gesture gesture,\n                                         @NonNull final PointF point) {\n            LOG.i(\"dispatchOnFocusStart\", gesture, point);\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    mMarkerLayout.onEvent(MarkerLayout.TYPE_AUTOFOCUS, new PointF[]{ point });\n                    if (mAutoFocusMarker != null) {\n                        AutoFocusTrigger trigger = gesture != null ?\n                                AutoFocusTrigger.GESTURE : AutoFocusTrigger.METHOD;\n                        mAutoFocusMarker.onAutoFocusStart(trigger, point);\n                    }\n\n                    for (CameraListener listener : mListeners) {\n                        listener.onAutoFocusStart(point);\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void dispatchOnFocusEnd(@Nullable final Gesture gesture,\n                                       final boolean success,\n                                       @NonNull final PointF point) {\n            LOG.i(\"dispatchOnFocusEnd\", gesture, success, point);\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    if (success && mPlaySounds) {\n                        playSound(MediaActionSound.FOCUS_COMPLETE);\n                    }\n\n                    if (mAutoFocusMarker != null) {\n                        AutoFocusTrigger trigger = gesture != null ?\n                                AutoFocusTrigger.GESTURE : AutoFocusTrigger.METHOD;\n                        mAutoFocusMarker.onAutoFocusEnd(trigger, success, point);\n                    }\n\n                    for (CameraListener listener : mListeners) {\n                        listener.onAutoFocusEnd(success, point);\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void onDeviceOrientationChanged(int deviceOrientation) {\n            LOG.i(\"onDeviceOrientationChanged\", deviceOrientation);\n            int displayOffset = mOrientationHelper.getLastDisplayOffset();\n            if (!mUseDeviceOrientation) {\n                // To fool the engine to return outputs in the VIEW reference system,\n                // The device orientation should be set to -displayOffset.\n                int fakeDeviceOrientation = (360 - displayOffset) % 360;\n                mCameraEngine.getAngles().setDeviceOrientation(fakeDeviceOrientation);\n            } else {\n                mCameraEngine.getAngles().setDeviceOrientation(deviceOrientation);\n            }\n            final int value = (deviceOrientation + displayOffset) % 360;\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    for (CameraListener listener : mListeners) {\n                        listener.onOrientationChanged(value);\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void onDisplayOffsetChanged() {\n            if (isOpened()) {\n                // We can't handle display offset (View angle) changes without restarting.\n                // See comments in OrientationHelper for more information.\n                LOG.w(\"onDisplayOffsetChanged\", \"restarting the camera.\");\n                close();\n                open();\n            }\n        }\n\n        @Override\n        public void dispatchOnZoomChanged(final float newValue, @Nullable final PointF[] fingers) {\n            LOG.i(\"dispatchOnZoomChanged\", newValue);\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    for (CameraListener listener : mListeners) {\n                        listener.onZoomChanged(newValue, new float[]{0, 1}, fingers);\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void dispatchOnExposureCorrectionChanged(final float newValue,\n                                                        @NonNull final float[] bounds,\n                                                        @Nullable final PointF[] fingers) {\n            LOG.i(\"dispatchOnExposureCorrectionChanged\", newValue);\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    for (CameraListener listener : mListeners) {\n                        listener.onExposureCorrectionChanged(newValue, bounds, fingers);\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void dispatchFrame(@NonNull final Frame frame) {\n            // The getTime() below might crash if developers incorrectly release\n            // frames asynchronously.\n            LOG.v(\"dispatchFrame:\", frame.getTime(), \"processors:\", mFrameProcessors.size());\n            if (mFrameProcessors.isEmpty()) {\n                // Mark as released. This instance will be reused.\n                frame.release();\n            } else {\n                // Dispatch this frame to frame processors.\n                mFrameProcessingExecutor.execute(new Runnable() {\n                    @Override\n                    public void run() {\n                        LOG.v(\"dispatchFrame: executing. Passing\", frame.getTime(),\n                                \"to processors.\");\n                        for (FrameProcessor processor : mFrameProcessors) {\n                            try {\n                                processor.process(frame);\n                            } catch (Exception e) {\n                                LOG.w(\"Frame processor crashed:\", e);\n                            }\n                        }\n                        frame.release();\n                    }\n                });\n            }\n        }\n\n        @Override\n        public void dispatchError(final CameraException exception) {\n            LOG.i(\"dispatchError\", exception);\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    for (CameraListener listener : mListeners) {\n                        listener.onCameraError(exception);\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void dispatchOnVideoRecordingStart() {\n            LOG.i(\"dispatchOnVideoRecordingStart\");\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    for (CameraListener listener : mListeners) {\n                        listener.onVideoRecordingStart();\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void dispatchOnVideoRecordingEnd() {\n            LOG.i(\"dispatchOnVideoRecordingEnd\");\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    for (CameraListener listener : mListeners) {\n                        listener.onVideoRecordingEnd();\n                    }\n                }\n            });\n        }\n    }\n\n    //endregion\n\n    //region Frame Processing\n\n    /**\n     * Adds a {@link FrameProcessor} instance to be notified of\n     * new frames in the preview stream.\n     *\n     * @param processor a frame processor.\n     */\n    public void addFrameProcessor(@Nullable FrameProcessor processor) {\n        if (processor != null) {\n            mFrameProcessors.add(processor);\n            if (mFrameProcessors.size() == 1) {\n                mCameraEngine.setHasFrameProcessors(true);\n            }\n        }\n    }\n\n    /**\n     * Remove a {@link FrameProcessor} that was previously registered.\n     *\n     * @param processor a frame processor\n     */\n    public void removeFrameProcessor(@Nullable FrameProcessor processor) {\n        if (processor != null) {\n            mFrameProcessors.remove(processor);\n            if (mFrameProcessors.size() == 0) {\n                mCameraEngine.setHasFrameProcessors(false);\n            }\n        }\n    }\n\n    /**\n     * Clears the list of {@link FrameProcessor} that have been registered\n     * to preview frames.\n     */\n    public void clearFrameProcessors() {\n        boolean had = mFrameProcessors.size() > 0;\n        mFrameProcessors.clear();\n        if (had) {\n            mCameraEngine.setHasFrameProcessors(false);\n        }\n    }\n\n    /**\n     * Sets the max width for frame processing {@link Frame}s.\n     * This option is only supported by {@link Engine#CAMERA2} and will have no effect\n     * on other engines.\n     *\n     * @param maxWidth max width for frames\n     */\n    public void setFrameProcessingMaxWidth(int maxWidth) {\n        mCameraEngine.setFrameProcessingMaxWidth(maxWidth);\n    }\n\n    /**\n     * Sets the max height for frame processing {@link Frame}s.\n     * This option is only supported by {@link Engine#CAMERA2} and will have no effect\n     * on other engines.\n     *\n     * @param maxHeight max height for frames\n     */\n    public void setFrameProcessingMaxHeight(int maxHeight) {\n        mCameraEngine.setFrameProcessingMaxHeight(maxHeight);\n    }\n\n    /**\n     * The max width for frame processing frames.\n     * @see #setFrameProcessingMaxWidth(int)\n     * @return max width\n     */\n    public int getFrameProcessingMaxWidth() {\n        return mCameraEngine.getFrameProcessingMaxWidth();\n    }\n\n    /**\n     * The max height for frame processing frames.\n     * @see #setFrameProcessingMaxHeight(int)\n     * @return max height\n     */\n    public int getFrameProcessingMaxHeight() {\n        return mCameraEngine.getFrameProcessingMaxHeight();\n    }\n\n    /**\n     * Sets the {@link android.graphics.ImageFormat} for frame processing.\n     * Before applying you should check {@link CameraOptions#getSupportedFrameProcessingFormats()}.\n     *\n     * @param format image format\n     */\n    public void setFrameProcessingFormat(int format) {\n        mCameraEngine.setFrameProcessingFormat(format);\n    }\n\n    /**\n     * Returns the current frame processing format.\n     * @see #setFrameProcessingFormat(int)\n     * @return image format\n     */\n    public int getFrameProcessingFormat() {\n        return mCameraEngine.getFrameProcessingFormat();\n    }\n\n    /**\n     * Sets the frame processing pool size. This is (roughly) the max number of\n     * {@link Frame} instances that can exist at a given moment in the frame pipeline,\n     * excluding frozen frames.\n     *\n     * Defaults to 2 - higher values will increase the memory usage with little benefit.\n     * Can be higher than 2 if {@link #setFrameProcessingExecutors(int)} is used.\n     * These values should be tuned together. We recommend setting a pool size that's equal to\n     * the number of executors plus 1, so that there's always a free Frame for the camera engine.\n     *\n     * Changing this value after camera initialization will have no effect.\n     * @param poolSize pool size\n     */\n    public void setFrameProcessingPoolSize(int poolSize) {\n        mCameraEngine.setFrameProcessingPoolSize(poolSize);\n    }\n\n    /**\n     * Returns the current frame processing pool size.\n     * @see #setFrameProcessingPoolSize(int)\n     * @return pool size\n     */\n    public int getFrameProcessingPoolSize() {\n        return mCameraEngine.getFrameProcessingPoolSize();\n    }\n\n    /**\n     * Sets the thread pool size for frame processing. This means that if the processing rate\n     * is slower than the preview rate, you can set this value to something bigger than 1\n     * to avoid losing frames.\n     * Defaults to 1 and this should be OK for most applications.\n     *\n     * Should be tuned depending on the task, the processor implementation, and along with\n     * {@link #setFrameProcessingPoolSize(int)}. We recommend choosing a pool size that is\n     * equal to the executors plus 1.\n     * @param executors thread count\n     */\n    public void setFrameProcessingExecutors(int executors) {\n        if (executors < 1) {\n            throw new IllegalArgumentException(\"Need at least 1 executor, got \" + executors);\n        }\n        mFrameProcessingExecutors = executors;\n        ThreadPoolExecutor executor = new ThreadPoolExecutor(\n                executors,\n                executors,\n                4,\n                TimeUnit.SECONDS,\n                new LinkedBlockingQueue<Runnable>(),\n                new ThreadFactory() {\n                    private final AtomicInteger mCount = new AtomicInteger(1);\n                    @Override\n                    public Thread newThread(@NonNull Runnable r) {\n                        return new Thread(r, \"FrameExecutor #\" + mCount.getAndIncrement());\n                    }\n                }\n        );\n        executor.allowCoreThreadTimeOut(true);\n        mFrameProcessingExecutor = executor;\n    }\n\n    /**\n     * Returns the current executors count.\n     * @see #setFrameProcessingExecutors(int)\n     * @return thread count\n     */\n    public int getFrameProcessingExecutors() {\n        return mFrameProcessingExecutors;\n    }\n\n    //endregion\n\n    //region Overlays\n\n    @Override\n    public LayoutParams generateLayoutParams(AttributeSet attributeSet) {\n        if (!mInEditor && mOverlayLayout.isOverlay(attributeSet)) {\n            return mOverlayLayout.generateLayoutParams(attributeSet);\n        }\n        return super.generateLayoutParams(attributeSet);\n    }\n\n    @Override\n    public void addView(View child, int index, ViewGroup.LayoutParams params) {\n        if (!mInEditor && mOverlayLayout.isOverlay(params)) {\n            mOverlayLayout.addView(child, params);\n        } else {\n            super.addView(child, index, params);\n        }\n    }\n\n    @Override\n    public void removeView(View view) {\n        ViewGroup.LayoutParams params = view.getLayoutParams();\n        if (!mInEditor && params != null && mOverlayLayout.isOverlay(params)) {\n            mOverlayLayout.removeView(view);\n        } else {\n            super.removeView(view);\n        }\n    }\n\n    //endregion\n\n    //region Filters\n\n    /**\n     * Applies a real-time filter to the camera preview, if it supports it.\n     * The only preview type that does so is currently {@link Preview#GL_SURFACE}.\n     *\n     * The filter will be applied to any picture snapshot taken with\n     * {@link #takePictureSnapshot()} and any video snapshot taken with\n     * {@link #takeVideoSnapshot(File)}.\n     *\n     * Use {@link NoFilter} to clear the existing filter,\n     * and take a look at the {@link Filters} class for commonly used filters.\n     *\n     * This method will throw an exception if the current preview does not support real-time\n     * filters. Make sure you use {@link Preview#GL_SURFACE} (the default).\n     *\n     * @see Filters\n     * @param filter a new filter\n     */\n    public void setFilter(@NonNull Filter filter) {\n        if (mCameraPreview == null) {\n            mPendingFilter = filter;\n        } else {\n            boolean isNoFilter = filter instanceof NoFilter;\n            boolean isFilterPreview = mCameraPreview instanceof FilterCameraPreview;\n            // If not a filter preview, we only allow NoFilter (called on creation).\n            if (!isNoFilter && !isFilterPreview) {\n                throw new RuntimeException(\"Filters are only supported by the GL_SURFACE preview.\" +\n                        \" Current preview:\" + mPreview);\n            }\n            // If we have a filter preview, apply.\n            if (isFilterPreview) {\n                ((FilterCameraPreview) mCameraPreview).setFilter(filter);\n            }\n            // No-op: !isFilterPreview && isNoPreview\n        }\n    }\n\n    /**\n     * Returns the current real-time filter applied to the camera preview.\n     *\n     * This method will throw an exception if the current preview does not support real-time\n     * filters. Make sure you use {@link Preview#GL_SURFACE} (the default).\n     *\n     * @see #setFilter(Filter)\n     * @return the current filter\n     */\n    @NonNull\n    public Filter getFilter() {\n        if (mCameraPreview == null) {\n            return mPendingFilter;\n        } else if (mCameraPreview instanceof FilterCameraPreview) {\n            return ((FilterCameraPreview) mCameraPreview).getCurrentFilter();\n        } else {\n            throw new RuntimeException(\"Filters are only supported by the GL_SURFACE preview. \" +\n                    \"Current:\" + mPreview);\n        }\n\n    }\n\n    //endregion\n", "target": "camera view"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/internal/CropHelper.java:CropHelper:0", "source": "\n\n    // It's important that size and aspect ratio belong to the same reference.\n    @NonNull\n    public static Rect computeCrop(@NonNull Size currentSize, @NonNull AspectRatio targetRatio) {\n        int currentWidth = currentSize.getWidth();\n        int currentHeight = currentSize.getHeight();\n        if (targetRatio.matches(currentSize, 0.0005F)) {\n            return new Rect(0, 0, currentWidth, currentHeight);\n        }\n\n        // They are not equal. Compute.\n        AspectRatio currentRatio = AspectRatio.of(currentWidth, currentHeight);\n        int x, y, width, height;\n        if (currentRatio.toFloat() > targetRatio.toFloat()) {\n            height = currentHeight;\n            width = Math.round(height * targetRatio.toFloat());\n            y = 0;\n            x = Math.round((currentWidth - width) / 2F);\n        } else {\n            width = currentWidth;\n            height = Math.round(width / targetRatio.toFloat());\n            y = Math.round((currentHeight - height) / 2F);\n            x = 0;\n        }\n        return new Rect(x, y, x + width, y + height);\n    }\n", "target": "crop helper"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/preview/GlCameraPreview.java:GlCameraPreview:0", "source": "\n\n    private boolean mDispatched;\n    private SurfaceTexture mInputSurfaceTexture;\n    private GlTextureDrawer mOutputTextureDrawer;\n    // A synchronized set was not enough to avoid crashes, probably due to external classes\n    // removing the callback while this set is being iterated. CopyOnWriteArraySet solves this.\n    private final Set<RendererFrameCallback> mRendererFrameCallbacks = new CopyOnWriteArraySet<>();\n    @VisibleForTesting float mCropScaleX = 1F;\n    @VisibleForTesting float mCropScaleY = 1F;\n    private View mRootView;\n    private Filter mCurrentFilter;\n\n    public CLASSTOKEN(@NonNull Context context, @NonNull ViewGroup parent) {\n        super(context, parent);\n    }\n\n    @NonNull\n    @Override\n    protected GLSurfaceView onCreateView(@NonNull Context context, @NonNull ViewGroup parent) {\n        ViewGroup root = (ViewGroup) LayoutInflater.from(context)\n                .inflate(R.layout.cameraview_gl_view, parent, false);\n        final GLSurfaceView glView = root.findViewById(R.id.gl_surface_view);\n        final Renderer renderer = instantiateRenderer();\n        glView.setEGLContextClientVersion(2);\n        glView.setRenderer(renderer);\n        glView.setRenderMode(GLSurfaceView.RENDERMODE_WHEN_DIRTY);\n        glView.getHolder().addCallback(new SurfaceHolder.Callback() {\n            public void surfaceCreated(SurfaceHolder holder) {}\n            public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) { }\n\n            @Override\n            public void surfaceDestroyed(SurfaceHolder holder) {\n                dispatchOnSurfaceDestroyed();\n                glView.queueEvent(new Runnable() {\n                    @Override\n                    public void run() {\n                        renderer.onSurfaceDestroyed();\n                    }\n                });\n                mDispatched = false;\n            }\n        });\n        parent.addView(root, 0);\n        mRootView = root;\n        return glView;\n    }\n\n    @NonNull\n    @Override\n    public View getRootView() {\n        return mRootView;\n    }\n\n    @Override\n    public void onResume() {\n        super.onResume();\n        getView().onResume();\n    }\n\n    @Override\n    public void onPause() {\n        super.onPause();\n        getView().onPause();\n    }\n\n    @Override\n    public void onDestroy() {\n        super.onDestroy();\n        // View is gone, so EGL context is gone: callbacks make no sense anymore.\n        mRendererFrameCallbacks.clear();\n    }\n\n    /**\n     * The core renderer that performs the actual drawing operations.\n     */\n    public class Renderer implements GLSurfaceView.Renderer {\n\n        @RendererThread\n        @Override\n        public void onSurfaceCreated(GL10 gl, EGLConfig config) {\n            if (mCurrentFilter == null) {\n                mCurrentFilter = new NoFilter();\n            }\n            mOutputTextureDrawer = new GlTextureDrawer();\n            mOutputTextureDrawer.setFilter(mCurrentFilter);\n            final int textureId = mOutputTextureDrawer.getTexture().getId();\n            mInputSurfaceTexture = new SurfaceTexture(textureId);\n            getView().queueEvent(new Runnable() {\n                @Override\n                public void run() {\n                    for (RendererFrameCallback callback : mRendererFrameCallbacks) {\n                        callback.onRendererTextureCreated(textureId);\n                    }\n                }\n            });\n\n            // Since we are using GLSurfaceView.RENDERMODE_WHEN_DIRTY, we must notify\n            // the SurfaceView of dirtyness, so that it draws again. This is how it's done.\n            mInputSurfaceTexture.setOnFrameAvailableListener(new SurfaceTexture.OnFrameAvailableListener() {\n                @Override\n                public void onFrameAvailable(SurfaceTexture surfaceTexture) {\n                    getView().requestRender(); // requestRender is thread-safe.\n                }\n            });\n        }\n\n        @SuppressWarnings(\"WeakerAccess\")\n        @RendererThread\n        public void onSurfaceDestroyed() {\n            if (mInputSurfaceTexture != null) {\n                mInputSurfaceTexture.setOnFrameAvailableListener(null);\n                mInputSurfaceTexture.release();\n                mInputSurfaceTexture = null;\n            }\n            if (mOutputTextureDrawer != null) {\n                mOutputTextureDrawer.release();\n                mOutputTextureDrawer = null;\n            }\n        }\n\n        @RendererThread\n        @Override\n        public void onSurfaceChanged(GL10 gl, final int width, final int height) {\n            gl.glViewport(0, 0, width, height);\n            mCurrentFilter.setSize(width, height);\n            if (!mDispatched) {\n                dispatchOnSurfaceAvailable(width, height);\n                mDispatched = true;\n            } else if (width != mOutputSurfaceWidth || height != mOutputSurfaceHeight) {\n                dispatchOnSurfaceSizeChanged(width, height);\n            }\n        }\n\n        @RendererThread\n        @Override\n        public void onDrawFrame(GL10 gl) {\n            if (mInputSurfaceTexture == null) return;\n            if (mInputStreamWidth <= 0 || mInputStreamHeight <= 0) {\n                // Skip drawing. Camera was not opened.\n                return;\n            }\n\n            // Latch the latest frame. If there isn't anything new,\n            // we'll just re-use whatever was there before.\n            final float[] transform = mOutputTextureDrawer.getTextureTransform();\n            mInputSurfaceTexture.updateTexImage();\n            mInputSurfaceTexture.getTransformMatrix(transform);\n            // LOG.v(\"onDrawFrame:\", \"timestamp:\", mInputSurfaceTexture.getTimestamp());\n\n            // For Camera2, apply the draw rotation.\n            // See TextureCameraPreview.setDrawRotation() for info.\n            if (mDrawRotation != 0) {\n                Matrix.translateM(transform, 0, 0.5F, 0.5F, 0);\n                Matrix.rotateM(transform, 0, mDrawRotation, 0, 0, 1);\n                Matrix.translateM(transform, 0, -0.5F, -0.5F, 0);\n            }\n\n            if (isCropping()) {\n                // Scaling is easy, but we must also translate before:\n                // If the view is 10x1000 (very tall), it will show only the left strip\n                // of the preview (not the center one).\n                // If the view is 1000x10 (very large), it will show only the bottom strip\n                // of the preview (not the center one).\n                float translX = (1F - mCropScaleX) / 2F;\n                float translY = (1F - mCropScaleY) / 2F;\n                Matrix.translateM(transform, 0, translX, translY, 0);\n                Matrix.scaleM(transform, 0, mCropScaleX, mCropScaleY, 1);\n            }\n\n            mOutputTextureDrawer.draw(mInputSurfaceTexture.getTimestamp() / 1000L);\n            for (RendererFrameCallback callback : mRendererFrameCallbacks) {\n                callback.onRendererFrame(mInputSurfaceTexture, mDrawRotation, mCropScaleX, mCropScaleY);\n            }\n        }\n    }\n\n    @NonNull\n    @Override\n    public Class<SurfaceTexture> getOutputClass() {\n        return SurfaceTexture.class;\n    }\n\n    @NonNull\n    @Override\n    public SurfaceTexture getOutput() {\n        return mInputSurfaceTexture;\n    }\n\n    @Override\n    public boolean supportsCropping() {\n        return true;\n    }\n\n    /**\n     * To crop in GL, we could actually use view.setScaleX and setScaleY, but only from Android N\n     * onward. See documentation: https://developer.android.com/reference/android/view/SurfaceView\n     *\n     *   Note: Starting in platform version Build.VERSION_CODES.N, SurfaceView's window position\n     *   is updated synchronously with other View rendering. This means that translating and scaling\n     *   a SurfaceView on screen will not cause rendering artifacts. Such artifacts may occur on\n     *   previous versions of the platform when its window is positioned asynchronously.\n     *\n     * But to support older platforms, this seem to work - computing scale values and requesting\n     * a new frame, then drawing it with a scaled transformation matrix.\n     * See {@link Renderer#onDrawFrame(GL10)}.\n     */\n    @Override\n    protected void crop(@Nullable final CropCallback callback) {\n        if (mInputStreamWidth > 0 && mInputStreamHeight > 0 && mOutputSurfaceWidth > 0\n                && mOutputSurfaceHeight > 0) {\n            float scaleX = 1f, scaleY = 1f;\n            AspectRatio current = AspectRatio.of(mOutputSurfaceWidth, mOutputSurfaceHeight);\n            AspectRatio target = AspectRatio.of(mInputStreamWidth, mInputStreamHeight);\n            if (current.toFloat() >= target.toFloat()) {\n                // We are too short. Must increase height.\n                scaleY = current.toFloat() / target.toFloat();\n            } else {\n                // We must increase width.\n                scaleX = target.toFloat() / current.toFloat();\n            }\n            mCropping = scaleX > 1.02f || scaleY > 1.02f;\n            mCropScaleX = 1F / scaleX;\n            mCropScaleY = 1F / scaleY;\n            getView().requestRender();\n        }\n        if (callback != null) callback.onCrop();\n    }\n\n    @Override\n    public void addRendererFrameCallback(@NonNull final RendererFrameCallback callback) {\n        getView().queueEvent(new Runnable() {\n            @Override\n            public void run() {\n                mRendererFrameCallbacks.add(callback);\n                if (mOutputTextureDrawer != null) {\n                    int textureId = mOutputTextureDrawer.getTexture().getId();\n                    callback.onRendererTextureCreated(textureId);\n                }\n                callback.onRendererFilterChanged(mCurrentFilter);\n            }\n        });\n    }\n\n    @Override\n    public void removeRendererFrameCallback(@NonNull final RendererFrameCallback callback) {\n        mRendererFrameCallbacks.remove(callback);\n    }\n\n    /**\n     * Returns the output GL texture id.\n     * @return the output GL texture id\n     */\n    @SuppressWarnings(\"unused\")\n    protected int getTextureId() {\n        return mOutputTextureDrawer != null ? mOutputTextureDrawer.getTexture().getId() : -1;\n    }\n\n    /**\n     * Creates the renderer for this GL surface.\n     * @return the renderer for this GL surface\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    @NonNull\n    protected Renderer instantiateRenderer() {\n        return new Renderer();\n    }\n\n    //region Filters\n\n\n    @NonNull\n    @Override\n    public Filter getCurrentFilter() {\n        return mCurrentFilter;\n    }\n\n    @Override\n    public void setFilter(final @NonNull Filter filter) {\n        mCurrentFilter = filter;\n        if (hasSurface()) {\n            filter.setSize(mOutputSurfaceWidth, mOutputSurfaceHeight);\n        }\n\n        getView().queueEvent(new Runnable() {\n            @Override\n            public void run() {\n                if (mOutputTextureDrawer != null) {\n                    mOutputTextureDrawer.setFilter(filter);\n                }\n                for (RendererFrameCallback callback : mRendererFrameCallbacks) {\n                    callback.onRendererFilterChanged(filter);\n                }\n            }\n        });\n    }\n", "target": "gl camera preview"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/metering/Camera1MeteringTransform.java:Camera1MeteringTransform:0", "source": "\n\n    protected static final String TAG = CLASSTOKEN.class.getSimpleName();\n    protected static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    private final int displayToSensor;\n    private final Size previewSize;\n\n    public CLASSTOKEN(@NonNull Angles angles, @NonNull Size previewSize) {\n        this.displayToSensor = -angles.offset(Reference.SENSOR, Reference.VIEW, Axis.ABSOLUTE);\n        this.previewSize = previewSize;\n    }\n\n    @NonNull\n    @Override\n    public PointF transformMeteringPoint(@NonNull PointF point) {\n        // First, rescale to the -1000 ... 1000 range.\n        PointF scaled = new PointF();\n        scaled.x = -1000F + (point.x / previewSize.getWidth()) * 2000F;\n        scaled.y = -1000F + (point.y / previewSize.getHeight()) * 2000F;\n\n        // Apply rotation to this point.\n        // https://academo.org/demos/rotation-about-point/\n        PointF rotated = new PointF();\n        double theta = ((double) displayToSensor) * Math.PI / 180;\n        rotated.x = (float) (scaled.x * Math.cos(theta) - scaled.y * Math.sin(theta));\n        rotated.y = (float) (scaled.x * Math.sin(theta) + scaled.y * Math.cos(theta));\n        LOG.i(\"scaled:\", scaled, \"rotated:\", rotated);\n        return rotated;\n    }\n\n    @NonNull\n    @Override\n    public Camera.Area transformMeteringRegion(@NonNull RectF region, int weight) {\n        Rect rect = new Rect();\n        region.round(rect);\n        return new Camera.Area(rect, weight);\n    }\n", "target": "camera 1 metering transform"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filters/TintFilter.java:TintFilter:0", "source": "\n\n    private final static String FRAGMENT_SHADER = \"#extension GL_OES_EGL_image_external : require\\n\"\n            + \"precision mediump float;\\n\"\n            + \"uniform samplerExternalOES sTexture;\\n\"\n            + \"uniform vec3 tint;\\n\"\n            + \"vec3 color_ratio;\\n\"\n            + \"varying vec2 \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\";\\n\"\n            + \"void main() {\\n\"\n            + \"  color_ratio[0] = \" + 0.21f + \";\\n\"\n            + \"  color_ratio[1] = \" + 0.71f + \";\\n\"\n            + \"  color_ratio[2] = \" + 0.07f + \";\\n\"\n            + \"  vec4 color = texture2D(sTexture, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\");\\n\"\n            + \"  float avg_color = dot(color_ratio, color.rgb);\\n\"\n            + \"  vec3 new_color = min(0.8 * avg_color + 0.2 * tint, 1.0);\\n\"\n            + \"  gl_FragColor = vec4(new_color.rgb, color.a);\\n\" + \"}\\n\";\n\n    private int tint = Color.RED;\n    private int tintLocation = -1;\n\n    public CLASSTOKEN() { }\n\n    /**\n     * Sets the current tint.\n     * @param color current tint\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public void setTint(@ColorInt int color) {\n        // Remove any alpha.\n        this.tint = Color.rgb(Color.red(color), Color.green(color), Color.blue(color));\n    }\n\n    /**\n     * Returns the current tint.\n     *\n     * @see #setTint(int)\n     * @return tint\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    @ColorInt\n    public int getTint() {\n        return tint;\n    }\n\n    @Override\n    public void setParameter1(float value) {\n        // no easy way to transform 0...1 into a color.\n        setTint((int) (value * 0xFFFFFF));\n    }\n\n    @Override\n    public float getParameter1() {\n        int color = getTint();\n        color = Color.argb(0, Color.red(color), Color.green(color), Color.blue(color));\n        return (float) color / 0xFFFFFF;\n    }\n\n    @NonNull\n    @Override\n    public String getFragmentShader() {\n        return FRAGMENT_SHADER;\n    }\n\n    @Override\n    public void onCreate(int programHandle) {\n        super.onCreate(programHandle);\n        tintLocation = GLES20.glGetUniformLocation(programHandle, \"tint\");\n        Egloo.checkGlProgramLocation(tintLocation, \"tint\");\n    }\n\n    @Override\n    public void onDestroy() {\n        super.onDestroy();\n        tintLocation = -1;\n    }\n\n    @Override\n    protected void onPreDraw(long timestampUs, @NonNull float[] transformMatrix) {\n        super.onPreDraw(timestampUs, transformMatrix);\n        float[] channels = new float[]{\n                Color.red(tint) / 255f,\n                Color.green(tint) / 255f,\n                Color.blue(tint) / 255f\n        };\n        GLES20.glUniform3fv(tintLocation, 1, channels, 0);\n        Egloo.checkGlError(\"glUniform3fv\");\n    }\n", "target": "tint filter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/meter/FocusReset.java:FocusReset:0", "source": "\n\n    private static final String TAG = CLASSTOKEN.class.getSimpleName();\n    private static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    @SuppressWarnings(\"WeakerAccess\")\n    public CLASSTOKEN() {\n        super(true);\n    }\n\n    @Override\n    protected void onStarted(@NonNull ActionHolder holder, @Nullable MeteringRectangle area) {\n        boolean changed = false;\n        int maxRegions = readCharacteristic(CameraCharacteristics.CONTROL_MAX_REGIONS_AF,\n                0);\n        if (area != null && maxRegions > 0) {\n            holder.getBuilder(this).set(CaptureRequest.CONTROL_AF_REGIONS,\n                    new MeteringRectangle[]{area});\n            changed = true;\n        }\n\n        // NOTE: trigger might not be supported, in which case I think it will be ignored.\n        CaptureResult lastResult = holder.getLastResult(this);\n        Integer trigger = lastResult == null ? null\n                : lastResult.get(CaptureResult.CONTROL_AF_TRIGGER);\n        LOG.w(\"onStarted:\", \"last focus trigger is\", trigger);\n        if (trigger != null && trigger == CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_START) {\n            holder.getBuilder(this).set(CaptureRequest.CONTROL_AF_TRIGGER,\n                    CaptureRequest.CONTROL_AF_TRIGGER_CANCEL);\n            changed = true;\n        }\n\n        if (changed) holder.applyBuilder(this);\n        setState(STATE_COMPLETED);\n    }\n", "target": "focus reset"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/encoding/ByteBufferPool.java:ByteBufferPool:0", "source": "\n\n    CLASSTOKEN(final int bufferSize, int maxPoolSize) {\n        super(maxPoolSize, new Factory<ByteBuffer>() {\n            @Override\n            public ByteBuffer create() {\n                return ByteBuffer.allocateDirect(bufferSize);\n            }\n        });\n    }\n", "target": "byte buffer pool"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/encoding/AudioMediaEncoder.java:AudioMediaEncoder:0", "source": "\n\n    private static final String TAG = CLASSTOKEN.class.getSimpleName();\n    private static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    private static final boolean PERFORMANCE_DEBUG = false;\n    private static final boolean PERFORMANCE_FILL_GAPS = true;\n    private static final int PERFORMANCE_MAX_GAPS = 8;\n\n    private boolean mRequestStop = false;\n    private AudioEncodingThread mEncoder;\n    private AudioRecordingThread mRecorder;\n    private ByteBufferPool mByteBufferPool;\n    private final AudioTimestamp mTimestamp;\n    private AudioConfig mConfig;\n    private InputBufferPool mInputBufferPool = new InputBufferPool();\n    private final LinkedBlockingQueue<InputBuffer> mInputBufferQueue = new LinkedBlockingQueue<>();\n    private AudioNoise mAudioNoise;\n\n    // Just to debug performance.\n    private int mDebugSendCount = 0;\n    private int mDebugExecuteCount = 0;\n    private long mDebugSendAvgDelay = 0;\n    private long mDebugExecuteAvgDelay = 0;\n    private Map<Long, Long> mDebugSendStartMap = new HashMap<>();\n\n    public CLASSTOKEN(@NonNull AudioConfig config) {\n        super(\"AudioEncoder\");\n        mConfig = config.copy();\n        mTimestamp = new AudioTimestamp(mConfig.byteRate());\n        // These two were in onPrepare() but it's better to do warm-up here\n        // since thread and looper creation is expensive.\n        mEncoder = new AudioEncodingThread();\n        mRecorder = new AudioRecordingThread();\n    }\n\n    @EncoderThread\n    @Override\n    protected void onPrepare(@NonNull MediaEncoderEngine.Controller controller, long maxLengthUs) {\n        final MediaFormat audioFormat = MediaFormat.createAudioFormat(\n                mConfig.mimeType,\n                mConfig.samplingFrequency,\n                mConfig.channels);\n        audioFormat.setInteger(MediaFormat.KEY_AAC_PROFILE,\n                MediaCodecInfo.CodecProfileLevel.AACObjectLC);\n        audioFormat.setInteger(MediaFormat.KEY_CHANNEL_MASK, mConfig.audioFormatChannels());\n        audioFormat.setInteger(MediaFormat.KEY_BIT_RATE, mConfig.bitRate);\n        try {\n            if (mConfig.encoder != null) {\n                mMediaCodec = MediaCodec.createByCodecName(mConfig.encoder);\n            } else {\n                mMediaCodec = MediaCodec.createEncoderByType(mConfig.mimeType);\n            }\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n        mMediaCodec.configure(audioFormat, null, null,\n                MediaCodec.CONFIGURE_FLAG_ENCODE);\n        mMediaCodec.start();\n        mByteBufferPool = new ByteBufferPool(mConfig.frameSize(), mConfig.bufferPoolMaxSize());\n        mAudioNoise = new AudioNoise(mConfig);\n    }\n\n    @EncoderThread\n    @Override\n    protected void onStart() {\n        mRequestStop = false;\n        mRecorder.start();\n        mEncoder.start();\n    }\n\n    @EncoderThread\n    @Override\n    protected void onStop() {\n        mRequestStop = true;\n    }\n\n    @Override\n    protected void onStopped() {\n        super.onStopped();\n        mRequestStop = false;\n        mEncoder = null;\n        mRecorder = null;\n        if (mByteBufferPool != null) {\n            mByteBufferPool.clear();\n            mByteBufferPool = null;\n        }\n    }\n\n    @Override\n    protected int getEncodedBitRate() {\n        return mConfig.bitRate;\n    }\n\n    /**\n     * Sleeps for some frames duration, to skip them. This can be used to slow down\n     * the recording operation to balance it with encoding.\n     */\n    private void skipFrames(int frames) {\n        try {\n            Thread.sleep(AudioTimestamp.bytesToMillis(\n                    mConfig.frameSize() * frames,\n                    mConfig.byteRate()));\n        } catch (InterruptedException ignore) {}\n    }\n\n    /**\n     * A thread recording from microphone using {@link AudioRecord} class.\n     * Communicates with {@link AudioEncodingThread} using {@link #mInputBufferQueue}.\n     */\n    private class AudioRecordingThread extends Thread {\n\n        private AudioRecord mAudioRecord;\n        private ByteBuffer mCurrentBuffer;\n        private int mCurrentReadBytes;\n\n        private long mLastTimeUs;\n        private long mFirstTimeUs = Long.MIN_VALUE;\n\n        private AudioRecordingThread() {\n            setPriority(Thread.MAX_PRIORITY);\n            final int minBufferSize = AudioRecord.getMinBufferSize(\n                    mConfig.samplingFrequency,\n                    mConfig.audioFormatChannels(),\n                    mConfig.encoding);\n            // Make this bigger so we don't skip frames. 25: Stereo: 51200. Mono: 25600\n            // 25 is quite big already. Tried to make it bigger to solve the read() delay\n            // but it just makes things worse (ruins MONO as well).\n            // Tried to make it smaller and things change as well.\n            int bufferSize = mConfig.frameSize() * mConfig.audioRecordBufferFrames();\n            while (bufferSize < minBufferSize) {\n                bufferSize += mConfig.frameSize(); // Unlikely.\n            }\n            mAudioRecord = new AudioRecord(MediaRecorder.AudioSource.CAMCORDER,\n                    mConfig.samplingFrequency,\n                    mConfig.audioFormatChannels(),\n                    mConfig.encoding,\n                    bufferSize);\n        }\n\n        @Override\n        public void run() {\n            mAudioRecord.startRecording();\n            while (!mRequestStop) {\n                if (!hasReachedMaxLength()) {\n                    read(false);\n                } else {\n                    // We have reached the max length, so stop reading.\n                    // However, do not get out of the loop - the controller\n                    // will call stop() on us soon. It's not our responsibility\n                    // to stop ourselves.\n                    //noinspection UnnecessaryContinue\n                    continue;\n                }\n            }\n            LOG.w(\"Stop was requested. We're out of the loop. Will post an endOfStream.\");\n            // Last input with 0 length. This will signal the endOfStream.\n            // Can't use drain(true); it is only available when writing to the codec InputSurface.\n            boolean didReadEos = false;\n            while (!didReadEos) {\n                didReadEos = read(true);\n            }\n            mAudioRecord.stop();\n            mAudioRecord.release();\n            mAudioRecord = null;\n        }\n\n        /**\n         * Returns true if we found a buffer and could proceed, false if we found no buffer\n         * so the operation should be performed again by the caller.\n         * @param endOfStream true if last read\n         * @return true if proceeded\n         */\n        private boolean read(boolean endOfStream) {\n            mCurrentBuffer = mByteBufferPool.get();\n            if (mCurrentBuffer == null) {\n                // This can happen and it means that encoding is slow with respect to recording.\n                // One might be tempted to fix precisely the next frame presentation time when\n                // this happens, but this is not needed because the current increaseTime()\n                // algorithm will consider delays when they get large.\n                // Sleeping before returning is a good way of balancing the two operations.\n                // However, if endOfStream, we CAN'T lose this frame!\n                if (endOfStream) {\n                    LOG.v(\"read thread - eos: true - No buffer, retrying.\");\n                } else {\n                    LOG.w(\"read thread - eos: false - Skipping audio frame,\",\n                            \"encoding is too slow.\");\n                    skipFrames(6); // sleep a bit\n                }\n                return false;\n            } else {\n                mCurrentBuffer.clear();\n                // When stereo, we read twice the data here and AudioRecord will fill the buffer\n                // with left and right bytes. https://stackoverflow.com/q/20594750/4288782\n                if (PERFORMANCE_DEBUG) {\n                    long before = System.nanoTime();\n                    mCurrentReadBytes = mAudioRecord.read(mCurrentBuffer, mConfig.frameSize());\n                    long after = System.nanoTime();\n                    float delayMillis = (after - before) / 1000000F;\n                    float durationMillis = AudioTimestamp.bytesToMillis(mCurrentReadBytes,\n                            mConfig.byteRate());\n                    LOG.v(\"read thread - reading took:\", delayMillis,\n                            \"should be:\", durationMillis,\n                            \"delay:\", delayMillis - durationMillis);\n                } else {\n                    mCurrentReadBytes = mAudioRecord.read(mCurrentBuffer, mConfig.frameSize());\n                }\n                LOG.v(\"read thread - eos:\", endOfStream, \"- Read new audio frame. Bytes:\",\n                        mCurrentReadBytes);\n                if (mCurrentReadBytes > 0) { // Good read: increase PTS.\n                    increaseTime(mCurrentReadBytes, endOfStream);\n                    LOG.v(\"read thread - eos:\", endOfStream, \"- mLastTimeUs:\", mLastTimeUs);\n                    mCurrentBuffer.limit(mCurrentReadBytes);\n                    enqueue(mCurrentBuffer, mLastTimeUs, endOfStream);\n                } else if (mCurrentReadBytes == AudioRecord.ERROR_INVALID_OPERATION) {\n                    LOG.e(\"read thread - eos:\", endOfStream,\n                            \"- Got AudioRecord.ERROR_INVALID_OPERATION\");\n                } else if (mCurrentReadBytes == AudioRecord.ERROR_BAD_VALUE) {\n                    LOG.e(\"read thread - eos:\", endOfStream,\n                            \"- Got AudioRecord.ERROR_BAD_VALUE\");\n                }\n                return true;\n            }\n        }\n\n        /**\n         * Increases presentation time and checks for max length constraint. This is much faster\n         * then waiting for the encoder to check it during {@link #drainOutput(boolean)}. We\n         * want to catch this as soon as possible so we stop recording useless frames and bother\n         * all the threads involved.\n         * @param readBytes bytes read in last reading\n         * @param endOfStream end of stream?\n         */\n        private void increaseTime(int readBytes, boolean endOfStream) {\n            // Get the latest frame timestamp.\n            mLastTimeUs = mTimestamp.increaseUs(readBytes);\n            if (mFirstTimeUs == Long.MIN_VALUE) {\n                mFirstTimeUs = mLastTimeUs;\n                // Compute the first frame milliseconds as well.\n                notifyFirstFrameMillis(System.currentTimeMillis()\n                        - AudioTimestamp.bytesToMillis(readBytes, mConfig.byteRate()));\n            }\n\n            // See if we reached the max length value.\n            if (!hasReachedMaxLength()) {\n                boolean didReachMaxLength = (mLastTimeUs - mFirstTimeUs) > getMaxLengthUs();\n                if (didReachMaxLength && !endOfStream) {\n                    LOG.w(\"read thread - this frame reached the maxLength! deltaUs:\",\n                            mLastTimeUs - mFirstTimeUs);\n                    notifyMaxLengthReached();\n                }\n            }\n\n            // Maybe add noise.\n            maybeAddNoise();\n        }\n\n        private void enqueue(@NonNull ByteBuffer byteBuffer,\n                             long timestamp,\n                             boolean isEndOfStream) {\n            if (PERFORMANCE_DEBUG) {\n                mDebugSendStartMap.put(timestamp, System.nanoTime() / 1000000);\n            }\n            int readBytes = byteBuffer.remaining();\n            InputBuffer inputBuffer = mInputBufferPool.get();\n            //noinspection ConstantConditions\n            inputBuffer.source = byteBuffer;\n            inputBuffer.timestamp = timestamp;\n            inputBuffer.length = readBytes;\n            inputBuffer.isEndOfStream = isEndOfStream;\n            mInputBufferQueue.add(inputBuffer);\n        }\n\n        /**\n         * If our {@link AudioTimestamp} detected huge gap, and the performance flag is enabled,\n         * we can add noise to fill them.\n         *\n         * Even if we always pass the correct timestamps, if there are big gaps between the frames,\n         * the encoder implementation might shrink all timestamps to have a continuous audio.\n         * This results in a video that is fast-forwarded.\n         *\n         * Adding noise does not solve the gaps issue, we'll still have distorted audio, but\n         * at least we get a video that has the correct playback speed.\n         *\n         * NOTE: this MUST be fast!\n         * If this operation is slow, we make the {@link AudioRecordingThread} busy, so we'll\n         * read the next frame with a delay, so we'll have even more gaps at the next call\n         * and spend even more time here. The result might be recording no audio at all - just\n         * random noise.\n         * This is the reason why we have a {@link #PERFORMANCE_MAX_GAPS} number.\n         */\n        private void maybeAddNoise() {\n            if (!PERFORMANCE_FILL_GAPS) return;\n            int gaps = mTimestamp.getGapCount(mConfig.frameSize());\n            if (gaps <= 0) return;\n\n            long gapStart = mTimestamp.getGapStartUs(mLastTimeUs);\n            long frameUs = AudioTimestamp.bytesToUs(mConfig.frameSize(), mConfig.byteRate());\n            LOG.w(\"read thread - GAPS: trying to add\", gaps,\n                    \"noise buffers. PERFORMANCE_MAX_GAPS:\", PERFORMANCE_MAX_GAPS);\n            for (int i = 0; i < Math.min(gaps, PERFORMANCE_MAX_GAPS); i++) {\n                ByteBuffer noiseBuffer = mByteBufferPool.get();\n                if (noiseBuffer == null) {\n                    LOG.e(\"read thread - GAPS: aborting because we have no free buffer.\");\n                    break;\n                }\n                noiseBuffer.clear();\n                mAudioNoise.fill(noiseBuffer);\n                noiseBuffer.rewind();\n                enqueue(noiseBuffer, gapStart, false);\n                gapStart += frameUs;\n            }\n        }\n    }\n\n    /**\n     * A thread encoding the microphone data using the media encoder APIs.\n     * Communicates with {@link AudioRecordingThread} using {@link #mInputBufferQueue}.\n     *\n     * We want to do this operation on a different thread than the recording one (to avoid\n     * losing frames while we're working here), and different than the {@link MediaEncoder}\n     * own thread (we want that to be reactive - stop() must become onStop() soon).\n     */\n    private class AudioEncodingThread extends Thread {\n        private AudioEncodingThread() {\n            // Not sure about this... This thread can do VERY time consuming operations,\n            // and slowing down the preview/camera threads can break them e.g. hit internal\n            // timeouts for camera requests to be consumed.\n            // setPriority(Thread.MAX_PRIORITY);\n        }\n\n        @Override\n        public void run() {\n            encoding: while (true) {\n                if (mInputBufferQueue.isEmpty()) {\n                    skipFrames(3);\n                } else {\n                    LOG.v(\"encoding thread - performing\", mInputBufferQueue.size(),\n                            \"pending operations.\");\n                    InputBuffer inputBuffer;\n                    while ((inputBuffer = mInputBufferQueue.peek()) != null) {\n\n                        // Performance logging\n                        if (PERFORMANCE_DEBUG) {\n                            long sendEnd = System.nanoTime() / 1000000;\n                            Long sendStart = mDebugSendStartMap.remove(inputBuffer.timestamp);\n                            //noinspection StatementWithEmptyBody\n                            if (sendStart != null) {\n                                mDebugSendAvgDelay = ((mDebugSendAvgDelay * mDebugSendCount)\n                                        + (sendEnd - sendStart)) / (++mDebugSendCount);\n                                LOG.v(\"send delay millis:\", sendEnd - sendStart,\n                                        \"average:\", mDebugSendAvgDelay);\n                            } else {\n                                // This input buffer was already processed\n                                // (but tryAcquire failed for now).\n                            }\n                        }\n\n                        // Actual work\n                        if (inputBuffer.isEndOfStream) {\n                            acquireInputBuffer(inputBuffer);\n                            encode(inputBuffer);\n                            break encoding;\n                        } else if (tryAcquireInputBuffer(inputBuffer)) {\n                            encode(inputBuffer);\n                        } else {\n                            skipFrames(3);\n                        }\n                    }\n                }\n            }\n            // We got an end of stream.\n            mInputBufferPool.clear();\n            if (PERFORMANCE_DEBUG) {\n                // After latest changes, the count here is not so different between MONO and STEREO.\n                // We get about 400 frames in both cases (430 for MONO, but doesn't seem like\n                // a big issue).\n                LOG.e(\"EXECUTE DELAY MILLIS:\", mDebugExecuteAvgDelay,\n                        \"COUNT:\", mDebugExecuteCount);\n                LOG.e(\"SEND DELAY MILLIS:\", mDebugSendAvgDelay,\n                        \"COUNT:\", mDebugSendCount);\n            }\n        }\n\n        private void encode(@NonNull InputBuffer buffer) {\n            long executeStart = System.nanoTime() / 1000000;\n\n            LOG.v(\"encoding thread - performing pending operation for timestamp:\",\n                    buffer.timestamp, \"- encoding.\");\n            // NOTE: this copy is prob. the worst part here for performance\n            buffer.data.put(buffer.source);\n            mByteBufferPool.recycle(buffer.source);\n            mInputBufferQueue.remove(buffer);\n            encodeInputBuffer(buffer);\n            boolean eos = buffer.isEndOfStream;\n            mInputBufferPool.recycle(buffer);\n            LOG.v(\"encoding thread - performing pending operation for timestamp:\",\n                    buffer.timestamp, \"- draining.\");\n            // NOTE: can consider calling this drainOutput on yet another thread, which would let us\n            // use an even smaller BUFFER_POOL_MAX_SIZE without losing audio frames. But this way\n            // we can accumulate delay on this new thread without noticing (no pool getting empty).\n            drainOutput(eos);\n\n            if (PERFORMANCE_DEBUG) {\n                long executeEnd = System.nanoTime() / 1000000;\n                mDebugExecuteAvgDelay = ((mDebugExecuteAvgDelay * mDebugExecuteCount)\n                        + (executeEnd - executeStart)) / (++mDebugExecuteCount);\n                LOG.v(\"execute delay millis:\", executeEnd - executeStart,\n                        \"average:\", mDebugExecuteAvgDelay);\n            }\n        }\n    }\n", "target": "audio media encoder"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/encoding/AudioMediaEncoder.java:AudioEncodingThread:2", "source": "\n        private CLASSTOKEN() {\n            // Not sure about this... This thread can do VERY time consuming operations,\n            // and slowing down the preview/camera threads can break them e.g. hit internal\n            // timeouts for camera requests to be consumed.\n            // setPriority(Thread.MAX_PRIORITY);\n        }\n\n        @Override\n        public void run() {\n            encoding: while (true) {\n                if (mInputBufferQueue.isEmpty()) {\n                    skipFrames(3);\n                } else {\n                    LOG.v(\"encoding thread - performing\", mInputBufferQueue.size(),\n                            \"pending operations.\");\n                    InputBuffer inputBuffer;\n                    while ((inputBuffer = mInputBufferQueue.peek()) != null) {\n\n                        // Performance logging\n                        if (PERFORMANCE_DEBUG) {\n                            long sendEnd = System.nanoTime() / 1000000;\n                            Long sendStart = mDebugSendStartMap.remove(inputBuffer.timestamp);\n                            //noinspection StatementWithEmptyBody\n                            if (sendStart != null) {\n                                mDebugSendAvgDelay = ((mDebugSendAvgDelay * mDebugSendCount)\n                                        + (sendEnd - sendStart)) / (++mDebugSendCount);\n                                LOG.v(\"send delay millis:\", sendEnd - sendStart,\n                                        \"average:\", mDebugSendAvgDelay);\n                            } else {\n                                // This input buffer was already processed\n                                // (but tryAcquire failed for now).\n                            }\n                        }\n\n                        // Actual work\n                        if (inputBuffer.isEndOfStream) {\n                            acquireInputBuffer(inputBuffer);\n                            encode(inputBuffer);\n                            break encoding;\n                        } else if (tryAcquireInputBuffer(inputBuffer)) {\n                            encode(inputBuffer);\n                        } else {\n                            skipFrames(3);\n                        }\n                    }\n                }\n            }\n            // We got an end of stream.\n            mInputBufferPool.clear();\n            if (PERFORMANCE_DEBUG) {\n                // After latest changes, the count here is not so different between MONO and STEREO.\n                // We get about 400 frames in both cases (430 for MONO, but doesn't seem like\n                // a big issue).\n                LOG.e(\"EXECUTE DELAY MILLIS:\", mDebugExecuteAvgDelay,\n                        \"COUNT:\", mDebugExecuteCount);\n                LOG.e(\"SEND DELAY MILLIS:\", mDebugSendAvgDelay,\n                        \"COUNT:\", mDebugSendCount);\n            }\n        }\n\n        private void encode(@NonNull InputBuffer buffer) {\n            long executeStart = System.nanoTime() / 1000000;\n\n            LOG.v(\"encoding thread - performing pending operation for timestamp:\",\n                    buffer.timestamp, \"- encoding.\");\n            // NOTE: this copy is prob. the worst part here for performance\n            buffer.data.put(buffer.source);\n            mByteBufferPool.recycle(buffer.source);\n            mInputBufferQueue.remove(buffer);\n            encodeInputBuffer(buffer);\n            boolean eos = buffer.isEndOfStream;\n            mInputBufferPool.recycle(buffer);\n            LOG.v(\"encoding thread - performing pending operation for timestamp:\",\n                    buffer.timestamp, \"- draining.\");\n            // NOTE: can consider calling this drainOutput on yet another thread, which would let us\n            // use an even smaller BUFFER_POOL_MAX_SIZE without losing audio frames. But this way\n            // we can accumulate delay on this new thread without noticing (no pool getting empty).\n            drainOutput(eos);\n\n            if (PERFORMANCE_DEBUG) {\n                long executeEnd = System.nanoTime() / 1000000;\n                mDebugExecuteAvgDelay = ((mDebugExecuteAvgDelay * mDebugExecuteCount)\n                        + (executeEnd - executeStart)) / (++mDebugExecuteCount);\n                LOG.v(\"execute delay millis:\", executeEnd - executeStart,\n                        \"average:\", mDebugExecuteAvgDelay);\n            }\n        }\n    ", "target": "audio encoding thread"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/CameraView.java:CameraCallbacks:1", "source": "\n\n        private final String TAG = CLASSTOKEN.class.getSimpleName();\n        private final CameraLogger LOG = CameraLogger.create(TAG);\n\n        @NonNull\n        @Override\n        public Context getContext() {\n            return CameraView.this.getContext();\n        }\n\n        @Override\n        public int getWidth() {\n            return CameraView.this.getWidth();\n        }\n\n        @Override\n        public int getHeight() {\n            return CameraView.this.getHeight();\n        }\n\n        @Override\n        public void dispatchOnCameraOpened(@NonNull final CameraOptions options) {\n            LOG.i(\"dispatchOnCameraOpened\", options);\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    for (CameraListener listener : mListeners) {\n                        listener.onCameraOpened(options);\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void dispatchOnCameraClosed() {\n            LOG.i(\"dispatchOnCameraClosed\");\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    for (CameraListener listener : mListeners) {\n                        listener.onCameraClosed();\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void onCameraPreviewStreamSizeChanged() {\n            // Camera preview size has changed.\n            // Request a layout pass for onMeasure() to do its stuff.\n            // Potentially this will change CameraView size, which changes Surface size,\n            // which triggers a new Preview size. But hopefully it will converge.\n            Size previewSize = mCameraEngine.getPreviewStreamSize(Reference.VIEW);\n            if (previewSize == null) {\n                throw new RuntimeException(\"Preview stream size should not be null here.\");\n            } else if (previewSize.equals(mLastPreviewStreamSize)) {\n                LOG.i(\"onCameraPreviewStreamSizeChanged:\",\n                        \"swallowing because the preview size has not changed.\", previewSize);\n            } else {\n                LOG.i(\"onCameraPreviewStreamSizeChanged: posting a requestLayout call.\",\n                        \"Preview stream size:\", previewSize);\n                mUiHandler.post(new Runnable() {\n                    @Override\n                    public void run() {\n                        requestLayout();\n                    }\n                });\n            }\n        }\n\n        @Override\n        public void dispatchOnPictureShutter(boolean shouldPlaySound) {\n            if (shouldPlaySound && mPlaySounds) {\n                playSound(MediaActionSound.SHUTTER_CLICK);\n            }\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    for (CameraListener listener : mListeners) {\n                        listener.onPictureShutter();\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void dispatchOnPictureTaken(@NonNull final PictureResult.Stub stub) {\n            LOG.i(\"dispatchOnPictureTaken\", stub);\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    PictureResult result = new PictureResult(stub);\n                    for (CameraListener listener : mListeners) {\n                        listener.onPictureTaken(result);\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void dispatchOnVideoTaken(@NonNull final VideoResult.Stub stub) {\n            LOG.i(\"dispatchOnVideoTaken\", stub);\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    VideoResult result = new VideoResult(stub);\n                    for (CameraListener listener : mListeners) {\n                        listener.onVideoTaken(result);\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void dispatchOnFocusStart(@Nullable final Gesture gesture,\n                                         @NonNull final PointF point) {\n            LOG.i(\"dispatchOnFocusStart\", gesture, point);\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    mMarkerLayout.onEvent(MarkerLayout.TYPE_AUTOFOCUS, new PointF[]{ point });\n                    if (mAutoFocusMarker != null) {\n                        AutoFocusTrigger trigger = gesture != null ?\n                                AutoFocusTrigger.GESTURE : AutoFocusTrigger.METHOD;\n                        mAutoFocusMarker.onAutoFocusStart(trigger, point);\n                    }\n\n                    for (CameraListener listener : mListeners) {\n                        listener.onAutoFocusStart(point);\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void dispatchOnFocusEnd(@Nullable final Gesture gesture,\n                                       final boolean success,\n                                       @NonNull final PointF point) {\n            LOG.i(\"dispatchOnFocusEnd\", gesture, success, point);\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    if (success && mPlaySounds) {\n                        playSound(MediaActionSound.FOCUS_COMPLETE);\n                    }\n\n                    if (mAutoFocusMarker != null) {\n                        AutoFocusTrigger trigger = gesture != null ?\n                                AutoFocusTrigger.GESTURE : AutoFocusTrigger.METHOD;\n                        mAutoFocusMarker.onAutoFocusEnd(trigger, success, point);\n                    }\n\n                    for (CameraListener listener : mListeners) {\n                        listener.onAutoFocusEnd(success, point);\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void onDeviceOrientationChanged(int deviceOrientation) {\n            LOG.i(\"onDeviceOrientationChanged\", deviceOrientation);\n            int displayOffset = mOrientationHelper.getLastDisplayOffset();\n            if (!mUseDeviceOrientation) {\n                // To fool the engine to return outputs in the VIEW reference system,\n                // The device orientation should be set to -displayOffset.\n                int fakeDeviceOrientation = (360 - displayOffset) % 360;\n                mCameraEngine.getAngles().setDeviceOrientation(fakeDeviceOrientation);\n            } else {\n                mCameraEngine.getAngles().setDeviceOrientation(deviceOrientation);\n            }\n            final int value = (deviceOrientation + displayOffset) % 360;\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    for (CameraListener listener : mListeners) {\n                        listener.onOrientationChanged(value);\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void onDisplayOffsetChanged() {\n            if (isOpened()) {\n                // We can't handle display offset (View angle) changes without restarting.\n                // See comments in OrientationHelper for more information.\n                LOG.w(\"onDisplayOffsetChanged\", \"restarting the camera.\");\n                close();\n                open();\n            }\n        }\n\n        @Override\n        public void dispatchOnZoomChanged(final float newValue, @Nullable final PointF[] fingers) {\n            LOG.i(\"dispatchOnZoomChanged\", newValue);\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    for (CameraListener listener : mListeners) {\n                        listener.onZoomChanged(newValue, new float[]{0, 1}, fingers);\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void dispatchOnExposureCorrectionChanged(final float newValue,\n                                                        @NonNull final float[] bounds,\n                                                        @Nullable final PointF[] fingers) {\n            LOG.i(\"dispatchOnExposureCorrectionChanged\", newValue);\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    for (CameraListener listener : mListeners) {\n                        listener.onExposureCorrectionChanged(newValue, bounds, fingers);\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void dispatchFrame(@NonNull final Frame frame) {\n            // The getTime() below might crash if developers incorrectly release\n            // frames asynchronously.\n            LOG.v(\"dispatchFrame:\", frame.getTime(), \"processors:\", mFrameProcessors.size());\n            if (mFrameProcessors.isEmpty()) {\n                // Mark as released. This instance will be reused.\n                frame.release();\n            } else {\n                // Dispatch this frame to frame processors.\n                mFrameProcessingExecutor.execute(new Runnable() {\n                    @Override\n                    public void run() {\n                        LOG.v(\"dispatchFrame: executing. Passing\", frame.getTime(),\n                                \"to processors.\");\n                        for (FrameProcessor processor : mFrameProcessors) {\n                            try {\n                                processor.process(frame);\n                            } catch (Exception e) {\n                                LOG.w(\"Frame processor crashed:\", e);\n                            }\n                        }\n                        frame.release();\n                    }\n                });\n            }\n        }\n\n        @Override\n        public void dispatchError(final CameraException exception) {\n            LOG.i(\"dispatchError\", exception);\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    for (CameraListener listener : mListeners) {\n                        listener.onCameraError(exception);\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void dispatchOnVideoRecordingStart() {\n            LOG.i(\"dispatchOnVideoRecordingStart\");\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    for (CameraListener listener : mListeners) {\n                        listener.onVideoRecordingStart();\n                    }\n                }\n            });\n        }\n\n        @Override\n        public void dispatchOnVideoRecordingEnd() {\n            LOG.i(\"dispatchOnVideoRecordingEnd\");\n            mUiHandler.post(new Runnable() {\n                @Override\n                public void run() {\n                    for (CameraListener listener : mListeners) {\n                        listener.onVideoRecordingEnd();\n                    }\n                }\n            });\n        }\n    ", "target": "camera callbacks"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/action/BaseAction.java:BaseAction:0", "source": "\n\n    private final List<ActionCallback> callbacks = new ArrayList<>();\n    private int state;\n    private ActionHolder holder;\n    private boolean needsOnStart;\n\n    @Override\n    public final int getState() {\n        return state;\n    }\n\n    @Override\n    public final void start(@NonNull ActionHolder holder) {\n        this.holder = holder;\n        holder.addAction(this);\n        if (holder.getLastResult(this) != null) {\n            onStart(holder);\n        } else {\n            needsOnStart = true;\n        }\n    }\n\n    @Override\n    public final void abort(@NonNull ActionHolder holder) {\n        holder.removeAction(this);\n        if (!isCompleted()) {\n            onAbort(holder);\n            setState(STATE_COMPLETED);\n        }\n        needsOnStart = false;\n    }\n\n    /**\n     * Action was started and will soon receive events from the\n     * holder stream.\n     * @param holder holder\n     */\n    @CallSuper\n    protected void onStart(@NonNull ActionHolder holder) {\n        // Repeating holder assignment here (already in start()) because we NEED it in start()\n        // but some special actions will not call start() at all for their children.\n        this.holder = holder;\n        // Overrideable\n    }\n\n    /**\n     * Action was aborted and will not receive events from the\n     * holder stream anymore. It will soon be marked as completed.\n     * @param holder holder\n     */\n    @SuppressWarnings(\"unused\")\n    protected void onAbort(@NonNull ActionHolder holder) {\n        // Overrideable\n    }\n\n    @CallSuper\n    @Override\n    public void onCaptureStarted(@NonNull ActionHolder holder, @NonNull CaptureRequest request) {\n        if (needsOnStart) {\n            onStart(holder);\n            needsOnStart = false;\n        }\n    }\n\n    @Override\n    public void onCaptureProgressed(@NonNull ActionHolder holder,\n                                    @NonNull CaptureRequest request,\n                                    @NonNull CaptureResult result) {\n        // Overrideable\n    }\n\n    @Override\n    public void onCaptureCompleted(@NonNull ActionHolder holder,\n                                   @NonNull CaptureRequest request,\n                                   @NonNull TotalCaptureResult result) {\n        // Overrideable\n    }\n\n    /**\n     * Called by subclasses to notify of their state. If state is {@link #STATE_COMPLETED},\n     * this removes this action from the holder.\n     * @param newState new state\n     */\n    protected final void setState(int newState) {\n        if (newState != state) {\n            state = newState;\n            for (ActionCallback callback : callbacks) {\n                callback.onActionStateChanged(this, state);\n            }\n            if (state == STATE_COMPLETED) {\n                holder.removeAction(this);\n                onCompleted(holder);\n            }\n        }\n    }\n\n    /**\n     * Whether this action has reached the completed state.\n     * @return true if completed\n     */\n    public boolean isCompleted() {\n        return state == STATE_COMPLETED;\n    }\n\n    /**\n     * Called when this action has completed (possibly aborted).\n     * @param holder holder\n     */\n    protected void onCompleted(@NonNull ActionHolder holder) {\n        // Overrideable\n    }\n\n    /**\n     * Returns the holder.\n     * @return the holder\n     */\n    @NonNull\n    protected ActionHolder getHolder() {\n        return holder;\n    }\n\n\n    /**\n     * Reads a characteristic with a fallback.\n     * @param key key\n     * @param fallback fallback\n     * @param <T> key type\n     * @return value or fallback\n     */\n    @NonNull\n    protected <T> T readCharacteristic(@NonNull CameraCharacteristics.Key<T> key,\n                                       @NonNull T fallback) {\n        T value = holder.getCharacteristics(this).get(key);\n        return value == null ? fallback : value;\n    }\n\n    @Override\n    public void addCallback(@NonNull ActionCallback callback) {\n        if (!callbacks.contains(callback)) {\n            callbacks.add(callback);\n            callback.onActionStateChanged(this, getState());\n        }\n    }\n\n    @Override\n    public void removeCallback(@NonNull ActionCallback callback) {\n        callbacks.remove(callback);\n    }\n", "target": "base action"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/encoding/MediaEncoder.java:MediaEncoder:0", "source": "\n\n    private final static String TAG = CLASSTOKEN.class.getSimpleName();\n    private final static CameraLogger LOG = CameraLogger.create(TAG);\n\n    // Did some test to see which value would maximize our performance in the current setup\n    // (infinite audio pool). Measured the time it would take to write a 30 seconds video.\n    // Based on this, we'll go with TIMEOUT=0 for now.\n    // INPUT_TIMEOUT_US 10000: 46 seconds\n    // INPUT_TIMEOUT_US 1000: 37 seconds\n    // INPUT_TIMEOUT_US 100: 33 seconds\n    // INPUT_TIMEOUT_US 0: 32 seconds\n    private final static int INPUT_TIMEOUT_US = 0;\n\n    // 0 also seems to be the best, although it does not change so much.\n    // Can't go too high or this is a bottleneck for the audio encoder.\n    private final static int OUTPUT_TIMEOUT_US = 0;\n\n    private final static int STATE_NONE = 0;\n    private final static int STATE_PREPARING = 1;\n    private final static int STATE_PREPARED = 2;\n    private final static int STATE_STARTING = 3;\n    private final static int STATE_STARTED = 4;\n    // max timestamp was reached. we will keep draining but have asked the engine to stop us.\n    // this step can be skipped in case stop() is called from outside before a limit is reached.\n    private final static int STATE_LIMIT_REACHED = 5;\n    private final static int STATE_STOPPING = 6;\n    private final static int STATE_STOPPED = 7;\n\n    private int mState = STATE_NONE;\n    private final String mName;\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected MediaCodec mMediaCodec;\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected WorkerHandler mWorker;\n\n    private MediaEncoderEngine.Controller mController;\n    private int mTrackIndex;\n    private OutputBufferPool mOutputBufferPool;\n    private MediaCodec.BufferInfo mBufferInfo;\n    private MediaCodecBuffers mBuffers;\n    private final Map<String, AtomicInteger> mPendingEvents = new HashMap<>();\n\n    private long mMaxLengthUs;\n    private boolean mMaxLengthReached;\n\n    private long mStartTimeMillis = 0; // In System.currentTimeMillis()\n    private long mFirstTimeUs = Long.MIN_VALUE; // In unknown reference\n    private long mLastTimeUs = 0;\n\n    private long mDebugSetStateTimestamp = Long.MIN_VALUE;\n\n    /**\n     * Needs a readable name for the thread and for logging.\n     * @param name a name\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    protected CLASSTOKEN(@NonNull String name) {\n        mName = name;\n    }\n\n    private void setState(int newState) {\n        if (mDebugSetStateTimestamp == Long.MIN_VALUE) {\n            mDebugSetStateTimestamp = System.currentTimeMillis();\n        }\n        long millis = System.currentTimeMillis() - mDebugSetStateTimestamp;\n        mDebugSetStateTimestamp = System.currentTimeMillis();\n\n        String newStateName = null;\n        switch (newState) {\n            case STATE_NONE: newStateName = \"NONE\"; break;\n            case STATE_PREPARING: newStateName = \"PREPARING\"; break;\n            case STATE_PREPARED: newStateName = \"PREPARED\"; break;\n            case STATE_STARTING: newStateName = \"STARTING\"; break;\n            case STATE_STARTED: newStateName = \"STARTED\"; break;\n            case STATE_LIMIT_REACHED: newStateName = \"LIMIT_REACHED\"; break;\n            case STATE_STOPPING: newStateName = \"STOPPING\"; break;\n            case STATE_STOPPED: newStateName = \"STOPPED\"; break;\n        }\n        LOG.w(mName, \"setState:\", newStateName, \"millisSinceLastState:\", millis);\n        mState = newState;\n    }\n\n    /**\n     * This encoder was attached to the engine. Keep the controller\n     * and run the internal thread.\n     *\n     * NOTE: it's important to call {@link WorkerHandler#post(Runnable)} instead of run()!\n     * The internal actions can cause a stop, and due to how {@link WorkerHandler#run(Runnable)}\n     * works, we might have {@link #onStop()} or {@link #onStopped()} to be executed before\n     * the previous step has completed.\n     */\n    final void prepare(@NonNull final MediaEncoderEngine.Controller controller,\n                       final long maxLengthUs) {\n        if (mState >= STATE_PREPARING) {\n            LOG.e(mName, \"Wrong state while preparing. Aborting.\", mState);\n            return;\n        }\n        mController = controller;\n        mBufferInfo = new MediaCodec.BufferInfo();\n        mMaxLengthUs = maxLengthUs;\n        mWorker = WorkerHandler.get(mName);\n        mWorker.getThread().setPriority(Thread.MAX_PRIORITY);\n        LOG.i(mName, \"Prepare was called. Posting.\");\n        mWorker.post(new Runnable() {\n            @Override\n            public void run() {\n                LOG.i(mName, \"Prepare was called. Executing.\");\n                setState(STATE_PREPARING);\n                onPrepare(controller, maxLengthUs);\n                setState(STATE_PREPARED);\n            }\n        });\n    }\n\n    /**\n     * Start recording. This might be a lightweight operation\n     * in case the encoder needs to wait for a certain event\n     * like a \"frame available\".\n     *\n     * The {@link #STATE_STARTED} state will be set when draining for the\n     * first time (not when onStart ends).\n     *\n     * NOTE: it's important to call {@link WorkerHandler#post(Runnable)} instead of run()!\n     */\n    final void start() {\n        LOG.w(mName, \"Start was called. Posting.\");\n        mWorker.post(new Runnable() {\n            @Override\n            public void run() {\n                if (mState < STATE_PREPARED || mState >= STATE_STARTING) {\n                    LOG.e(mName, \"Wrong state while starting. Aborting.\", mState);\n                    return;\n                }\n                setState(STATE_STARTING);\n                LOG.w(mName, \"Start was called. Executing.\");\n                onStart();\n            }\n        });\n    }\n\n    /**\n     * The caller notifying of a certain event occurring.\n     * Should analyze the string and see if the event is important.\n     *\n     * NOTE: it's important to call {@link WorkerHandler#post(Runnable)} instead of run()!\n     *\n     * @param event what happened\n     * @param data object\n     */\n    @SuppressWarnings(\"ConstantConditions\")\n    final void notify(final @NonNull String event, final @Nullable Object data) {\n        if (!mPendingEvents.containsKey(event)) mPendingEvents.put(event,\n                 new AtomicInteger(0));\n        final AtomicInteger pendingEvents = mPendingEvents.get(event);\n        pendingEvents.incrementAndGet();\n        LOG.v(mName, \"Notify was called. Posting. pendingEvents:\", pendingEvents.intValue());\n        mWorker.post(new Runnable() {\n            @Override\n            public void run() {\n                LOG.v(mName, \"Notify was called. Executing. pendingEvents:\",\n                        pendingEvents.intValue());\n                onEvent(event, data);\n                pendingEvents.decrementAndGet();\n            }\n        });\n    }\n\n    /**\n     * Stop recording. This involves signaling the end of stream and draining\n     * all output left.\n     *\n     * The {@link #STATE_STOPPED} state will be set when draining for the\n     * last time (not when onStart ends).\n     *\n     * NOTE: it's important to call {@link WorkerHandler#post(Runnable)} instead of run()!\n     */\n    final void stop() {\n        if (mState >= STATE_STOPPING) {\n            LOG.e(mName, \"Wrong state while stopping. Aborting.\", mState);\n            return;\n        }\n        setState(STATE_STOPPING);\n        LOG.w(mName, \"Stop was called. Posting.\");\n        mWorker.post(new Runnable() {\n            @Override\n            public void run() {\n                LOG.w(mName, \"Stop was called. Executing.\");\n                onStop();\n            }\n        });\n    }\n\n    /**\n     * Called to prepare this encoder before starting.\n     * Any initialization should be done here as it does not interfere with the original\n     * thread (that, generally, is the rendering thread).\n     *\n     * At this point subclasses MUST create the {@link #mMediaCodec} object.\n     *\n     * @param controller the muxer controller\n     * @param maxLengthUs the maxLength in microseconds\n     */\n    @EncoderThread\n    protected abstract void onPrepare(@NonNull MediaEncoderEngine.Controller controller,\n                                      long maxLengthUs);\n\n    /**\n     * Start recording. This might be a lightweight operation\n     * in case the encoder needs to wait for a certain event\n     * like a \"frame available\".\n     */\n    @EncoderThread\n    protected abstract void onStart();\n\n    /**\n     * The caller notifying of a certain event occurring.\n     * Should analyze the string and see if the event is important.\n     * @param event what happened\n     * @param data object\n     */\n    @EncoderThread\n    protected void onEvent(@NonNull String event, @Nullable Object data) {}\n\n    /**\n     * Stop recording. This involves signaling the end of stream and draining\n     * all output left.\n     */\n    @EncoderThread\n    protected abstract void onStop();\n\n    /**\n     * Called by {@link #drainOutput(boolean)} when we get an EOS signal (not necessarily in the\n     * parameters, might also be through an input buffer flag).\n     *\n     * This is a good moment to release all resources, although the muxer might still\n     * be alive (we wait for the other Encoder, see MediaEncoderEngine.Controller).\n     */\n    @CallSuper\n    protected void onStopped() {\n        LOG.w(mName, \"is being released. Notifying controller and releasing codecs.\");\n        // TODO should we call notifyStopped after this method ends?\n        mController.notifyStopped(mTrackIndex);\n        mMediaCodec.stop();\n        mMediaCodec.release();\n        mMediaCodec = null;\n        mOutputBufferPool.clear();\n        mOutputBufferPool = null;\n        mBuffers = null;\n        setState(STATE_STOPPED);\n        mWorker.destroy();\n    }\n\n    /**\n     * Returns a new input buffer and index, waiting at most {@link #INPUT_TIMEOUT_US} if none\n     * is available. Callers should check the boolean result - true if the buffer was filled.\n     *\n     * @param holder the input buffer holder\n     * @return true if acquired\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    protected boolean tryAcquireInputBuffer(@NonNull InputBuffer holder) {\n        if (mBuffers == null) {\n            mBuffers = new MediaCodecBuffers(mMediaCodec);\n        }\n        int inputBufferIndex = mMediaCodec.dequeueInputBuffer(INPUT_TIMEOUT_US);\n        if (inputBufferIndex < 0) {\n            return false;\n        } else {\n            holder.index = inputBufferIndex;\n            holder.data = mBuffers.getInputBuffer(inputBufferIndex);\n            return true;\n        }\n    }\n\n    /**\n     * Returns a new input buffer and index, waiting indefinitely if none is available.\n     * The buffer should be written into, then be passed to {@link #encodeInputBuffer(InputBuffer)}.\n     *\n     * @param holder the input buffer holder\n     */\n    @SuppressWarnings({\"StatementWithEmptyBody\", \"WeakerAccess\"})\n    protected void acquireInputBuffer(@NonNull InputBuffer holder) {\n        while (!tryAcquireInputBuffer(holder)) {}\n    }\n\n    /**\n     * Encode data into the {@link #mMediaCodec}.\n     *\n     * @param buffer the input buffer\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    protected void encodeInputBuffer(InputBuffer buffer) {\n        LOG.v(mName, \"ENCODING - Buffer:\", buffer.index,\n                \"Bytes:\", buffer.length,\n                \"Presentation:\", buffer.timestamp);\n        if (buffer.isEndOfStream) { // send EOS\n            mMediaCodec.queueInputBuffer(buffer.index, 0, 0,\n                    buffer.timestamp, MediaCodec.BUFFER_FLAG_END_OF_STREAM);\n        } else {\n            mMediaCodec.queueInputBuffer(buffer.index, 0, buffer.length,\n                    buffer.timestamp, 0);\n        }\n    }\n\n    /**\n     * Extracts all pending data that was written and encoded into {@link #mMediaCodec},\n     * and forwards it to the muxer.\n     *\n     * If drainAll is not set, this returns after TIMEOUT_USEC if there is no more data to drain.\n     * If drainAll is set, we wait until we see EOS on the output.\n     * Calling this with drainAll set should be done once, right before stopping the muxer.\n     *\n     * @param drainAll whether to drain all\n     */\n    @SuppressLint(\"LogNotTimber\")\n    @SuppressWarnings(\"WeakerAccess\")\n    protected final void drainOutput(boolean drainAll) {\n        LOG.i(mName, \"DRAINING - EOS:\", drainAll);\n        if (mMediaCodec == null) {\n            LOG.e(\"drain() was called before prepare() or after releasing.\");\n            return;\n        }\n        if (mBuffers == null) {\n            mBuffers = new MediaCodecBuffers(mMediaCodec);\n        }\n        while (true) {\n            int encoderStatus = mMediaCodec.dequeueOutputBuffer(mBufferInfo, OUTPUT_TIMEOUT_US);\n            LOG.i(mName, \"DRAINING - Got status:\", encoderStatus);\n            if (encoderStatus == MediaCodec.INFO_TRY_AGAIN_LATER) {\n                // no output available yet\n                if (!drainAll) break; // out of while\n\n            } else if (encoderStatus == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {\n                // not expected for an encoder\n                mBuffers.onOutputBuffersChanged();\n\n            } else if (encoderStatus == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {\n                // should happen before receiving buffers, and should only happen once\n                if (mController.isStarted()) {\n                    // throw new RuntimeException(\"MediaFormat changed twice.\");\n                    // Seen this happen in API31. TODO handle differently?\n                } else {\n                    MediaFormat newFormat = mMediaCodec.getOutputFormat();\n                    mTrackIndex = mController.notifyStarted(newFormat);\n                    setState(STATE_STARTED);\n                    mOutputBufferPool = new OutputBufferPool(mTrackIndex);\n                }\n            } else if (encoderStatus < 0) {\n                LOG.e(\"Unexpected result from dequeueOutputBuffer: \" + encoderStatus);\n                // let's ignore it\n            } else {\n                ByteBuffer encodedData = mBuffers.getOutputBuffer(encoderStatus);\n\n                // Codec config means that config data was pulled out and fed to the muxer\n                // when we got the INFO_OUTPUT_FORMAT_CHANGED status. Ignore it.\n                boolean isCodecConfig = (mBufferInfo.flags\n                        & MediaCodec.BUFFER_FLAG_CODEC_CONFIG) != 0;\n                if (!isCodecConfig && mController.isStarted() && mBufferInfo.size != 0) {\n\n                    // adjust the ByteBuffer values to match BufferInfo (not needed?)\n                    encodedData.position(mBufferInfo.offset);\n                    encodedData.limit(mBufferInfo.offset + mBufferInfo.size);\n\n                    // Store mStartTimeUs and mLastTimeUs, useful to detect the max length\n                    // reached and stop recording when needed.\n                    if (mFirstTimeUs == Long.MIN_VALUE) {\n                        mFirstTimeUs = mBufferInfo.presentationTimeUs;\n                        LOG.w(mName, \"DRAINING - Got the first presentation time:\",\n                                mFirstTimeUs);\n                    }\n                    mLastTimeUs = mBufferInfo.presentationTimeUs;\n\n                    // Adjust the presentation times. Subclasses can pass a presentation time in any\n                    // reference system - possibly some that has no real meaning, and frequently,\n                    // presentation times from different encoders have a different time-base.\n                    // To address this, encoders are required to call notifyFirstFrameMillis\n                    // so we can adjust here - moving to 1970 reference.\n                    // Extra benefit: we never pass a pts equal to 0, which some encoders refuse.\n                    mBufferInfo.presentationTimeUs = (mStartTimeMillis * 1000)\n                            + mLastTimeUs - mFirstTimeUs;\n\n                    // Write.\n                    LOG.v(mName, \"DRAINING - About to write(). Adjusted presentation:\",\n                            mBufferInfo.presentationTimeUs);\n                    OutputBuffer buffer = mOutputBufferPool.get();\n                    //noinspection ConstantConditions\n                    buffer.info = mBufferInfo;\n                    buffer.trackIndex = mTrackIndex;\n                    buffer.data = encodedData;\n                    onWriteOutput(mOutputBufferPool, buffer);\n                }\n                mMediaCodec.releaseOutputBuffer(encoderStatus, false);\n\n                // Check for the maxLength constraint (with appropriate conditions)\n                // Not needed if drainAll because we already were asked to stop\n                if (!drainAll\n                        && !mMaxLengthReached\n                        && mFirstTimeUs != Long.MIN_VALUE\n                        && mLastTimeUs - mFirstTimeUs > mMaxLengthUs) {\n                    LOG.w(mName, \"DRAINING - Reached maxLength! mLastTimeUs:\", mLastTimeUs,\n                            \"mStartTimeUs:\", mFirstTimeUs,\n                            \"mDeltaUs:\", mLastTimeUs - mFirstTimeUs,\n                            \"mMaxLengthUs:\", mMaxLengthUs);\n                    onMaxLengthReached();\n                    break;\n                }\n\n                // Check for the EOS flag so we can call onStopped.\n                if ((mBufferInfo.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {\n                    LOG.w(mName, \"DRAINING - Got EOS. Releasing the codec.\");\n                    onStopped();\n                    break;\n                }\n            }\n        }\n    }\n\n    @CallSuper\n    protected void onWriteOutput(@NonNull OutputBufferPool pool, @NonNull OutputBuffer buffer) {\n        mController.write(pool, buffer);\n    }\n\n    protected abstract int getEncodedBitRate();\n\n    /**\n     * Returns the max length setting, in microseconds, which can be used\n     * to compute the current state and eventually call {@link #notifyMaxLengthReached()}.\n     * This is not a requirement for subclasses - we do this check anyway when draining,\n     * but doing so might be better.\n     *\n     * @return the max length setting\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    protected long getMaxLengthUs() {\n        return mMaxLengthUs;\n    }\n\n    /**\n     * Called by subclasses to notify that the max length was reached.\n     * We will move to {@link #STATE_LIMIT_REACHED} and request a stop.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    protected void notifyMaxLengthReached() {\n        onMaxLengthReached();\n    }\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected boolean hasReachedMaxLength() {\n        return mMaxLengthReached;\n    }\n\n    /**\n     * Called by us (during {@link #drainOutput(boolean)}) or by subclasses\n     * (through {@link #notifyMaxLengthReached()}) to notify that we reached the\n     * max length allowed. We will move to {@link #STATE_LIMIT_REACHED} and request a stop.\n     */\n    private void onMaxLengthReached() {\n        if (mMaxLengthReached) {\n            LOG.w(mName, \"onMaxLengthReached: Called twice.\");\n        } else {\n            mMaxLengthReached = true;\n            if (mState >= STATE_LIMIT_REACHED) {\n                LOG.w(mName, \"onMaxLengthReached: Reached in wrong state. Aborting.\", mState);\n            } else {\n                LOG.w(mName, \"onMaxLengthReached: Requesting a stop.\");\n                setState(STATE_LIMIT_REACHED);\n                mController.requestStop(mTrackIndex);\n            }\n        }\n    }\n\n    /**\n     * Should be called by subclasses to pass the milliseconds of the first frame - as soon\n     * as this information is available. The milliseconds should be in the\n     * {@link System#currentTimeMillis()} reference system, so we can coordinate between different\n     * encoders.\n     *\n     * @param firstFrameMillis the milliseconds of the first frame presentation\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    protected final void notifyFirstFrameMillis(long firstFrameMillis) {\n        mStartTimeMillis = firstFrameMillis;\n    }\n\n    /**\n     * Returns the number of events (see {@link #onEvent(String, Object)}) that were scheduled\n     * but still not passed to that function. Could be used to drop some of them if this\n     * number is too high.\n     *\n     * @param event the event type\n     * @return the pending events number\n     */\n    @SuppressWarnings({\"SameParameterValue\", \"ConstantConditions\", \"WeakerAccess\"})\n    protected final int getPendingEvents(@NonNull String event) {\n        return mPendingEvents.get(event).intValue();\n    }\n", "target": "media encoder"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/metering/MeteringRegions.java:MeteringRegions:0", "source": "\n    private final static float POINT_AREA = 0.05F;\n\n    @VisibleForTesting\n    final static float BLUR_FACTOR_WEIGHT = 0.1F;\n    private final static float BLUR_FACTOR_SIZE = 1.5F;\n\n    @VisibleForTesting\n    final List<MeteringRegion> mRegions;\n\n    private CLASSTOKEN(@NonNull List<MeteringRegion> regions) {\n        mRegions = regions;\n    }\n\n    @NonNull\n    public CLASSTOKEN transform(@NonNull MeteringTransform transform) {\n        List<MeteringRegion> regions = new ArrayList<>();\n        for (MeteringRegion region : mRegions) {\n            regions.add(region.transform(transform));\n        }\n        return new CLASSTOKEN(regions);\n    }\n\n    @NonNull\n    public <T> List<T> get(int atMost, @NonNull MeteringTransform<T> transform) {\n        List<T> result = new ArrayList<>();\n        Collections.sort(mRegions);\n        for (MeteringRegion region : mRegions) {\n            result.add(transform.transformMeteringRegion(region.mRegion, region.mWeight));\n        }\n        atMost = Math.min(atMost, result.size());\n        return result.subList(0, atMost);\n    }\n\n    @NonNull\n    public static CLASSTOKEN fromPoint(@NonNull Size bounds,\n                                            @NonNull PointF point) {\n        return fromPoint(bounds, point, MeteringRegion.MAX_WEIGHT);\n    }\n\n    @NonNull\n    public static CLASSTOKEN fromPoint(@NonNull Size bounds,\n                                            @NonNull PointF point,\n                                            int weight) {\n        float width = POINT_AREA * bounds.getWidth();\n        float height = POINT_AREA * bounds.getHeight();\n        RectF rectF = expand(point, width, height);\n        return fromArea(bounds, rectF, weight, true);\n    }\n\n    @NonNull\n    public static CLASSTOKEN fromArea(@NonNull Size bounds,\n                                           @NonNull RectF area) {\n        return fromArea(bounds, area, MeteringRegion.MAX_WEIGHT);\n    }\n\n    @NonNull\n    public static CLASSTOKEN fromArea(@NonNull Size bounds,\n                                           @NonNull RectF area,\n                                           int weight) {\n        return fromArea(bounds, area, weight, false);\n    }\n\n    @NonNull\n    public static CLASSTOKEN fromArea(@NonNull Size bounds,\n                                           @NonNull RectF area,\n                                           int weight,\n                                           boolean blur) {\n        List<MeteringRegion> regions = new ArrayList<>();\n        final PointF center = new PointF(area.centerX(), area.centerY());\n        final float width = area.width();\n        final float height = area.height();\n        regions.add(new MeteringRegion(area, weight));\n        if (blur) {\n            RectF background = expand(center,\n                    BLUR_FACTOR_SIZE * width,\n                    BLUR_FACTOR_SIZE * height);\n            regions.add(new MeteringRegion(background,\n                    Math.round(BLUR_FACTOR_WEIGHT * weight)));\n        }\n        List<MeteringRegion> clipped = new ArrayList<>();\n        for (MeteringRegion region : regions) {\n            clipped.add(region.clip(bounds));\n        }\n        return new CLASSTOKEN(clipped);\n    }\n\n    @NonNull\n    private static RectF expand(@NonNull PointF center, float width, float height) {\n        return new RectF(\n                center.x - width / 2F,\n                center.y - height / 2F,\n                center.x + width / 2F,\n                center.y + height / 2F\n        );\n    }\n\n", "target": "metering regions"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/lock/WhiteBalanceLock.java:WhiteBalanceLock:0", "source": "\n\n    private final static String TAG = CLASSTOKEN.class.getSimpleName();\n    private final static CameraLogger LOG = CameraLogger.create(TAG);\n\n    @Override\n    protected boolean checkIsSupported(@NonNull ActionHolder holder) {\n        boolean isNotLegacy = readCharacteristic(\n                CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL, -1)\n                != CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY;\n        Integer awbMode = holder.getBuilder(this).get(CaptureRequest.CONTROL_AWB_MODE);\n        boolean result = isNotLegacy\n                && awbMode != null\n                && awbMode == CaptureRequest.CONTROL_AWB_MODE_AUTO;\n        LOG.i(\"checkIsSupported:\", result);\n        return result;\n    }\n\n    @Override\n    protected boolean checkShouldSkip(@NonNull ActionHolder holder) {\n        CaptureResult lastResult = holder.getLastResult(this);\n        if (lastResult != null) {\n            Integer awbState = lastResult.get(CaptureResult.CONTROL_AWB_STATE);\n            boolean result = awbState != null\n                    && awbState == CaptureRequest.CONTROL_AWB_STATE_LOCKED;\n            LOG.i(\"checkShouldSkip:\", result);\n            return result;\n        } else {\n            LOG.i(\"checkShouldSkip: false - lastResult is null.\");\n            return false;\n        }\n    }\n\n    @Override\n    protected void onStarted(@NonNull ActionHolder holder) {\n        holder.getBuilder(this).set(CaptureRequest.CONTROL_AWB_LOCK, true);\n        holder.applyBuilder(this);\n    }\n\n    @Override\n    public void onCaptureCompleted(@NonNull ActionHolder holder,\n                                   @NonNull CaptureRequest request,\n                                   @NonNull TotalCaptureResult result) {\n        super.onCaptureCompleted(holder, request, result);\n        Integer awbState = result.get(CaptureResult.CONTROL_AWB_STATE);\n        LOG.i(\"processCapture:\", \"awbState:\", awbState);\n        if (awbState == null) return;\n        switch (awbState) {\n            case CaptureRequest.CONTROL_AWB_STATE_LOCKED: {\n                setState(STATE_COMPLETED);\n                break;\n            }\n            case CaptureRequest.CONTROL_AWB_STATE_CONVERGED:\n            case CaptureRequest.CONTROL_AWB_STATE_INACTIVE:\n            case CaptureRequest.CONTROL_AWB_STATE_SEARCHING: {\n                // Wait...\n                break;\n            }\n        }\n    }\n", "target": "white balance lock"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/internal/RotationHelper.java:RotationHelper:0", "source": "\n\n    /**\n     * Rotates the given yuv image into another yuv array, by the given angle.\n     * @param yuv image\n     * @param size image size\n     * @param rotation desired angle\n     * @return a new yuv array\n     */\n    public static byte[] rotate(@NonNull final byte[] yuv,\n                                @NonNull final Size size,\n                                final int rotation) {\n        if (rotation == 0) return yuv;\n        if (rotation % 90 != 0 || rotation < 0 || rotation > 270) {\n            throw new IllegalArgumentException(\"0 <= rotation < 360, rotation % 90 == 0\");\n        }\n        final int width = size.getWidth();\n        final int height = size.getHeight();\n        final byte[] output = new byte[yuv.length];\n        final int frameSize = width * height;\n        final boolean swap = rotation % 180 != 0;\n        final boolean xflip = rotation % 270 != 0;\n        final boolean yflip = rotation >= 180;\n\n        for (int j = 0; j < height; j++) {\n            for (int i = 0; i < width; i++) {\n                final int yIn = j * width + i;\n                final int uIn = frameSize + (j >> 1) * width + (i & ~1);\n                final int vIn = uIn + 1;\n\n                final int wOut = swap ? height : width;\n                final int hOut = swap ? width : height;\n                final int iSwapped = swap ? j : i;\n                final int jSwapped = swap ? i : j;\n                final int iOut = xflip ? wOut - iSwapped - 1 : iSwapped;\n                final int jOut = yflip ? hOut - jSwapped - 1 : jSwapped;\n\n                final int yOut = jOut * wOut + iOut;\n                final int uOut = frameSize + (jOut >> 1) * wOut + (iOut & ~1);\n                final int vOut = uOut + 1;\n\n                output[yOut] = (byte) (0xff & yuv[yIn]);\n                output[uOut] = (byte) (0xff & yuv[uIn]);\n                output[vOut] = (byte) (0xff & yuv[vIn]);\n            }\n        }\n\n        return output;\n    }\n", "target": "rotation helper"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filters/GammaFilter.java:GammaFilter:0", "source": "\n\n    private final static String FRAGMENT_SHADER = \"#extension GL_OES_EGL_image_external : require\\n\"\n            + \"precision mediump float;\\n\"\n            + \"varying vec2 \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\";\\n\"\n            + \"uniform samplerExternalOES sTexture;\\n\"\n            + \"uniform float gamma;\\n\"\n            + \"void main() {\\n\"\n            + \"  vec4 textureColor = texture2D(sTexture, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME\n            + \");\\n\"\n            + \"  gl_FragColor = vec4(pow(textureColor.rgb, vec3(gamma)), textureColor.w);\\n\"\n            + \"}\\n\";\n\n    private float gamma = 2.0f;\n    private int gammaLocation = -1;\n\n    public CLASSTOKEN() { }\n\n    /**\n     * Sets the new gamma value in the 0.0 - 2.0 range.\n     * The 1.0 value means no correction will be applied.\n     *\n     * @param gamma gamma value\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public void setGamma(float gamma) {\n        if (gamma < 0.0f) gamma = 0.0f;\n        if (gamma > 2.0f) gamma = 2.0f;\n        this.gamma = gamma;\n    }\n\n    /**\n     * Returns the current gamma.\n     *\n     * @see #setGamma(float)\n     * @return gamma\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public float getGamma() {\n        return gamma;\n    }\n\n    @Override\n    public void setParameter1(float value) {\n        setGamma(value * 2F);\n    }\n\n    @Override\n    public float getParameter1() {\n        return getGamma() / 2F;\n    }\n\n    @NonNull\n    @Override\n    public String getFragmentShader() {\n        return FRAGMENT_SHADER;\n    }\n\n    @Override\n    public void onCreate(int programHandle) {\n        super.onCreate(programHandle);\n        gammaLocation = GLES20.glGetUniformLocation(programHandle, \"gamma\");\n        Egloo.checkGlProgramLocation(gammaLocation, \"gamma\");\n    }\n\n    @Override\n    public void onDestroy() {\n        super.onDestroy();\n        gammaLocation = -1;\n    }\n\n    @Override\n    protected void onPreDraw(long timestampUs, @NonNull float[] transformMatrix) {\n        super.onPreDraw(timestampUs, transformMatrix);\n        GLES20.glUniform1f(gammaLocation, gamma);\n        Egloo.checkGlError(\"glUniform1f\");\n    }\n", "target": "gamma filter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/encoding/TextureMediaEncoder.java:Frame:1", "source": "\n        private CLASSTOKEN() {}\n\n        /**\n         * Nanoseconds, in no meaningful time-base. Will be used for offsets only.\n         * Typically this comes from {@link SurfaceTexture#getTimestamp()}.\n         */\n        public long timestampNanos;\n\n        /**\n         * Milliseconds in the {@link System#currentTimeMillis()} reference.\n         * This is actually needed/read only for the first frame.\n         */\n        public long timestampMillis;\n\n        /**\n         * The transformation matrix for the base texture.\n         */\n        public float[] transform = new float[16];\n\n        private long timestampUs() {\n            return timestampNanos / 1000L;\n        }\n    ", "target": "frame"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/frame/ImageFrameManager.java:ImageFrameManager:0", "source": "\n\n    public CLASSTOKEN(int poolSize) {\n        super(poolSize, Image.class);\n    }\n\n    @Override\n    protected void onFrameDataReleased(@NonNull Image data, boolean recycled) {\n        try {\n            data.close();\n        } catch (Exception ignore) {}\n    }\n\n    @NonNull\n    @Override\n    protected Image onCloneFrameData(@NonNull Image data) {\n        throw new RuntimeException(\"Cannot freeze() an Image Frame. \" +\n                \"Please consider using the frame synchronously in your process() method, \" +\n                \"which also gives better performance.\");\n    }\n", "target": "image frame manager"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/size/SizeSelectors.java:SizeSelectors:0", "source": "\n\n    /**\n     * A size constraint to easily filter out\n     * sizes in a list.\n     */\n    public interface Filter {\n        boolean accepts(@NonNull Size size);\n    }\n\n    /**\n     * Returns a new {@link SizeSelector} with the given {@link Filter}.\n     * This kind of selector will respect the order in the source array.\n     *\n     * @param filter a filter\n     * @return a new selector\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    @NonNull\n    public static SizeSelector withFilter(@NonNull Filter filter) {\n        return new FilterSelector(filter);\n    }\n\n    /**\n     * Returns a new {@link SizeSelector} that keeps only sizes\n     * whose width is at most equal to the given width.\n     *\n     * @param width the max width\n     * @return a new selector\n     */\n    @NonNull\n    public static SizeSelector maxWidth(final int width) {\n        return withFilter(new Filter() {\n            @Override\n            public boolean accepts(@NonNull Size size) {\n                return size.getWidth() <= width;\n            }\n        });\n    }\n\n    /**\n     * Returns a new {@link SizeSelector} that keeps only sizes\n     * whose width is at least equal to the given width.\n     *\n     * @param width the min width\n     * @return a new selector\n     */\n    @NonNull\n    public static SizeSelector minWidth(final int width) {\n        return withFilter(new Filter() {\n            @Override\n            public boolean accepts(@NonNull Size size) {\n                return size.getWidth() >= width;\n            }\n        });\n    }\n\n    /**\n     * Returns a new {@link SizeSelector} that keeps only sizes\n     * whose height is at most equal to the given height.\n     *\n     * @param height the max height\n     * @return a new selector\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    @NonNull\n    public static SizeSelector maxHeight(final int height) {\n        return withFilter(new Filter() {\n            @Override\n            public boolean accepts(@NonNull Size size) {\n                return size.getHeight() <= height;\n            }\n        });\n    }\n\n    /**\n     * Returns a new {@link SizeSelector} that keeps only sizes\n     * whose height is at least equal to the given height.\n     *\n     * @param height the min height\n     * @return a new selector\n     */\n    @NonNull\n    public static SizeSelector minHeight(final int height) {\n        return withFilter(new Filter() {\n            @Override\n            public boolean accepts(@NonNull Size size) {\n                return size.getHeight() >= height;\n            }\n        });\n    }\n\n    /**\n     * Returns a new {@link SizeSelector} that keeps only sizes\n     * which respect the given {@link AspectRatio}. You can pass a tolerance\n     * value to include aspect ratios that are slightly different.\n     *\n     * @param ratio the desired aspect ratio\n     * @param delta a small tolerance value\n     * @return a new selector\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    @NonNull\n    public static SizeSelector aspectRatio(AspectRatio ratio, final float delta) {\n        final float desired = ratio.toFloat();\n        return withFilter(new Filter() {\n            @Override\n            public boolean accepts(@NonNull Size size) {\n                float candidate = AspectRatio.of(size.getWidth(), size.getHeight()).toFloat();\n                return candidate >= desired - delta && candidate <= desired + delta;\n            }\n        });\n    }\n\n    /**\n     * Returns a {@link SizeSelector} that will order sizes from\n     * the biggest to the smallest. This means that the biggest size will be taken.\n     *\n     * @return a new selector\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    @NonNull\n    public static SizeSelector biggest() {\n        return new SizeSelector() {\n            @NonNull\n            @Override\n            public List<Size> select(@NonNull List<Size> source) {\n                Collections.sort(source);\n                Collections.reverse(source);\n                return source;\n            }\n        };\n    }\n\n    /**\n     * Returns a {@link SizeSelector} that will order sizes from\n     * the smallest to the biggest. This means that the smallest size will be taken.\n     *\n     * @return a new selector\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    @NonNull\n    public static SizeSelector smallest() {\n        return new SizeSelector() {\n            @NonNull\n            @Override\n            public List<Size> select(@NonNull List<Size> source) {\n                Collections.sort(source);\n                return source;\n            }\n        };\n    }\n\n    /**\n     * Returns a new {@link SizeSelector} that keeps only sizes\n     * whose area is at most equal to the given area in pixels.\n     *\n     * @param area the max area\n     * @return a new selector\n     */\n    @NonNull\n    @SuppressWarnings(\"WeakerAccess\")\n    public static SizeSelector maxArea(final int area) {\n        return withFilter(new Filter() {\n            @Override\n            public boolean accepts(@NonNull Size size) {\n                return size.getHeight() * size.getWidth() <= area;\n            }\n        });\n    }\n\n    /**\n     * Returns a new {@link SizeSelector} that keeps only sizes\n     * whose area is at least equal to the given area in pixels.\n     *\n     * @param area the min area\n     * @return a new selector\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    @NonNull\n    public static SizeSelector minArea(final int area) {\n        return withFilter(new Filter() {\n            @Override\n            public boolean accepts(@NonNull Size size) {\n                return size.getHeight() * size.getWidth() >= area;\n            }\n        });\n    }\n\n    /**\n     * Joins all the given selectors to create a new one that returns\n     * the intersection of all the inputs. Basically, all constraints are\n     * respected.\n     *\n     * Keep in mind there is good chance that the final list will be empty.\n     *\n     * @param selectors input selectors\n     * @return a new selector\n     */\n    @NonNull\n    public static SizeSelector and(SizeSelector... selectors) {\n        return new AndSelector(selectors);\n    }\n\n    /**\n     * Creates a new {@link SizeSelector} that 'or's the given filters.\n     * If the first selector returns an empty list, the next selector is queried,\n     * and so on until a non-empty list is found.\n     *\n     * @param selectors input selectors\n     * @return a new selector\n     */\n    @NonNull\n    public static SizeSelector or(SizeSelector... selectors) {\n        return new OrSelector(selectors);\n    }\n\n\n    //region private utilities\n\n    private static class FilterSelector implements SizeSelector {\n\n        private Filter constraint;\n\n        private FilterSelector(@NonNull Filter constraint) {\n            this.constraint = constraint;\n        }\n\n        @Override\n        @NonNull\n        public List<Size> select(@NonNull List<Size> source) {\n            List<Size> sizes = new ArrayList<>();\n            for (Size size : source) {\n                if (constraint.accepts(size)) {\n                    sizes.add(size);\n                }\n            }\n            return sizes;\n        }\n    }\n\n    private static class AndSelector implements SizeSelector {\n\n        private SizeSelector[] values;\n\n        private AndSelector(@NonNull SizeSelector... values) {\n            this.values = values;\n        }\n\n        @Override\n        @NonNull\n        public List<Size> select(@NonNull List<Size> source) {\n            List<Size> temp = source;\n            for (SizeSelector selector : values) {\n                temp = selector.select(temp);\n            }\n            return temp;\n        }\n    }\n\n    private static class OrSelector implements SizeSelector {\n\n        private SizeSelector[] values;\n\n        private OrSelector(@NonNull SizeSelector... values) {\n            this.values = values;\n        }\n\n        @Override\n        @NonNull\n        public List<Size> select(@NonNull List<Size> source) {\n            List<Size> temp = null;\n            for (SizeSelector selector : values) {\n                temp = selector.select(source);\n                if (!temp.isEmpty()) {\n                    break;\n                }\n            }\n            return temp == null ? new ArrayList<Size>() : temp;\n        }\n\n    }\n    //endregion\n", "target": "size selectors"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/action/Actions.java:Actions:0", "source": "\n\n    /**\n     * Creates a {@link BaseAction} that executes all the child actions\n     * together, at the same time, and completes once all of them are\n     * completed.\n     *\n     * @param actions input actions\n     * @return a new action\n     */\n    @NonNull\n    public static BaseAction together(@NonNull BaseAction... actions) {\n        return new TogetherAction(Arrays.asList(actions));\n    }\n\n    /**\n     * Creates a {@link BaseAction} that executes all the child actions\n     * in sequence, waiting for the first to complete, then going on with\n     * the second and so on, finally completing when all are completed.\n     *\n     * @param actions input actions\n     * @return a new action\n     */\n    @NonNull\n    public static BaseAction sequence(@NonNull BaseAction... actions) {\n        return new SequenceAction(Arrays.asList(actions));\n    }\n\n    /**\n     * Creates a {@link BaseAction} that completes as normal, but is also\n     * forced to complete if the given timeout is reached, by calling\n     * {@link Action#abort(ActionHolder)}.\n     *\n     * @param timeoutMillis timeout in milliseconds\n     * @param action action\n     * @return a new action\n     */\n    @NonNull\n    public static BaseAction timeout(long timeoutMillis, @NonNull BaseAction action) {\n        return new TimeoutAction(timeoutMillis, action);\n    }\n\n", "target": "actions"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/gesture/GestureParser.java:GestureParser:0", "source": "\n\n    private int tapAction;\n    private int longTapAction;\n    private int pinchAction;\n    private int horizontalScrollAction;\n    private int verticalScrollAction;\n\n    public CLASSTOKEN(@NonNull TypedArray array) {\n        tapAction = array.getInteger(R.styleable.CameraView_cameraGestureTap,\n                GestureAction.DEFAULT_TAP.value());\n        longTapAction = array.getInteger(R.styleable.CameraView_cameraGestureLongTap,\n                GestureAction.DEFAULT_LONG_TAP.value());\n        pinchAction = array.getInteger(R.styleable.CameraView_cameraGesturePinch,\n                GestureAction.DEFAULT_PINCH.value());\n        horizontalScrollAction = array.getInteger(\n                R.styleable.CameraView_cameraGestureScrollHorizontal,\n                GestureAction.DEFAULT_SCROLL_HORIZONTAL.value());\n        verticalScrollAction = array.getInteger(R.styleable.CameraView_cameraGestureScrollVertical,\n                GestureAction.DEFAULT_SCROLL_VERTICAL.value());\n    }\n\n    private GestureAction get(int which) {\n        return GestureAction.fromValue(which);\n    }\n\n    public GestureAction getTapAction() {\n        return get(tapAction);\n    }\n\n    public GestureAction getLongTapAction() {\n        return get(longTapAction);\n    }\n\n    public GestureAction getPinchAction() {\n        return get(pinchAction);\n    }\n\n    public GestureAction getHorizontalScrollAction() {\n        return get(horizontalScrollAction);\n    }\n\n    public GestureAction getVerticalScrollAction() {\n        return get(verticalScrollAction);\n    }\n\n", "target": "gesture parser"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/encoding/MediaCodecBuffers.java:MediaCodecBuffers:0", "source": "\n\n    private final MediaCodec mMediaCodec;\n    private final ByteBuffer[] mInputBuffers;\n    private ByteBuffer[] mOutputBuffers;\n\n    CLASSTOKEN(MediaCodec mediaCodec) {\n        mMediaCodec = mediaCodec;\n\n        if (Build.VERSION.SDK_INT < 21) {\n            mInputBuffers = mediaCodec.getInputBuffers();\n            mOutputBuffers = mediaCodec.getOutputBuffers();\n        } else {\n            mInputBuffers = mOutputBuffers = null;\n        }\n    }\n\n    ByteBuffer getInputBuffer(final int index) {\n        if (Build.VERSION.SDK_INT >= 21) {\n            return mMediaCodec.getInputBuffer(index);\n        }\n        ByteBuffer buffer = mInputBuffers[index];\n        buffer.clear();\n        return buffer;\n    }\n\n    ByteBuffer getOutputBuffer(final int index) {\n        if (Build.VERSION.SDK_INT >= 21) {\n            return mMediaCodec.getOutputBuffer(index);\n        }\n        return mOutputBuffers[index];\n    }\n\n    void onOutputBuffersChanged() {\n        if (Build.VERSION.SDK_INT < 21) {\n            mOutputBuffers = mMediaCodec.getOutputBuffers();\n        }\n    }\n", "target": "media codec buffers"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/CameraEngine.java:CameraEngine:0", "source": "\n\n    public interface Callback {\n        @NonNull Context getContext();\n        void dispatchOnCameraOpened(@NonNull CameraOptions options);\n        void dispatchOnCameraClosed();\n        void onCameraPreviewStreamSizeChanged();\n        void dispatchOnPictureShutter(boolean shouldPlaySound);\n        void dispatchOnVideoTaken(@NonNull VideoResult.Stub stub);\n        void dispatchOnPictureTaken(@NonNull PictureResult.Stub stub);\n        void dispatchOnFocusStart(@Nullable Gesture trigger, @NonNull PointF where);\n        void dispatchOnFocusEnd(@Nullable Gesture trigger, boolean success, @NonNull PointF where);\n        void dispatchOnZoomChanged(final float newValue, @Nullable final PointF[] fingers);\n        void dispatchOnExposureCorrectionChanged(float newValue, @NonNull float[] bounds,\n                                                 @Nullable PointF[] fingers);\n        void dispatchFrame(@NonNull Frame frame);\n        void dispatchError(CameraException exception);\n        void dispatchOnVideoRecordingStart();\n        void dispatchOnVideoRecordingEnd();\n    }\n\n    protected static final String TAG = CLASSTOKEN.class.getSimpleName();\n    protected static final CameraLogger LOG = CameraLogger.create(TAG);\n    // If this is 2, this means we'll try to run destroy() twice.\n    private static final int DESTROY_RETRIES = 2;\n\n    private WorkerHandler mHandler;\n    @VisibleForTesting Handler mCrashHandler;\n    private final Callback mCallback;\n    private final CameraStateOrchestrator mOrchestrator\n            = new CameraStateOrchestrator(new CameraOrchestrator.Callback() {\n        @Override\n        @NonNull\n        public WorkerHandler getJobWorker(@NonNull String job) {\n            return mHandler;\n        }\n\n        @Override\n        public void handleJobException(@NonNull String job, @NonNull Exception exception) {\n            handleException(exception, false);\n        }\n    });\n\n    protected CLASSTOKEN(@NonNull Callback callback) {\n        mCallback = callback;\n        mCrashHandler = new Handler(Looper.getMainLooper());\n        recreateHandler(false);\n    }\n\n    @NonNull\n    protected final Callback getCallback() {\n        return mCallback;\n    }\n\n    @NonNull\n    protected final CameraStateOrchestrator getOrchestrator() {\n        return mOrchestrator;\n    }\n\n    //region Error handling\n\n    /**\n     * The base exception handler, which inspects the exception and\n     * decides what to do.\n     */\n    private class CrashExceptionHandler implements Thread.UncaughtExceptionHandler {\n        @Override\n        public void uncaughtException(@NonNull Thread thread, @NonNull Throwable throwable) {\n            handleException(throwable, true);\n        }\n    }\n\n    /**\n     * A static exception handler used during destruction to avoid leaks,\n     * since the default handler is not static and the thread might survive the engine.\n     */\n    private static class NoOpExceptionHandler implements Thread.UncaughtExceptionHandler {\n        @Override\n        public void uncaughtException(@NonNull Thread thread, @NonNull Throwable throwable) {\n            LOG.w(\"EXCEPTION:\", \"In the NoOpExceptionHandler, probably while destroying.\",\n                    \"Thread:\", thread, \"Error:\", throwable);\n        }\n    }\n\n    /**\n     * Handles exceptions coming from either runtime errors on the {@link #mHandler} code that is\n     * not caught (using the {@link CrashExceptionHandler}), as might happen during standard\n     * mHandler.post() operations that subclasses might do, OR for errors caught by tasks and\n     * continuations that we launch here.\n     *\n     * In the first case, the thread is about to be terminated. In the second case,\n     * we can actually keep using it.\n     *\n     * @param throwable the throwable\n     * @param isUncaught true if coming from exception handler\n     */\n    private void handleException(@NonNull final Throwable throwable,\n                                 final boolean isUncaught) {\n        // 1. If this comes from the exception handler, the thread has crashed. Replace it.\n        // Most actions are wrapped into Tasks so don't go here, but some callbacks do\n        // (at least in Camera1, e.g. onError).\n        if (isUncaught) {\n            LOG.e(\"EXCEPTION:\", \"Handler thread is gone. Replacing.\");\n            recreateHandler(false);\n        }\n\n        // 2. Depending on the exception, we must destroy(false|true) to release resources, and\n        // notify the outside, either with the callback or by crashing the app.\n        LOG.e(\"EXCEPTION:\", \"Scheduling on the crash handler...\");\n        mCrashHandler.post(new Runnable() {\n            @Override\n            public void run() {\n                if (throwable instanceof CameraException) {\n                    CameraException exception = (CameraException) throwable;\n                    if (exception.isUnrecoverable()) {\n                        LOG.e(\"EXCEPTION:\", \"Got CameraException. \" +\n                                \"Since it is unrecoverable, executing destroy(false).\");\n                        destroy(false);\n                    }\n                    LOG.e(\"EXCEPTION:\", \"Got CameraException. Dispatching to callback.\");\n                    mCallback.dispatchError(exception);\n                } else {\n                    LOG.e(\"EXCEPTION:\", \"Unexpected error! Executing destroy(true).\");\n                    destroy(true);\n                    LOG.e(\"EXCEPTION:\", \"Unexpected error! Throwing.\");\n                    if (throwable instanceof RuntimeException) {\n                        throw (RuntimeException) throwable;\n                    } else {\n                        throw new RuntimeException(throwable);\n                    }\n                }\n            }\n        });\n    }\n\n    /**\n     * Recreates the handler, to ensure we use a fresh one from now on.\n     * If we suspect that handler is currently stuck, the orchestrator should be reset\n     * because it hosts a chain of tasks and the last one will never complete.\n     * @param resetOrchestrator true to reset\n     */\n    private void recreateHandler(boolean resetOrchestrator) {\n        if (mHandler != null) mHandler.destroy();\n        mHandler = WorkerHandler.get(\"CameraViewEngine\");\n        mHandler.getThread().setUncaughtExceptionHandler(new CrashExceptionHandler());\n        if (resetOrchestrator) mOrchestrator.reset();\n    }\n\n    //endregion\n\n    //region State management\n\n    @NonNull\n    public final CameraState getState() {\n        return mOrchestrator.getCurrentState();\n    }\n\n    @NonNull\n    public final CameraState getTargetState() {\n        return mOrchestrator.getTargetState();\n    }\n\n    public final boolean isChangingState() {\n        return mOrchestrator.hasPendingStateChange();\n    }\n\n    /**\n     * Calls {@link #stop(boolean)} and waits for it.\n     * Not final due to mockito requirements.\n     *\n     * If unrecoverably is true, this also releases resources and the engine will not be in a\n     * functional state after. If forever is false, this really is just a synchronous stop.\n     *\n     * NOTE: Should not be called on the orchestrator thread! This would cause deadlocks due to us\n     * awaiting for {@link #stop(boolean)} to return.\n     */\n    public void destroy(boolean unrecoverably) {\n        destroy(unrecoverably, 0);\n    }\n\n    private void destroy(boolean unrecoverably, int depth) {\n        LOG.i(\"DESTROY:\", \"state:\", getState(),\n                \"thread:\", Thread.currentThread(),\n                \"depth:\", depth,\n                \"unrecoverably:\", unrecoverably);\n        if (unrecoverably) {\n            // Prevent CLASSTOKEN leaks. Don't set to null, or exceptions\n            // inside the standard stop() method might crash the main thread.\n            mHandler.getThread().setUncaughtExceptionHandler(new NoOpExceptionHandler());\n        }\n        // Cannot use Tasks.await() because we might be on the UI thread.\n        final CountDownLatch latch = new CountDownLatch(1);\n        stop(true).addOnCompleteListener(\n                mHandler.getExecutor(),\n                new OnCompleteListener<Void>() {\n                    @Override\n                    public void onComplete(@NonNull Task<Void> task) {\n                        latch.countDown();\n                    }\n                });\n        try {\n            boolean success = latch.await(6, TimeUnit.SECONDS);\n            if (!success) {\n                // This thread is likely stuck. The reason might be deadlock issues in the internal\n                // camera implementation, at least in emulators: see Camera1Engine and Camera2Engine\n                // onStopEngine() implementation and comments.\n                LOG.e(\"DESTROY: Could not destroy synchronously after 6 seconds.\",\n                        \"Current thread:\", Thread.currentThread(),\n                        \"Handler thread:\", mHandler.getThread());\n                depth++;\n                if (depth < DESTROY_RETRIES) {\n                    recreateHandler(true);\n                    LOG.e(\"DESTROY: Trying again on thread:\", mHandler.getThread());\n                    destroy(unrecoverably, depth);\n                } else {\n                    LOG.w(\"DESTROY: Giving up because DESTROY_RETRIES was reached.\");\n                }\n            }\n        } catch (InterruptedException ignore) {}\n    }\n\n    @SuppressWarnings(\"WeakerAccess\")\n    public void restart() {\n        LOG.i(\"RESTART:\", \"scheduled. State:\", getState());\n        stop(false);\n        start();\n    }\n\n    @NonNull\n    public Task<Void> start() {\n        LOG.i(\"START:\", \"scheduled. State:\", getState());\n        Task<Void> engine = startEngine();\n        startBind();\n        startPreview();\n        return engine;\n    }\n\n    @NonNull\n    public Task<Void> stop(final boolean swallowExceptions) {\n        LOG.i(\"STOP:\", \"scheduled. State:\", getState());\n        stopPreview(swallowExceptions);\n        stopBind(swallowExceptions);\n        return stopEngine(swallowExceptions);\n    }\n\n    @SuppressWarnings({\"WeakerAccess\", \"UnusedReturnValue\"})\n    @NonNull\n    protected Task<Void> restartBind() {\n        LOG.i(\"RESTART BIND:\", \"scheduled. State:\", getState());\n        stopPreview(false);\n        stopBind(false);\n        startBind();\n        return startPreview();\n    }\n\n    @SuppressWarnings({\"WeakerAccess\", \"UnusedReturnValue\"})\n    @NonNull\n    protected Task<Void> restartPreview() {\n        LOG.i(\"RESTART PREVIEW:\", \"scheduled. State:\", getState());\n        stopPreview(false);\n        return startPreview();\n    }\n\n    //endregion\n\n    //region Start & Stop the engine\n\n    @NonNull\n    @EngineThread\n    private Task<Void> startEngine() {\n        return mOrchestrator.scheduleStateChange(CameraState.OFF, CameraState.ENGINE,\n                true,\n                new Callable<Task<CameraOptions>>() {\n            @Override\n            public Task<CameraOptions> call() {\n                if (!collectCameraInfo(getFacing())) {\n                    LOG.e(\"onStartEngine:\", \"No camera available for facing\", getFacing());\n                    throw new CameraException(CameraException.REASON_NO_CAMERA);\n                }\n                return onStartEngine();\n            }\n        }).onSuccessTask(new SuccessContinuation<CameraOptions, Void>() {\n            @NonNull\n            @Override\n            public Task<Void> then(@Nullable CameraOptions cameraOptions) {\n                // Put this on the outer task so we're sure it's called after getState() is changed.\n                // This was breaking some tests on rare occasions.\n                if (cameraOptions == null) throw new RuntimeException(\"Null options!\");\n                mCallback.dispatchOnCameraOpened(cameraOptions);\n                return Tasks.forResult(null);\n            }\n        });\n    }\n\n    @NonNull\n    @EngineThread\n    private Task<Void> stopEngine(boolean swallowExceptions) {\n        return mOrchestrator.scheduleStateChange(CameraState.ENGINE, CameraState.OFF,\n                !swallowExceptions,\n                new Callable<Task<Void>>() {\n            @Override\n            public Task<Void> call() {\n                return onStopEngine();\n            }\n        }).addOnSuccessListener(new OnSuccessListener<Void>() {\n            @Override\n            public void onSuccess(Void aVoid) {\n                // Put this on the outer task so we're sure it's called after getState() is OFF.\n                // This was breaking some tests on rare occasions.\n                mCallback.dispatchOnCameraClosed();\n            }\n        });\n    }\n\n    /**\n     * Camera is about to be opened. Implementors should look into available cameras\n     * and see if anyone matches the given {@link Facing value}.\n     *\n     * If so, implementors should set {@link Angles#setSensorOffset(Facing, int)}\n     * and any other information (like camera ID) needed to start the engine.\n     *\n     * @param facing the facing value\n     * @return true if we have one\n     */\n    @EngineThread\n    protected abstract boolean collectCameraInfo(@NonNull Facing facing);\n\n    /**\n     * Starts the engine.\n     * @return a task\n     */\n    @NonNull\n    @EngineThread\n    protected abstract Task<CameraOptions> onStartEngine();\n\n    /**\n     * Stops the engine.\n     * Stop events should generally not throw exceptions. We\n     * want to release resources either way.\n     * @return a task\n     */\n    @NonNull\n    @EngineThread\n    protected abstract Task<Void> onStopEngine();\n\n    //endregion\n\n    //region Start & Stop binding\n\n    @NonNull\n    @EngineThread\n    private Task<Void> startBind() {\n        return mOrchestrator.scheduleStateChange(CameraState.ENGINE, CameraState.BIND,\n                true,\n                new Callable<Task<Void>>() {\n            @Override\n            public Task<Void> call() {\n                if (getPreview() != null && getPreview().hasSurface()) {\n                    return onStartBind();\n                } else {\n                    return Tasks.forCanceled();\n                }\n            }\n        });\n    }\n\n    @SuppressWarnings(\"UnusedReturnValue\")\n    @NonNull\n    @EngineThread\n    private Task<Void> stopBind(boolean swallowExceptions) {\n        return mOrchestrator.scheduleStateChange(CameraState.BIND, CameraState.ENGINE,\n                !swallowExceptions,\n                new Callable<Task<Void>>() {\n            @Override\n            public Task<Void> call() {\n                return onStopBind();\n            }\n        });\n    }\n\n    /**\n     * Starts the binding process.\n     * @return a task\n     */\n    @NonNull\n    @EngineThread\n    protected abstract Task<Void> onStartBind();\n\n    /**\n     * Stops the binding process.\n     * Stop events should generally not throw exceptions. We\n     * want to release resources either way.\n     * @return a task\n     */\n    @NonNull\n    @EngineThread\n    protected abstract Task<Void> onStopBind();\n\n    //endregion\n\n    //region Start & Stop preview\n\n    @NonNull\n    @EngineThread\n    private Task<Void> startPreview() {\n        return mOrchestrator.scheduleStateChange(CameraState.BIND, CameraState.PREVIEW,\n                true,\n                new Callable<Task<Void>>() {\n            @Override\n            public Task<Void> call() {\n                return onStartPreview();\n            }\n        });\n    }\n\n    @SuppressWarnings(\"UnusedReturnValue\")\n    @NonNull\n    @EngineThread\n    private Task<Void> stopPreview(boolean swallowExceptions) {\n        return mOrchestrator.scheduleStateChange(CameraState.PREVIEW, CameraState.BIND,\n                !swallowExceptions,\n                new Callable<Task<Void>>() {\n            @Override\n            public Task<Void> call() {\n                return onStopPreview();\n            }\n        });\n    }\n\n    /**\n     * Starts the preview streaming.\n     * @return a task\n     */\n    @NonNull\n    @EngineThread\n    protected abstract Task<Void> onStartPreview();\n\n    /**\n     * Stops the preview streaming.\n     * Stop events should generally not throw exceptions. We\n     * want to release resources either way.\n     * @return a task\n     */\n    @NonNull\n    @EngineThread\n    protected abstract Task<Void> onStopPreview();\n\n    //endregion\n\n    //region Surface callbacks\n\n    /**\n     * The surface is now available, which means that step 1 has completed.\n     * If we have also completed step 2, go on with binding and streaming.\n     */\n    @SuppressWarnings(\"ConstantConditions\")\n    @Override\n    public final void onSurfaceAvailable() {\n        LOG.i(\"onSurfaceAvailable:\", \"Size is\", getPreview().getSurfaceSize());\n        startBind();\n        startPreview();\n    }\n\n    @Override\n    public final void onSurfaceDestroyed() {\n        LOG.i(\"onSurfaceDestroyed\");\n        stopPreview(false);\n        stopBind(false);\n    }\n\n    //endregion\n\n    //region Abstract getters\n\n    @NonNull\n    public abstract Angles getAngles();\n\n    @NonNull\n    public abstract FrameManager getFrameManager();\n\n    @Nullable\n    public abstract CameraOptions getCameraOptions();\n\n    @Nullable\n    public abstract Size getPictureSize(@NonNull Reference reference);\n\n    @Nullable\n    public abstract Size getVideoSize(@NonNull Reference reference);\n\n    @Nullable\n    public abstract Size getPreviewStreamSize(@NonNull Reference reference);\n\n    @Nullable\n    public abstract Size getUncroppedSnapshotSize(@NonNull Reference reference);\n\n    //endregion\n\n    //region Abstract APIs\n\n    public abstract void setPreview(@NonNull CameraPreview cameraPreview);\n    @Nullable public abstract CameraPreview getPreview();\n\n    public abstract void setOverlay(@Nullable Overlay overlay);\n    @Nullable public abstract Overlay getOverlay();\n\n    public abstract void setPreviewStreamSizeSelector(@Nullable SizeSelector selector);\n    @Nullable public abstract SizeSelector getPreviewStreamSizeSelector();\n\n    public abstract void setPictureSizeSelector(@NonNull SizeSelector selector);\n    @NonNull public abstract SizeSelector getPictureSizeSelector();\n\n    public abstract void setVideoSizeSelector(@NonNull SizeSelector selector);\n    @NonNull public abstract SizeSelector getVideoSizeSelector();\n\n    public abstract void setVideoMaxSize(long videoMaxSizeBytes);\n    public abstract long getVideoMaxSize();\n\n    public abstract void setVideoMaxDuration(int videoMaxDurationMillis);\n    public abstract int getVideoMaxDuration();\n\n    public abstract void setVideoCodec(@NonNull VideoCodec codec);\n    @NonNull public abstract VideoCodec getVideoCodec();\n\n    public abstract void setVideoBitRate(int videoBitRate);\n    public abstract int getVideoBitRate();\n\n    public abstract void setAudioBitRate(int audioBitRate);\n    public abstract int getAudioBitRate();\n\n    public abstract void setAudioCodec(@NonNull AudioCodec codec);\n    @NonNull public abstract AudioCodec getAudioCodec();\n\n    public abstract void setSnapshotMaxWidth(int maxWidth);\n    public abstract int getSnapshotMaxWidth();\n\n    public abstract void setSnapshotMaxHeight(int maxHeight);\n    public abstract int getSnapshotMaxHeight();\n\n    public abstract void setFrameProcessingMaxWidth(int maxWidth);\n    public abstract int getFrameProcessingMaxWidth();\n\n    public abstract void setFrameProcessingMaxHeight(int maxHeight);\n    public abstract int getFrameProcessingMaxHeight();\n\n    public abstract void setFrameProcessingFormat(int format);\n    public abstract int getFrameProcessingFormat();\n\n    public abstract void setFrameProcessingPoolSize(int poolSize);\n    public abstract int getFrameProcessingPoolSize();\n\n    public abstract void setAutoFocusResetDelay(long delayMillis);\n    public abstract long getAutoFocusResetDelay();\n\n    public abstract void setFacing(final @NonNull Facing facing);\n    @NonNull public abstract Facing getFacing();\n\n    public abstract void setAudio(@NonNull Audio audio);\n    @NonNull public abstract Audio getAudio();\n\n    public abstract void setMode(@NonNull Mode mode);\n    @NonNull public abstract Mode getMode();\n\n    public abstract void setZoom(float zoom, @Nullable PointF[] points, boolean notify);\n    public abstract float getZoomValue();\n\n    public abstract void setExposureCorrection(float EVvalue, @NonNull float[] bounds,\n                                               @Nullable PointF[] points, boolean notify);\n    public abstract float getExposureCorrectionValue();\n\n    public abstract void setFlash(@NonNull Flash flash);\n    @NonNull public abstract Flash getFlash();\n\n    public abstract void setWhiteBalance(@NonNull WhiteBalance whiteBalance);\n    @NonNull public abstract WhiteBalance getWhiteBalance();\n\n    public abstract void setHdr(@NonNull Hdr hdr);\n    @NonNull public abstract Hdr getHdr();\n\n    public abstract void setLocation(@Nullable Location location);\n    @Nullable public abstract Location getLocation();\n\n    public abstract void setPictureFormat(@NonNull PictureFormat pictureFormat);\n    @NonNull public abstract PictureFormat getPictureFormat();\n\n    public abstract void setPreviewFrameRateExact(boolean previewFrameRateExact);\n    public abstract boolean getPreviewFrameRateExact();\n    public abstract void setPreviewFrameRate(float previewFrameRate);\n    public abstract float getPreviewFrameRate();\n\n    public abstract void setHasFrameProcessors(boolean hasFrameProcessors);\n    public abstract boolean hasFrameProcessors();\n\n    public abstract void setPictureMetering(boolean enable);\n    public abstract boolean getPictureMetering();\n\n    public abstract void setPictureSnapshotMetering(boolean enable);\n    public abstract boolean getPictureSnapshotMetering();\n\n    public abstract void startAutoFocus(@Nullable Gesture gesture,\n                                        @NonNull MeteringRegions regions,\n                                        @NonNull PointF legacyPoint);\n\n    public abstract void setPlaySounds(boolean playSounds);\n\n    public abstract boolean isTakingPicture();\n    public abstract void takePicture(@NonNull PictureResult.Stub stub);\n    public abstract void takePictureSnapshot(final @NonNull PictureResult.Stub stub);\n\n    public abstract boolean isTakingVideo();\n    public abstract void takeVideo(@NonNull VideoResult.Stub stub,\n                                   @Nullable File file,\n                                   @Nullable FileDescriptor fileDescriptor);\n    public abstract void takeVideoSnapshot(@NonNull VideoResult.Stub stub, @NonNull File file);\n    public abstract void stopVideo();\n\n    //endregion\n", "target": "camera engine"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/orchestrator/CameraStateOrchestrator.java:CameraStateOrchestrator:0", "source": "\n\n    private CameraState mCurrentState = CameraState.OFF;\n    private CameraState mTargetState = CameraState.OFF;\n    private int mStateChangeCount = 0;\n\n    public CLASSTOKEN(@NonNull Callback callback) {\n        super(callback);\n    }\n\n    @NonNull\n    public CameraState getCurrentState() {\n        return mCurrentState;\n    }\n\n    @NonNull\n    public CameraState getTargetState() {\n        return mTargetState;\n    }\n\n    public boolean hasPendingStateChange() {\n        synchronized (mJobsLock) {\n            for (Job<?> job : mJobs) {\n                if ((job.name.contains(\" >> \") || job.name.contains(\" << \"))\n                        && !job.source.getTask().isComplete()) {\n                    return true;\n                }\n            }\n            return false;\n        }\n    }\n\n    @NonNull\n    public <T> Task<T> scheduleStateChange(@NonNull final CameraState fromState,\n                                           @NonNull final CameraState toState,\n                                           boolean dispatchExceptions,\n                                           @NonNull final Callable<Task<T>> stateChange) {\n        final int changeCount = ++mStateChangeCount;\n        mTargetState = toState;\n\n        final boolean isTearDown = !toState.isAtLeast(fromState);\n        final String name = isTearDown ? fromState.name() + \" << \" + toState.name()\n                : fromState.name() + \" >> \" + toState.name();\n        return schedule(name, dispatchExceptions, new Callable<Task<T>>() {\n            @Override\n            public Task<T> call() throws Exception {\n                if (getCurrentState() != fromState) {\n                    LOG.w(name.toUpperCase(), \"- State mismatch, aborting. current:\",\n                            getCurrentState(), \"from:\", fromState, \"to:\", toState);\n                    return Tasks.forCanceled();\n                } else {\n                    Executor executor = mCallback.getJobWorker(name).getExecutor();\n                    return stateChange.call().continueWithTask(executor,\n                            new Continuation<T, Task<T>>() {\n                        @Override\n                        public Task<T> then(@NonNull Task<T> task) {\n                            if (task.isSuccessful() || isTearDown) {\n                                mCurrentState = toState;\n                            }\n                            return task;\n                        }\n                    });\n                }\n            }\n        }).addOnCompleteListener(new OnCompleteListener<T>() {\n            @Override\n            public void onComplete(@NonNull Task<T> task) {\n                if (changeCount == mStateChangeCount) {\n                    mTargetState = mCurrentState;\n                }\n            }\n        });\n    }\n\n    @SuppressWarnings(\"UnusedReturnValue\")\n    @NonNull\n    public Task<Void> scheduleStateful(@NonNull String name,\n                                       @NonNull final CameraState atLeast,\n                                       @NonNull final Runnable job) {\n        return schedule(name, true, new Runnable() {\n            @Override\n            public void run() {\n                if (getCurrentState().isAtLeast(atLeast)) {\n                    job.run();\n                }\n            }\n        });\n    }\n\n    public void scheduleStatefulDelayed(@NonNull String name,\n                                        @NonNull final CameraState atLeast,\n                                        long delay,\n                                        @NonNull final Runnable job) {\n        scheduleDelayed(name, true, delay, new Runnable() {\n            @Override\n            public void run() {\n                if (getCurrentState().isAtLeast(atLeast)) {\n                    job.run();\n                }\n            }\n        });\n    }\n", "target": "camera state orchestrator"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/encoding/OutputBufferPool.java:OutputBufferPool:0", "source": "\n\n    CLASSTOKEN(final int trackIndex) {\n        super(Integer.MAX_VALUE, new Factory<OutputBuffer>() {\n            @Override\n            public OutputBuffer create() {\n                OutputBuffer buffer = new OutputBuffer();\n                buffer.trackIndex = trackIndex;\n                buffer.info = new MediaCodec.BufferInfo();\n                return buffer;\n            }\n        });\n    }\n", "target": "output buffer pool"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/internal/Issue514Workaround.java:Issue514Workaround:0", "source": "\n\n    private final int textureId;\n\n    public CLASSTOKEN(int textureId) {\n        this.textureId = textureId;\n    }\n\n    public void beforeOverlayUpdateTexImage() {\n        bindTexture(textureId);\n    }\n\n    public void end() {\n        bindTexture(0);\n    }\n\n    private void bindTexture(int textureId) {\n        GLES20.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, textureId);\n    }\n", "target": "issue 514 workaround"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/CameraBaseEngine.java:CameraBaseEngine:0", "source": "\n\n    protected final static int ALLOWED_ZOOM_OPS = 20;\n    protected final static int ALLOWED_EV_OPS = 20;\n\n    @SuppressWarnings(\"WeakerAccess\") protected CameraPreview mPreview;\n    @SuppressWarnings(\"WeakerAccess\") protected CameraOptions mCameraOptions;\n    @SuppressWarnings(\"WeakerAccess\") protected PictureRecorder mPictureRecorder;\n    @SuppressWarnings(\"WeakerAccess\") protected VideoRecorder mVideoRecorder;\n    @SuppressWarnings(\"WeakerAccess\") protected Size mCaptureSize;\n    @SuppressWarnings(\"WeakerAccess\") protected Size mPreviewStreamSize;\n    @SuppressWarnings(\"WeakerAccess\") protected Size mFrameProcessingSize;\n    @SuppressWarnings(\"WeakerAccess\") protected int mFrameProcessingFormat;\n    @SuppressWarnings(\"WeakerAccess\") protected boolean mHasFrameProcessors;\n    @SuppressWarnings(\"WeakerAccess\") protected Flash mFlash;\n    @SuppressWarnings(\"WeakerAccess\") protected WhiteBalance mWhiteBalance;\n    @SuppressWarnings(\"WeakerAccess\") protected VideoCodec mVideoCodec;\n    @SuppressWarnings(\"WeakerAccess\") protected AudioCodec mAudioCodec;\n    @SuppressWarnings(\"WeakerAccess\") protected Hdr mHdr;\n    @SuppressWarnings(\"WeakerAccess\") protected PictureFormat mPictureFormat;\n    @SuppressWarnings(\"WeakerAccess\") protected Location mLocation;\n    @SuppressWarnings(\"WeakerAccess\") protected float mZoomValue;\n    @SuppressWarnings(\"WeakerAccess\") protected float mExposureCorrectionValue;\n    @SuppressWarnings(\"WeakerAccess\") protected boolean mPlaySounds;\n    @SuppressWarnings(\"WeakerAccess\") protected boolean mPictureMetering;\n    @SuppressWarnings(\"WeakerAccess\") protected boolean mPictureSnapshotMetering;\n    @SuppressWarnings(\"WeakerAccess\") protected float mPreviewFrameRate;\n    @SuppressWarnings(\"WeakerAccess\") private boolean mPreviewFrameRateExact;\n\n    private FrameManager mFrameManager;\n    private final Angles mAngles = new Angles();\n    @Nullable private SizeSelector mPreviewStreamSizeSelector;\n    private SizeSelector mPictureSizeSelector;\n    private SizeSelector mVideoSizeSelector;\n    private Facing mFacing;\n    private Mode mMode;\n    private Audio mAudio;\n    private long mVideoMaxSize;\n    private int mVideoMaxDuration;\n    private int mVideoBitRate;\n    private int mAudioBitRate;\n    private long mAutoFocusResetDelayMillis;\n    private int mSnapshotMaxWidth; // in REF_VIEW like SizeSelectors\n    private int mSnapshotMaxHeight; // in REF_VIEW like SizeSelectors\n    private int mFrameProcessingMaxWidth; // in REF_VIEW like SizeSelectors\n    private int mFrameProcessingMaxHeight; // in REF_VIEW like SizeSelectors\n    private int mFrameProcessingPoolSize;\n    private Overlay mOverlay;\n\n    // Ops used for testing.\n    @VisibleForTesting(otherwise = VisibleForTesting.PROTECTED) Task<Void> mZoomTask\n            = Tasks.forResult(null);\n    @VisibleForTesting(otherwise = VisibleForTesting.PROTECTED) Task<Void> mExposureCorrectionTask\n            = Tasks.forResult(null);\n    @VisibleForTesting(otherwise = VisibleForTesting.PROTECTED) Task<Void> mFlashTask\n            = Tasks.forResult(null);\n    @VisibleForTesting(otherwise = VisibleForTesting.PROTECTED) Task<Void> mWhiteBalanceTask\n            = Tasks.forResult(null);\n    @VisibleForTesting(otherwise = VisibleForTesting.PROTECTED) Task<Void> mHdrTask\n            = Tasks.forResult(null);\n    @VisibleForTesting(otherwise = VisibleForTesting.PROTECTED) Task<Void> mLocationTask\n            = Tasks.forResult(null);\n    @VisibleForTesting(otherwise = VisibleForTesting.PROTECTED) Task<Void> mPlaySoundsTask\n            = Tasks.forResult(null);\n    @VisibleForTesting(otherwise = VisibleForTesting.PROTECTED) Task<Void> mPreviewFrameRateTask\n            = Tasks.forResult(null);\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected CLASSTOKEN(@NonNull Callback callback) {\n        super(callback);\n    }\n\n    /**\n     * Called at construction time to get a frame manager that can later be\n     * accessed through {@link #getFrameManager()}.\n     * @param poolSize pool size\n     * @return a frame manager\n     */\n    @NonNull\n    protected abstract FrameManager instantiateFrameManager(int poolSize);\n\n    @NonNull\n    @Override\n    public final Angles getAngles() {\n        return mAngles;\n    }\n\n    @NonNull\n    @Override\n    public FrameManager getFrameManager() {\n        if (mFrameManager == null) {\n            mFrameManager = instantiateFrameManager(mFrameProcessingPoolSize);\n        }\n        return mFrameManager;\n    }\n\n    @Nullable\n    @Override\n    public final CameraOptions getCameraOptions() {\n        return mCameraOptions;\n    }\n\n    @Override\n    public final void setPreview(@NonNull CameraPreview cameraPreview) {\n        if (mPreview != null) mPreview.setSurfaceCallback(null);\n        mPreview = cameraPreview;\n        mPreview.setSurfaceCallback(this);\n    }\n\n    @NonNull\n    @Override\n    public final CameraPreview getPreview() {\n        return mPreview;\n    }\n\n    @Override\n    public final void setOverlay(@Nullable Overlay overlay) {\n        mOverlay = overlay;\n    }\n\n    @Nullable\n    @Override\n    public final Overlay getOverlay() {\n        return mOverlay;\n    }\n\n    @Override\n    public final void setPreviewStreamSizeSelector(@Nullable SizeSelector selector) {\n        mPreviewStreamSizeSelector = selector;\n    }\n\n    @Nullable\n    @Override\n    public final SizeSelector getPreviewStreamSizeSelector() {\n        return mPreviewStreamSizeSelector;\n    }\n\n    @Override\n    public final void setPictureSizeSelector(@NonNull SizeSelector selector) {\n        mPictureSizeSelector = selector;\n    }\n\n    @NonNull\n    @Override\n    public final SizeSelector getPictureSizeSelector() {\n        return mPictureSizeSelector;\n    }\n\n    @Override\n    public final void setVideoSizeSelector(@NonNull SizeSelector selector) {\n        mVideoSizeSelector = selector;\n    }\n\n    @NonNull\n    @Override\n    public final SizeSelector getVideoSizeSelector() {\n        return mVideoSizeSelector;\n    }\n\n    @Override\n    public final void setVideoMaxSize(long videoMaxSizeBytes) {\n        mVideoMaxSize = videoMaxSizeBytes;\n    }\n\n    @Override\n    public final long getVideoMaxSize() {\n        return mVideoMaxSize;\n    }\n\n    @Override\n    public final void setVideoMaxDuration(int videoMaxDurationMillis) {\n        mVideoMaxDuration = videoMaxDurationMillis;\n    }\n\n    @Override\n    public final int getVideoMaxDuration() {\n        return mVideoMaxDuration;\n    }\n\n    @Override\n    public final void setVideoCodec(@NonNull VideoCodec codec) {\n        mVideoCodec = codec;\n    }\n\n    @NonNull\n    @Override\n    public final VideoCodec getVideoCodec() {\n        return mVideoCodec;\n    }\n\n    @Override\n    public final void setVideoBitRate(int videoBitRate) {\n        mVideoBitRate = videoBitRate;\n    }\n\n    @Override\n    public final int getVideoBitRate() {\n        return mVideoBitRate;\n    }\n\n    @Override\n    public final void setAudioCodec(@NonNull AudioCodec codec) {\n        mAudioCodec = codec;\n    }\n\n    @NonNull\n    @Override\n    public final AudioCodec getAudioCodec() {\n        return mAudioCodec;\n    }\n\n    @Override\n    public final void setAudioBitRate(int audioBitRate) {\n        mAudioBitRate = audioBitRate;\n    }\n\n    @Override\n    public final int getAudioBitRate() {\n        return mAudioBitRate;\n    }\n\n    @Override\n    public final void setSnapshotMaxWidth(int maxWidth) {\n        mSnapshotMaxWidth = maxWidth;\n    }\n\n    @Override\n    public final int getSnapshotMaxWidth() {\n        return mSnapshotMaxWidth;\n    }\n\n    @Override\n    public final void setSnapshotMaxHeight(int maxHeight) {\n        mSnapshotMaxHeight = maxHeight;\n    }\n\n    @Override\n    public final int getSnapshotMaxHeight() {\n        return mSnapshotMaxHeight;\n    }\n\n    @Override\n    public final void setFrameProcessingMaxWidth(int maxWidth) {\n        mFrameProcessingMaxWidth = maxWidth;\n    }\n\n    @Override\n    public final int getFrameProcessingMaxWidth() {\n        return mFrameProcessingMaxWidth;\n    }\n\n    @Override\n    public final void setFrameProcessingMaxHeight(int maxHeight) {\n        mFrameProcessingMaxHeight = maxHeight;\n    }\n\n    @Override\n    public final int getFrameProcessingMaxHeight() {\n        return mFrameProcessingMaxHeight;\n    }\n\n    @Override\n    public final int getFrameProcessingFormat() {\n        return mFrameProcessingFormat;\n    }\n\n    @Override\n    public final void setFrameProcessingPoolSize(int poolSize) {\n        mFrameProcessingPoolSize = poolSize;\n    }\n\n    @Override\n    public final int getFrameProcessingPoolSize() {\n        return mFrameProcessingPoolSize;\n    }\n\n    @Override\n    public final void setAutoFocusResetDelay(long delayMillis) {\n        mAutoFocusResetDelayMillis = delayMillis;\n    }\n\n    @Override\n    public final long getAutoFocusResetDelay() {\n        return mAutoFocusResetDelayMillis;\n    }\n\n    /**\n     * Helper function for subclasses.\n     * @return true if AF should be reset\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    protected final boolean shouldResetAutoFocus() {\n        return mAutoFocusResetDelayMillis > 0 && mAutoFocusResetDelayMillis != Long.MAX_VALUE;\n    }\n\n    /**\n     * Sets a new facing value. This will restart the engine session (if there's any)\n     * so that we can open the new facing camera.\n     * @param facing facing\n     */\n    @Override\n    public final void setFacing(final @NonNull Facing facing) {\n        final Facing old = mFacing;\n        if (facing != old) {\n            mFacing = facing;\n            getOrchestrator().scheduleStateful(\"facing\", CameraState.ENGINE,\n                    new Runnable() {\n                @Override\n                public void run() {\n                    if (collectCameraInfo(facing)) {\n                        restart();\n                    } else {\n                        mFacing = old;\n                    }\n                }\n            });\n        }\n    }\n\n    @NonNull\n    @Override\n    public final Facing getFacing() {\n        return mFacing;\n    }\n\n    /**\n     * Sets a new audio value that will be used for video recordings.\n     * @param audio desired audio\n     */\n    @Override\n    public final void setAudio(@NonNull Audio audio) {\n        if (mAudio != audio) {\n            if (isTakingVideo()) {\n                LOG.w(\"Audio setting was changed while recording. \" +\n                        \"Changes will take place starting from next video\");\n            }\n            mAudio = audio;\n        }\n    }\n\n    @NonNull\n    @Override\n    public final Audio getAudio() {\n        return mAudio;\n    }\n\n    /**\n     * Sets the desired mode (either picture or video).\n     * @param mode desired mode.\n     */\n    @Override\n    public final void setMode(@NonNull Mode mode) {\n        if (mode != mMode) {\n            mMode = mode;\n            getOrchestrator().scheduleStateful(\"mode\", CameraState.ENGINE,\n                    new Runnable() {\n                @Override\n                public void run() {\n                    restart();\n                }\n            });\n        }\n    }\n\n    @NonNull\n    @Override\n    public final Mode getMode() {\n        return mMode;\n    }\n\n    @Override\n    public final float getZoomValue() {\n        return mZoomValue;\n    }\n\n    @Override\n    public final float getExposureCorrectionValue() {\n        return mExposureCorrectionValue;\n    }\n\n    @NonNull\n    @Override\n    public final Flash getFlash() {\n        return mFlash;\n    }\n\n    @NonNull\n    @Override\n    public final WhiteBalance getWhiteBalance() {\n        return mWhiteBalance;\n    }\n\n    @NonNull\n    @Override\n    public final Hdr getHdr() {\n        return mHdr;\n    }\n\n    @Nullable\n    @Override\n    public final Location getLocation() {\n        return mLocation;\n    }\n\n    @NonNull\n    @Override\n    public final PictureFormat getPictureFormat() {\n        return mPictureFormat;\n    }\n\n    @Override\n    public final void setPreviewFrameRateExact(boolean previewFrameRateExact) {\n        mPreviewFrameRateExact = previewFrameRateExact;\n    }\n\n    @Override\n    public final boolean getPreviewFrameRateExact() {\n        return mPreviewFrameRateExact;\n    }\n\n    @Override\n    public final float getPreviewFrameRate() {\n        return mPreviewFrameRate;\n    }\n\n    @Override\n    public final boolean hasFrameProcessors() {\n        return mHasFrameProcessors;\n    }\n\n    @Override\n    public final void setPictureMetering(boolean enable) {\n        mPictureMetering = enable;\n    }\n\n    @Override\n    public final boolean getPictureMetering() {\n        return mPictureMetering;\n    }\n\n    @Override\n    public final void setPictureSnapshotMetering(boolean enable) {\n        mPictureSnapshotMetering = enable;\n    }\n\n    @Override\n    public final boolean getPictureSnapshotMetering() {\n        return mPictureSnapshotMetering;\n    }\n\n    //region Picture and video control\n\n    @Override\n    public final boolean isTakingPicture() {\n        return mPictureRecorder != null;\n    }\n\n    @Override\n    public /* final */ void takePicture(final @NonNull PictureResult.Stub stub) {\n        // Save boolean before scheduling! See how Camera2Engine calls this with a temp value.\n        final boolean metering = mPictureMetering;\n        getOrchestrator().scheduleStateful(\"take picture\", CameraState.BIND,\n                new Runnable() {\n            @Override\n            public void run() {\n                LOG.i(\"takePicture:\", \"running. isTakingPicture:\", isTakingPicture());\n                if (isTakingPicture()) return;\n                if (mMode == Mode.VIDEO) {\n                    throw new IllegalStateException(\"Can't take hq pictures while in VIDEO mode\");\n                }\n                stub.isSnapshot = false;\n                stub.location = mLocation;\n                stub.facing = mFacing;\n                stub.format = mPictureFormat;\n                onTakePicture(stub, metering);\n            }\n        });\n    }\n\n    /**\n     * The snapshot size is the {@link #getPreviewStreamSize(Reference)}, but cropped based on the\n     * view/surface aspect ratio.\n     * @param stub a picture stub\n     */\n    @Override\n    public /* final */ void takePictureSnapshot(final @NonNull PictureResult.Stub stub) {\n        // Save boolean before scheduling! See how Camera2Engine calls this with a temp value.\n        final boolean metering = mPictureSnapshotMetering;\n        getOrchestrator().scheduleStateful(\"take picture snapshot\", CameraState.BIND,\n                new Runnable() {\n            @Override\n            public void run() {\n                LOG.i(\"takePictureSnapshot:\", \"running. isTakingPicture:\", isTakingPicture());\n                if (isTakingPicture()) return;\n                stub.location = mLocation;\n                stub.isSnapshot = true;\n                stub.facing = mFacing;\n                stub.format = PictureFormat.JPEG;\n                // Leave the other parameters to subclasses.\n                //noinspection ConstantConditions\n                AspectRatio ratio = AspectRatio.of(getPreviewSurfaceSize(Reference.OUTPUT));\n                onTakePictureSnapshot(stub, ratio, metering);\n            }\n        });\n    }\n\n    @Override\n    public void onPictureShutter(boolean didPlaySound) {\n        getCallback().dispatchOnPictureShutter(!didPlaySound);\n    }\n\n    @Override\n    public void onPictureResult(@Nullable PictureResult.Stub result, @Nullable Exception error) {\n        mPictureRecorder = null;\n        if (result != null && result.data != null) {\n            getCallback().dispatchOnPictureTaken(result);\n        } else {\n            LOG.e(\"onPictureResult\", \"result or data is null: something went wrong.\", error);\n            getCallback().dispatchError(new CameraException(error,\n                    CameraException.REASON_PICTURE_FAILED));\n        }\n    }\n\n    @Override\n    public final boolean isTakingVideo() {\n        return mVideoRecorder != null && mVideoRecorder.isRecording();\n    }\n\n    @Override\n    public final void takeVideo(final @NonNull VideoResult.Stub stub,\n                                final @Nullable File file,\n                                final @Nullable FileDescriptor fileDescriptor) {\n        getOrchestrator().scheduleStateful(\"take video\", CameraState.BIND, new Runnable() {\n            @Override\n            public void run() {\n                LOG.i(\"takeVideo:\", \"running. isTakingVideo:\", isTakingVideo());\n                if (isTakingVideo()) return;\n                if (mMode == Mode.PICTURE) {\n                    throw new IllegalStateException(\"Can't record video while in PICTURE mode\");\n                }\n                if (file != null) {\n                    stub.file = file;\n                } else if (fileDescriptor != null) {\n                    stub.fileDescriptor = fileDescriptor;\n                } else {\n                    throw new IllegalStateException(\"file and fileDescriptor are both null.\");\n                }\n                stub.isSnapshot = false;\n                stub.videoCodec = mVideoCodec;\n                stub.audioCodec = mAudioCodec;\n                stub.location = mLocation;\n                stub.facing = mFacing;\n                stub.audio = mAudio;\n                stub.maxSize = mVideoMaxSize;\n                stub.maxDuration = mVideoMaxDuration;\n                stub.videoBitRate = mVideoBitRate;\n                stub.audioBitRate = mAudioBitRate;\n                onTakeVideo(stub);\n            }\n        });\n    }\n\n    /**\n     * @param stub a video stub\n     * @param file the output file\n     */\n    @Override\n    public final void takeVideoSnapshot(@NonNull final VideoResult.Stub stub,\n                                        @NonNull final File file) {\n        getOrchestrator().scheduleStateful(\"take video snapshot\", CameraState.BIND,\n                new Runnable() {\n            @Override\n            public void run() {\n                LOG.i(\"takeVideoSnapshot:\", \"running. isTakingVideo:\", isTakingVideo());\n                stub.file = file;\n                stub.isSnapshot = true;\n                stub.videoCodec = mVideoCodec;\n                stub.audioCodec = mAudioCodec;\n                stub.location = mLocation;\n                stub.facing = mFacing;\n                stub.videoBitRate = mVideoBitRate;\n                stub.audioBitRate = mAudioBitRate;\n                stub.audio = mAudio;\n                stub.maxSize = mVideoMaxSize;\n                stub.maxDuration = mVideoMaxDuration;\n                //noinspection ConstantConditions\n                AspectRatio ratio = AspectRatio.of(getPreviewSurfaceSize(Reference.OUTPUT));\n                onTakeVideoSnapshot(stub, ratio);\n            }\n        });\n    }\n\n    @Override\n    public final void stopVideo() {\n        getOrchestrator().schedule(\"stop video\", true, new Runnable() {\n            @Override\n            public void run() {\n                LOG.i(\"stopVideo\", \"running. isTakingVideo?\", isTakingVideo());\n                onStopVideo();\n            }\n        });\n    }\n\n    @EngineThread\n    @SuppressWarnings(\"WeakerAccess\")\n    protected void onStopVideo() {\n        if (mVideoRecorder != null) {\n            mVideoRecorder.stop(false);\n            // Do not null this, so we respond correctly to isTakingVideo(),\n            // which checks for recorder presence and recorder.isRecording().\n            // It will be nulled in onVideoResult.\n        }\n    }\n\n    @CallSuper\n    @Override\n    public void onVideoResult(@Nullable VideoResult.Stub result, @Nullable Exception exception) {\n        mVideoRecorder = null;\n        if (result != null) {\n            getCallback().dispatchOnVideoTaken(result);\n        } else {\n            LOG.e(\"onVideoResult\", \"result is null: something went wrong.\", exception);\n            getCallback().dispatchError(new CameraException(exception,\n                    CameraException.REASON_VIDEO_FAILED));\n        }\n    }\n\n    @Override\n    public void onVideoRecordingStart() {\n        getCallback().dispatchOnVideoRecordingStart();\n    }\n\n    @Override\n    public void onVideoRecordingEnd() {\n        getCallback().dispatchOnVideoRecordingEnd();\n    }\n\n    @EngineThread\n    protected abstract void onTakePicture(@NonNull PictureResult.Stub stub, boolean doMetering);\n\n    @EngineThread\n    protected abstract void onTakePictureSnapshot(@NonNull PictureResult.Stub stub,\n                                                  @NonNull AspectRatio outputRatio,\n                                                  boolean doMetering);\n\n    @EngineThread\n    protected abstract void onTakeVideoSnapshot(@NonNull VideoResult.Stub stub,\n                                                @NonNull AspectRatio outputRatio);\n\n    @EngineThread\n    protected abstract void onTakeVideo(@NonNull VideoResult.Stub stub);\n\n    //endregion\n\n    //region Size / Surface\n\n    @Override\n    public final void onSurfaceChanged() {\n        LOG.i(\"onSurfaceChanged:\", \"Size is\", getPreviewSurfaceSize(Reference.VIEW));\n        getOrchestrator().scheduleStateful(\"surface changed\", CameraState.BIND,\n                new Runnable() {\n            @Override\n            public void run() {\n                // Compute a new camera preview size and apply.\n                Size newSize = computePreviewStreamSize();\n                if (newSize.equals(mPreviewStreamSize)) {\n                    LOG.i(\"onSurfaceChanged:\",\n                            \"The computed preview size is identical. No op.\");\n                } else {\n                    LOG.i(\"onSurfaceChanged:\",\n                            \"Computed a new preview size. Calling onPreviewStreamSizeChanged().\");\n                    mPreviewStreamSize = newSize;\n                    onPreviewStreamSizeChanged();\n                }\n            }\n        });\n    }\n\n    /**\n     * The preview stream size has changed. At this point, some engine might want to\n     * simply call {@link #restartPreview()}, others to {@link #restartBind()}.\n     *\n     * It basically depends on the step at which the preview stream size is actually used.\n     */\n    @EngineThread\n    protected abstract void onPreviewStreamSizeChanged();\n\n    @Nullable\n    @Override\n    public final Size getPictureSize(@SuppressWarnings(\"SameParameterValue\") @NonNull Reference reference) {\n        Size size = mCaptureSize;\n        if (size == null || mMode == Mode.VIDEO) return null;\n        return getAngles().flip(Reference.SENSOR, reference) ? size.flip() : size;\n    }\n\n    @Nullable\n    @Override\n    public final Size getVideoSize(@SuppressWarnings(\"SameParameterValue\") @NonNull Reference reference) {\n        Size size = mCaptureSize;\n        if (size == null || mMode == Mode.PICTURE) return null;\n        return getAngles().flip(Reference.SENSOR, reference) ? size.flip() : size;\n    }\n\n    @Nullable\n    @Override\n    public final Size getPreviewStreamSize(@NonNull Reference reference) {\n        Size size = mPreviewStreamSize;\n        if (size == null) return null;\n        return getAngles().flip(Reference.SENSOR, reference) ? size.flip() : size;\n    }\n\n    @SuppressWarnings(\"SameParameterValue\")\n    @Nullable\n    private Size getPreviewSurfaceSize(@NonNull Reference reference) {\n        CameraPreview preview = mPreview;\n        if (preview == null) return null;\n        return getAngles().flip(Reference.VIEW, reference) ? preview.getSurfaceSize().flip()\n                : preview.getSurfaceSize();\n    }\n\n    /**\n     * Returns the snapshot size, but not cropped with the view dimensions, which\n     * is what we will do before creating the snapshot. However, cropping is done at various\n     * levels so we don't want to perform the op here.\n     *\n     * The base snapshot size is based on PreviewStreamSize (later cropped with view ratio). Why?\n     * One might be tempted to say that it's the SurfaceSize (which already matches the view ratio).\n     *\n     * The camera sensor will capture preview frames with PreviewStreamSize and that's it. Then they\n     * are hardware-scaled by the preview surface, but this does not affect the snapshot, as the\n     * snapshot recorder simply creates another surface.\n     *\n     * Done tests to ensure that this is true, by using\n     * 1. small SurfaceSize and biggest() PreviewStreamSize: output is not low quality\n     * 2. big SurfaceSize and smallest() PreviewStreamSize: output is low quality\n     * In both cases the result.size here was set to the biggest of the two.\n     *\n     * I could not find the same evidence for videos, but I would say that the same things should\n     * apply, despite the capturing mechanism being different.\n     *\n     * @param reference the reference system\n     * @return the uncropped snapshot size\n     */\n    @Nullable\n    @Override\n    public final Size getUncroppedSnapshotSize(@NonNull Reference reference) {\n        Size baseSize = getPreviewStreamSize(reference);\n        if (baseSize == null) return null;\n        boolean flip = getAngles().flip(reference, Reference.VIEW);\n        int maxWidth = flip ? mSnapshotMaxHeight : mSnapshotMaxWidth;\n        int maxHeight = flip ? mSnapshotMaxWidth : mSnapshotMaxHeight;\n        if (maxWidth <= 0) maxWidth = Integer.MAX_VALUE;\n        if (maxHeight <= 0) maxHeight = Integer.MAX_VALUE;\n        float baseRatio = AspectRatio.of(baseSize).toFloat();\n        float maxValuesRatio = AspectRatio.of(maxWidth, maxHeight).toFloat();\n        if (maxValuesRatio >= baseRatio) {\n            // Height is the real constraint.\n            int outHeight = Math.min(baseSize.getHeight(), maxHeight);\n            int outWidth = (int) Math.floor((float) outHeight * baseRatio);\n            return new Size(outWidth, outHeight);\n        } else {\n            // Width is the real constraint.\n            int outWidth = Math.min(baseSize.getWidth(), maxWidth);\n            int outHeight = (int) Math.floor((float) outWidth / baseRatio);\n            return new Size(outWidth, outHeight);\n        }\n    }\n\n    /**\n     * This is called either on cameraView.start(), or when the underlying surface changes.\n     * It is possible that in the first call the preview surface has not already computed its\n     * dimensions.\n     * But when it does, the {@link CameraPreview.SurfaceCallback} should be called,\n     * and this should be refreshed.\n     *\n     * @return the capture size\n     */\n    @NonNull\n    @SuppressWarnings(\"WeakerAccess\")\n    protected final Size computeCaptureSize() {\n        return computeCaptureSize(mMode);\n    }\n\n    @NonNull\n    @SuppressWarnings(\"WeakerAccess\")\n    protected final Size computeCaptureSize(@NonNull Mode mode) {\n        // We want to pass stuff into the REF_VIEW reference, not the sensor one.\n        // This is already managed by CameraOptions, so we just flip again at the end.\n        boolean flip = getAngles().flip(Reference.SENSOR, Reference.VIEW);\n        SizeSelector selector;\n        Collection<Size> sizes;\n        if (mode == Mode.PICTURE) {\n            selector = mPictureSizeSelector;\n            sizes = mCameraOptions.getSupportedPictureSizes();\n        } else {\n            selector = mVideoSizeSelector;\n            sizes = mCameraOptions.getSupportedVideoSizes();\n        }\n        selector = SizeSelectors.or(selector, SizeSelectors.biggest());\n        List<Size> list = new ArrayList<>(sizes);\n        Size result = selector.select(list).get(0);\n        if (!list.contains(result)) {\n            throw new RuntimeException(\"SizeSelectors must not return Sizes other than \" +\n                    \"those in the input list.\");\n        }\n        LOG.i(\"computeCaptureSize:\", \"result:\", result, \"flip:\", flip, \"mode:\", mode);\n        if (flip) result = result.flip(); // Go back to REF_SENSOR\n        return result;\n    }\n\n    /**\n     * This is called anytime {@link #computePreviewStreamSize()} is called.\n     * This means that it should be called during the binding process, when\n     * we can be sure that the camera is available (engineState == STARTED).\n     * @return a list of available sizes for preview\n     */\n    @EngineThread\n    @NonNull\n    protected abstract List<Size> getPreviewStreamAvailableSizes();\n\n    @EngineThread\n    @NonNull\n    @SuppressWarnings(\"WeakerAccess\")\n    protected final Size computePreviewStreamSize() {\n        @NonNull List<Size> previewSizes = getPreviewStreamAvailableSizes();\n        // These sizes come in REF_SENSOR. Since there is an external selector involved,\n        // we must convert all of them to REF_VIEW, then flip back when returning.\n        boolean flip = getAngles().flip(Reference.SENSOR, Reference.VIEW);\n        List<Size> sizes = new ArrayList<>(previewSizes.size());\n        for (Size size : previewSizes) {\n            sizes.add(flip ? size.flip() : size);\n        }\n\n        // Create our own default selector, which will be used if the external\n        // mPreviewStreamSizeSelector is null, or if it fails in finding a size.\n        Size targetMinSize = getPreviewSurfaceSize(Reference.VIEW);\n        if (targetMinSize == null) {\n            throw new IllegalStateException(\"targetMinSize should not be null here.\");\n        }\n        AspectRatio targetRatio = AspectRatio.of(mCaptureSize.getWidth(), mCaptureSize.getHeight());\n        if (flip) targetRatio = targetRatio.flip();\n        LOG.i(\"computePreviewStreamSize:\",\n                \"targetRatio:\", targetRatio,\n                \"targetMinSize:\", targetMinSize);\n        SizeSelector matchRatio = SizeSelectors.and( // Match this aspect ratio and sort by biggest\n                SizeSelectors.aspectRatio(targetRatio, 0),\n                SizeSelectors.biggest());\n        SizeSelector matchSize = SizeSelectors.and( // Bigger than this size, and sort by smallest\n                SizeSelectors.minHeight(targetMinSize.getHeight()),\n                SizeSelectors.minWidth(targetMinSize.getWidth()),\n                SizeSelectors.smallest());\n        SizeSelector matchAll = SizeSelectors.or(\n                SizeSelectors.and(matchRatio, matchSize), // Try to respect both constraints.\n                matchSize, // If couldn't match aspect ratio, at least respect the size\n                matchRatio, // If couldn't respect size, at least match aspect ratio\n                SizeSelectors.biggest() // If couldn't match any, take the biggest.\n        );\n\n        // Apply the external selector with this as a fallback,\n        // and return a size in REF_SENSOR reference.\n        SizeSelector selector;\n        if (mPreviewStreamSizeSelector != null) {\n            selector = SizeSelectors.or(mPreviewStreamSizeSelector, matchAll);\n        } else {\n            selector = matchAll;\n        }\n        Size result = selector.select(sizes).get(0);\n        if (!sizes.contains(result)) {\n            throw new RuntimeException(\"SizeSelectors must not return Sizes other than \" +\n                    \"those in the input list.\");\n        }\n        if (flip) result = result.flip();\n        LOG.i(\"computePreviewStreamSize:\", \"result:\", result, \"flip:\", flip);\n        return result;\n    }\n\n    /**\n     * This is called anytime {@link #computeFrameProcessingSize()} is called.\n     * Implementors can return null if frame processor size is not selectable\n     * @return a list of available sizes for frame processing\n     */\n    @EngineThread\n    @NonNull\n    protected abstract List<Size> getFrameProcessingAvailableSizes();\n\n    @EngineThread\n    @NonNull\n    @SuppressWarnings(\"WeakerAccess\")\n    protected final Size computeFrameProcessingSize() {\n        @NonNull List<Size> frameSizes = getFrameProcessingAvailableSizes();\n        // These sizes come in REF_SENSOR. Since there is an external selector involved,\n        // we must convert all of them to REF_VIEW, then flip back when returning.\n        boolean flip = getAngles().flip(Reference.SENSOR, Reference.VIEW);\n        List<Size> sizes = new ArrayList<>(frameSizes.size());\n        for (Size size : frameSizes) {\n            sizes.add(flip ? size.flip() : size);\n        }\n        AspectRatio targetRatio = AspectRatio.of(\n                mPreviewStreamSize.getWidth(),\n                mPreviewStreamSize.getHeight());\n        if (flip) targetRatio = targetRatio.flip();\n        int maxWidth = mFrameProcessingMaxWidth;\n        int maxHeight = mFrameProcessingMaxHeight;\n        if (maxWidth <= 0 || maxWidth == Integer.MAX_VALUE) maxWidth = 640;\n        if (maxHeight <= 0 || maxHeight == Integer.MAX_VALUE) maxHeight = 640;\n        Size targetMaxSize = new Size(maxWidth, maxHeight);\n        LOG.i(\"computeFrameProcessingSize:\",\n                \"targetRatio:\", targetRatio,\n                \"targetMaxSize:\", targetMaxSize);\n        SizeSelector matchRatio = SizeSelectors.aspectRatio(targetRatio, 0);\n        SizeSelector matchSize = SizeSelectors.and(\n                SizeSelectors.maxHeight(targetMaxSize.getHeight()),\n                SizeSelectors.maxWidth(targetMaxSize.getWidth()),\n                SizeSelectors.biggest());\n        SizeSelector matchAll = SizeSelectors.or(\n                SizeSelectors.and(matchRatio, matchSize), // Try to respect both constraints.\n                matchSize, // If couldn't match aspect ratio, at least respect the size\n                SizeSelectors.smallest() // If couldn't match any, take the smallest.\n        );\n        Size result = matchAll.select(sizes).get(0);\n        if (!sizes.contains(result)) {\n            throw new RuntimeException(\"SizeSelectors must not return Sizes other than \" +\n                    \"those in the input list.\");\n        }\n        if (flip) result = result.flip();\n        LOG.i(\"computeFrameProcessingSize:\", \"result:\", result, \"flip:\", flip);\n        return result;\n    }\n\n    //endregion\n", "target": "camera base engine"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/overlay/OverlayDrawer.java:OverlayDrawer:0", "source": "\n\n    private static final String TAG = CLASSTOKEN.class.getSimpleName();\n    private static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    private Overlay mOverlay;\n    private SurfaceTexture mSurfaceTexture;\n    private Surface mSurface;\n    @VisibleForTesting GlTextureDrawer mTextureDrawer;\n    private Issue514Workaround mIssue514Workaround;\n    private final Object mIssue514WorkaroundLock = new Object();\n\n    public CLASSTOKEN(@NonNull Overlay overlay, @NonNull Size size) {\n        mOverlay = overlay;\n        mTextureDrawer = new GlTextureDrawer();\n        mSurfaceTexture = new SurfaceTexture(mTextureDrawer.getTexture().getId());\n        mSurfaceTexture.setDefaultBufferSize(size.getWidth(), size.getHeight());\n        mSurface = new Surface(mSurfaceTexture);\n        mIssue514Workaround = new Issue514Workaround(mTextureDrawer.getTexture().getId());\n    }\n\n    /**\n     * Should be called to draw the {@link Overlay} on the given {@link Overlay.Target}.\n     * This will provide a working {@link Canvas} to the overlay and also update the\n     * drawn contents to a GLES texture.\n     * @param target the target\n     */\n    public void draw(@NonNull Overlay.Target target) {\n        try {\n            final Canvas surfaceCanvas;\n            if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.M && mOverlay.getHardwareCanvasEnabled()) {\n                surfaceCanvas = mSurface.lockHardwareCanvas();\n            } else {\n                surfaceCanvas = mSurface.lockCanvas(null);\n            }\n            surfaceCanvas.drawColor(Color.TRANSPARENT, PorterDuff.Mode.CLEAR);\n            mOverlay.drawOn(target, surfaceCanvas);\n            mSurface.unlockCanvasAndPost(surfaceCanvas);\n        } catch (Surface.OutOfResourcesException e) {\n            LOG.w(\"Got Surface.OutOfResourcesException while drawing video overlays\", e);\n        }\n        synchronized (mIssue514WorkaroundLock) {\n            mIssue514Workaround.beforeOverlayUpdateTexImage();\n            try {\n                mSurfaceTexture.updateTexImage();\n            } catch (IllegalStateException e) {\n                LOG.w(\"Got IllegalStateException while updating texture contents\", e);\n            }\n        }\n        mSurfaceTexture.getTransformMatrix(mTextureDrawer.getTextureTransform());\n    }\n\n    /**\n     * Returns the transform that should be used to render the drawn content.\n     * This should be called after {@link #draw(Overlay.Target)} and can be modified.\n     * @return the transform matrix\n     */\n    public float[] getTransform() {\n        return mTextureDrawer.getTextureTransform();\n    }\n\n    /**\n     * Renders the drawn content in the current EGL surface, assuming there is one.\n     * Should be called after {@link #draw(Overlay.Target)} and any {@link #getTransform()}\n     * modification.\n     *\n     * @param timestampUs frame timestamp\n     */\n    public void render(long timestampUs) {\n        // Enable blending\n        // Reference http://www.learnopengles.com/android-lesson-five-an-introduction-to-blending/\n        GLES20.glDisable(GLES20.GL_CULL_FACE);\n        GLES20.glDisable(GLES20.GL_DEPTH_TEST);\n        GLES20.glEnable(GLES20.GL_BLEND);\n        GLES20.glBlendFunc(GLES20.GL_SRC_ALPHA, GLES20.GL_ONE_MINUS_SRC_ALPHA);\n\n        synchronized (mIssue514WorkaroundLock) {\n            mTextureDrawer.draw(timestampUs);\n        }\n    }\n\n    /**\n     * Releases resources.\n     */\n    public void release() {\n        if (mIssue514Workaround != null) {\n            mIssue514Workaround.end();\n            mIssue514Workaround = null;\n        }\n        if (mSurfaceTexture != null) {\n            mSurfaceTexture.release();\n            mSurfaceTexture = null;\n        }\n        if (mSurface != null) {\n            mSurface.release();\n            mSurface = null;\n        }\n        if (mTextureDrawer != null) {\n            mTextureDrawer.release();\n            mTextureDrawer = null;\n        }\n    }\n", "target": "overlay drawer"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/encoding/AudioConfig.java:AudioConfig:0", "source": "\n\n    // Configurable options\n    public int bitRate; // ENCODED bit rate\n    public int channels = 1;\n    public String encoder;\n    public String mimeType = \"audio/mp4a-latm\";\n    public int samplingFrequency = 44100; // samples/sec\n\n    // Not configurable options (for now)\n    final int encoding = AudioFormat.ENCODING_PCM_16BIT; // Determines the sampleSizePerChannel\n    // The 44.1KHz frequency is the only setting guaranteed to be available on all devices.\n    // If sampleSizePerChannel changes, review noise introduction\n    final int sampleSizePerChannel = 2; // byte/sample/channel [16bit].\n    final int byteRatePerChannel = samplingFrequency * sampleSizePerChannel; // byte/sec/channel\n\n    @NonNull\n    CLASSTOKEN copy() {\n        CLASSTOKEN config = new CLASSTOKEN();\n        config.bitRate = bitRate;\n        config.channels = channels;\n        config.encoder = encoder;\n        config.mimeType = mimeType;\n        config.samplingFrequency = samplingFrequency;\n        return config;\n    }\n\n    int byteRate() { // RAW byte rate\n        return byteRatePerChannel * channels; // byte/sec\n    }\n\n    @SuppressWarnings(\"unused\")\n    int bitRate() { // RAW bit rate\n        return byteRate() * 8; // bit/sec\n    }\n\n    int audioFormatChannels() {\n        if (channels == 1) {\n            return AudioFormat.CHANNEL_IN_MONO;\n        } else if (channels == 2) {\n            return AudioFormat.CHANNEL_IN_STEREO;\n        }\n        throw new RuntimeException(\"Invalid number of channels: \" + channels);\n    }\n\n    /**\n     * We call FRAME here the chunk of data that we want to read at each loop cycle.\n     *\n     * When this number is HIGH, the AudioRecord might be unable to keep a good pace and\n     * we might end up skip some frames.\n     *\n     * When this number is LOW, we pull a bigger number of frames and this might end up\n     * delaying our recorder/encoder balance (more frames means more encoding operations).\n     * In the end, this means that the recorder will skip some frames to restore the balance.\n     *\n     * @return the frame size\n     */\n    int frameSize() {\n        return 1024 * channels;\n    }\n\n    /**\n     * Number of frames contained in the {@link android.media.AudioRecord} buffer.\n     * In theory, the higher this value is, the safer it is to delay reading as the\n     * audioRecord will hold the recorded samples anyway and return to us next time we read.\n     *\n     * Should be coordinated with {@link #frameSize()}.\n     *\n     * @return the number of frames\n     */\n    int audioRecordBufferFrames() {\n        return 50;\n    }\n\n    /**\n     * We allocate buffers of {@link #frameSize()} each, which is not much.\n     *\n     * This value indicates the maximum number of these buffers that we can allocate at a given\n     * instant. This value is the number of runnables that the encoder thread is allowed to be\n     * 'behind' the recorder thread. It's not safe to have it very large or we can end encoding\n     * A LOT AFTER the actual recording. It's better to reduce this and skip recording at all.\n     *\n     * Should be coordinated with {@link #frameSize()}.\n     *\n     * @return the buffer pool max size\n     */\n    int bufferPoolMaxSize() {\n        return 500;\n    }\n", "target": "audio config"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/size/SizeSelectorParser.java:SizeSelectorParser:0", "source": "\n\n    private SizeSelector pictureSizeSelector;\n    private SizeSelector videoSizeSelector;\n\n    public CLASSTOKEN(@NonNull TypedArray array) {\n        List<SizeSelector> pictureConstraints = new ArrayList<>(3);\n        if (array.hasValue(R.styleable.CameraView_cameraPictureSizeMinWidth)) {\n            pictureConstraints.add(SizeSelectors.minWidth(\n                    array.getInteger(R.styleable.CameraView_cameraPictureSizeMinWidth, 0)\n            ));\n        }\n        if (array.hasValue(R.styleable.CameraView_cameraPictureSizeMaxWidth)) {\n            pictureConstraints.add(SizeSelectors.maxWidth(\n                    array.getInteger(R.styleable.CameraView_cameraPictureSizeMaxWidth, 0)\n            ));\n        }\n        if (array.hasValue(R.styleable.CameraView_cameraPictureSizeMinHeight)) {\n            pictureConstraints.add(SizeSelectors.minHeight(\n                    array.getInteger(R.styleable.CameraView_cameraPictureSizeMinHeight, 0)\n            ));\n        }\n        if (array.hasValue(R.styleable.CameraView_cameraPictureSizeMaxHeight)) {\n            pictureConstraints.add(SizeSelectors.maxHeight(\n                    array.getInteger(R.styleable.CameraView_cameraPictureSizeMaxHeight, 0)\n            ));\n        }\n        if (array.hasValue(R.styleable.CameraView_cameraPictureSizeMinArea)) {\n            pictureConstraints.add(SizeSelectors.minArea(\n                    array.getInteger(R.styleable.CameraView_cameraPictureSizeMinArea, 0)\n            ));\n        }\n        if (array.hasValue(R.styleable.CameraView_cameraPictureSizeMaxArea)) {\n            pictureConstraints.add(SizeSelectors.maxArea(\n                    array.getInteger(R.styleable.CameraView_cameraPictureSizeMaxArea, 0)\n            ));\n        }\n        if (array.hasValue(R.styleable.CameraView_cameraPictureSizeAspectRatio)) {\n            //noinspection ConstantConditions\n            pictureConstraints.add(SizeSelectors.aspectRatio(\n                    AspectRatio.parse(array.getString(\n                            R.styleable.CameraView_cameraPictureSizeAspectRatio)), 0));\n        }\n\n        if (array.getBoolean(R.styleable.CameraView_cameraPictureSizeSmallest, false)) {\n            pictureConstraints.add(SizeSelectors.smallest());\n        }\n        if (array.getBoolean(R.styleable.CameraView_cameraPictureSizeBiggest, false)) {\n            pictureConstraints.add(SizeSelectors.biggest());\n        }\n        pictureSizeSelector = !pictureConstraints.isEmpty() ?\n                SizeSelectors.and(pictureConstraints.toArray(new SizeSelector[0])) :\n                SizeSelectors.biggest();\n\n        // Video size selector\n        List<SizeSelector> videoConstraints = new ArrayList<>(3);\n        if (array.hasValue(R.styleable.CameraView_cameraVideoSizeMinWidth)) {\n            videoConstraints.add(SizeSelectors.minWidth(\n                    array.getInteger(R.styleable.CameraView_cameraVideoSizeMinWidth, 0)\n            ));\n        }\n        if (array.hasValue(R.styleable.CameraView_cameraVideoSizeMaxWidth)) {\n            videoConstraints.add(SizeSelectors.maxWidth(\n                    array.getInteger(R.styleable.CameraView_cameraVideoSizeMaxWidth, 0)\n            ));\n        }\n        if (array.hasValue(R.styleable.CameraView_cameraVideoSizeMinHeight)) {\n            videoConstraints.add(SizeSelectors.minHeight(\n                    array.getInteger(R.styleable.CameraView_cameraVideoSizeMinHeight, 0)\n            ));\n        }\n        if (array.hasValue(R.styleable.CameraView_cameraVideoSizeMaxHeight)) {\n            videoConstraints.add(SizeSelectors.maxHeight(\n                    array.getInteger(R.styleable.CameraView_cameraVideoSizeMaxHeight, 0)\n            ));\n        }\n        if (array.hasValue(R.styleable.CameraView_cameraVideoSizeMinArea)) {\n            videoConstraints.add(SizeSelectors.minArea(\n                    array.getInteger(R.styleable.CameraView_cameraVideoSizeMinArea, 0)\n            ));\n        }\n        if (array.hasValue(R.styleable.CameraView_cameraVideoSizeMaxArea)) {\n            videoConstraints.add(SizeSelectors.maxArea(\n                    array.getInteger(R.styleable.CameraView_cameraVideoSizeMaxArea, 0)\n            ));\n        }\n        if (array.hasValue(R.styleable.CameraView_cameraVideoSizeAspectRatio)) {\n            //noinspection ConstantConditions\n            videoConstraints.add(SizeSelectors.aspectRatio(\n                    AspectRatio.parse(array.getString(\n                            R.styleable.CameraView_cameraVideoSizeAspectRatio)), 0));\n        }\n        if (array.getBoolean(R.styleable.CameraView_cameraVideoSizeSmallest, false)) {\n            videoConstraints.add(SizeSelectors.smallest());\n        }\n        if (array.getBoolean(R.styleable.CameraView_cameraVideoSizeBiggest, false)) {\n            videoConstraints.add(SizeSelectors.biggest());\n        }\n        videoSizeSelector = !videoConstraints.isEmpty() ?\n                SizeSelectors.and(videoConstraints.toArray(new SizeSelector[0])) :\n                SizeSelectors.biggest();\n    }\n\n    @NonNull\n    public SizeSelector getPictureSizeSelector() {\n        return pictureSizeSelector;\n    }\n\n    @NonNull\n    public SizeSelector getVideoSizeSelector() {\n        return videoSizeSelector;\n    }\n\n", "target": "size selector parser"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filters/AutoFixFilter.java:AutoFixFilter:0", "source": "\n\n    private final static String FRAGMENT_SHADER = \"#extension GL_OES_EGL_image_external : require\\n\"\n            + \"precision mediump float;\\n\"\n            + \"uniform samplerExternalOES tex_sampler_0;\\n\"\n            + \"uniform samplerExternalOES tex_sampler_1;\\n\"\n            + \"uniform samplerExternalOES tex_sampler_2;\\n\"\n            + \"uniform float scale;\\n\"\n            + \"float shift_scale;\\n\"\n            + \"float hist_offset;\\n\"\n            + \"float hist_scale;\\n\"\n            + \"float density_offset;\\n\"\n            + \"float density_scale;\\n\"\n            + \"varying vec2 \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\";\\n\"\n            + \"void main() {\\n\"\n            + \"  shift_scale = \" + (1.0f / 256f) + \";\\n\"\n            + \"  hist_offset = \" + (0.5f / 766f) + \";\\n\"\n            + \"  hist_scale = \" + (765f / 766f) + \";\\n\"\n            + \"  density_offset = \" + (0.5f / 1024f) + \";\\n\"\n            + \"  density_scale = \" + (1023f / 1024f) + \";\\n\"\n            + \"  const vec3 weights = vec3(0.33333, 0.33333, 0.33333);\\n\"\n            + \"  vec4 color = texture2D(tex_sampler_0, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME\n            + \");\\n\"\n            + \"  float energy = dot(color.rgb, weights);\\n\"\n            + \"  float mask_value = energy - 0.5;\\n\"\n            + \"  float alpha;\\n\"\n            + \"  if (mask_value > 0.0) {\\n\"\n            + \"    alpha = (pow(2.0 * mask_value, 1.5) - 1.0) * scale + 1.0;\\n\"\n            + \"  } else { \\n\"\n            + \"    alpha = (pow(2.0 * mask_value, 2.0) - 1.0) * scale + 1.0;\\n\"\n            + \"  }\\n\"\n            + \"  float index = energy * hist_scale + hist_offset;\\n\"\n            + \"  vec4 temp = texture2D(tex_sampler_1, vec2(index, 0.5));\\n\"\n            + \"  float value = temp.g + temp.r * shift_scale;\\n\"\n            + \"  index = value * density_scale + density_offset;\\n\"\n            + \"  temp = texture2D(tex_sampler_2, vec2(index, 0.5));\\n\"\n            + \"  value = temp.g + temp.r * shift_scale;\\n\"\n            + \"  float dst_energy = energy * alpha + value * (1.0 - alpha);\\n\"\n            + \"  float max_energy = energy / max(color.r, max(color.g, color.b));\\n\"\n            + \"  if (dst_energy > max_energy) {\\n\"\n            + \"    dst_energy = max_energy;\\n\"\n            + \"  }\\n\"\n            + \"  if (energy == 0.0) {\\n\"\n            + \"    gl_FragColor = color;\\n\"\n            + \"  } else {\\n\"\n            + \"    gl_FragColor = vec4(color.rgb * dst_energy / energy, color.a);\\n\"\n            + \"  }\\n\"\n            + \"}\\n\";\n\n    private float scale = 1.0f;\n    private int scaleLocation = -1;\n\n    public CLASSTOKEN() { }\n\n    /**\n     * A parameter between 0 and 1. Zero means no adjustment, while 1 indicates\n     * the maximum amount of adjustment.\n     *\n     * @param scale scale\n     */\n    public void setScale(float scale) {\n        if (scale < 0.0f) scale = 0.0f;\n        if (scale > 1.0f) scale = 1.0f;\n        this.scale = scale;\n    }\n\n    /**\n     * Returns the current scale.\n     *\n     * @see #setScale(float)\n     * @return current scale\n     */\n    public float getScale() {\n        return scale;\n    }\n\n    @Override\n    public void setParameter1(float value) {\n        setScale(value);\n    }\n\n    @Override\n    public float getParameter1() {\n        return getScale();\n    }\n\n    @NonNull\n    @Override\n    public String getFragmentShader() {\n        return FRAGMENT_SHADER;\n    }\n\n    @Override\n    public void onCreate(int programHandle) {\n        super.onCreate(programHandle);\n        scaleLocation = GLES20.glGetUniformLocation(programHandle, \"scale\");\n        Egloo.checkGlProgramLocation(scaleLocation, \"scale\");\n    }\n\n    @Override\n    public void onDestroy() {\n        super.onDestroy();\n        scaleLocation = -1;\n    }\n\n    @Override\n    protected void onPreDraw(long timestampUs, @NonNull float[] transformMatrix) {\n        super.onPreDraw(timestampUs, transformMatrix);\n        GLES20.glUniform1f(scaleLocation, scale);\n        Egloo.checkGlError(\"glUniform1f\");\n    }\n", "target": "auto fix filter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/mappers/Camera1Mapper.java:Camera1Mapper:0", "source": "\n\n    private static CLASSTOKEN sInstance;\n\n    @NonNull\n    public static CLASSTOKEN get() {\n        if (sInstance == null) {\n            sInstance = new CLASSTOKEN();\n        }\n        return sInstance;\n    }\n\n    private static final Map<Flash, String> FLASH = new HashMap<>();\n    private static final Map<WhiteBalance, String> WB = new HashMap<>();\n    private static final Map<Facing, Integer> FACING = new HashMap<>();\n    private static final Map<Hdr, String> HDR = new HashMap<>();\n\n    static {\n        FLASH.put(Flash.OFF, Camera.Parameters.FLASH_MODE_OFF);\n        FLASH.put(Flash.ON, Camera.Parameters.FLASH_MODE_ON);\n        FLASH.put(Flash.AUTO, Camera.Parameters.FLASH_MODE_AUTO);\n        FLASH.put(Flash.TORCH, Camera.Parameters.FLASH_MODE_TORCH);\n        FACING.put(Facing.BACK, Camera.CameraInfo.CAMERA_FACING_BACK);\n        FACING.put(Facing.FRONT, Camera.CameraInfo.CAMERA_FACING_FRONT);\n        WB.put(WhiteBalance.AUTO, Camera.Parameters.WHITE_BALANCE_AUTO);\n        WB.put(WhiteBalance.INCANDESCENT, Camera.Parameters.WHITE_BALANCE_INCANDESCENT);\n        WB.put(WhiteBalance.FLUORESCENT, Camera.Parameters.WHITE_BALANCE_FLUORESCENT);\n        WB.put(WhiteBalance.DAYLIGHT, Camera.Parameters.WHITE_BALANCE_DAYLIGHT);\n        WB.put(WhiteBalance.CLOUDY, Camera.Parameters.WHITE_BALANCE_CLOUDY_DAYLIGHT);\n        HDR.put(Hdr.OFF, Camera.Parameters.SCENE_MODE_AUTO);\n        if (Build.VERSION.SDK_INT >= 17) {\n            HDR.put(Hdr.ON, Camera.Parameters.SCENE_MODE_HDR);\n        } else {\n            HDR.put(Hdr.ON, \"hdr\");\n        }\n    }\n\n    private CLASSTOKEN() {}\n\n    @NonNull\n    public String mapFlash(@NonNull Flash flash) {\n        //noinspection ConstantConditions\n        return FLASH.get(flash);\n    }\n\n    public int mapFacing(@NonNull Facing facing) {\n        //noinspection ConstantConditions\n        return FACING.get(facing);\n    }\n\n    @NonNull\n    public String mapWhiteBalance(@NonNull WhiteBalance whiteBalance) {\n        //noinspection ConstantConditions\n        return WB.get(whiteBalance);\n    }\n\n    @NonNull\n    public String mapHdr(@NonNull Hdr hdr) {\n        //noinspection ConstantConditions\n        return HDR.get(hdr);\n    }\n\n    @Nullable\n    public Flash unmapFlash(@NonNull String cameraConstant) {\n        return reverseLookup(FLASH, cameraConstant);\n    }\n\n    @Nullable\n    public Facing unmapFacing(int cameraConstant) {\n        return reverseLookup(FACING, cameraConstant);\n    }\n\n    @Nullable\n    public WhiteBalance unmapWhiteBalance(@NonNull String cameraConstant) {\n        return reverseLookup(WB, cameraConstant);\n    }\n\n    @Nullable\n    public Hdr unmapHdr(@NonNull String cameraConstant) {\n        return reverseLookup(HDR, cameraConstant);\n    }\n\n    @Nullable\n    private <C extends Control, T> C reverseLookup(@NonNull Map<C, T> map, @NonNull T object) {\n        for (C value : map.keySet()) {\n            if (object.equals(map.get(value))) {\n                return value;\n            }\n        }\n        return null;\n    }\n", "target": "camera 1 mapper"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filters/DocumentaryFilter.java:DocumentaryFilter:0", "source": "\n\n    private final static Random RANDOM = new Random();\n    private final static String FRAGMENT_SHADER = \"#extension GL_OES_EGL_image_external : require\\n\"\n            + \"precision mediump float;\\n\"\n            + \"uniform samplerExternalOES sTexture;\\n\"\n            + \"vec2 seed;\\n\"\n            + \"float stepsize;\\n\"\n            + \"uniform float inv_max_dist;\\n\"\n            + \"uniform vec2 scale;\\n\"\n            + \"varying vec2 \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\";\\n\"\n            + \"float rand(vec2 loc) {\\n\"\n            + \"  float theta1 = dot(loc, vec2(0.9898, 0.233));\\n\"\n            + \"  float theta2 = dot(loc, vec2(12.0, 78.0));\\n\"\n            + \"  float value = cos(theta1) * sin(theta2) + sin(theta1) * cos(theta2);\\n\"\n            +\n            // keep value of part1 in range: (2^-14 to 2^14).\n            \"  float temp = mod(197.0 * value, 1.0) + value;\\n\"\n            + \"  float part1 = mod(220.0 * temp, 1.0) + temp;\\n\"\n            + \"  float part2 = value * 0.5453;\\n\"\n            + \"  float part3 = cos(theta1 + theta2) * 0.43758;\\n\"\n            + \"  return fract(part1 + part2 + part3);\\n\"\n            + \"}\\n\"\n            + \"void main() {\\n\"\n            + \"  seed[0] = \" + RANDOM.nextFloat() + \";\\n\"\n            + \"  seed[1] = \" + RANDOM.nextFloat() + \";\\n\"\n            + \"  stepsize = \" + 1.0f / 255.0f + \";\\n\"\n\n            // black white\n            + \"  vec4 color = texture2D(sTexture, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\");\\n\"\n            + \"  float dither = rand(\"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\" + seed);\\n\"\n            + \"  vec3 xform = clamp(2.0 * color.rgb, 0.0, 1.0);\\n\"\n            + \"  vec3 temp = clamp(2.0 * (color.rgb + stepsize), 0.0, 1.0);\\n\"\n            + \"  vec3 new_color = clamp(xform + (temp - xform) * (dither - 0.5), 0.0, 1.0);\\n\"\n            // grayscale\n            + \"  float gray = dot(new_color, vec3(0.299, 0.587, 0.114));\\n\"\n            + \"  new_color = vec3(gray, gray, gray);\\n\"\n            // vignette\n            + \"  vec2 coord = \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\" - vec2(0.5, 0.5);\\n\"\n            + \"  float dist = length(coord * scale);\\n\"\n            + \"  float lumen = 0.85 / (1.0 + exp((dist * inv_max_dist - 0.83) * 20.0)) + 0.15;\\n\"\n            + \"  gl_FragColor = vec4(new_color * lumen, color.a);\\n\"\n            + \"}\\n\";\n\n    private int mWidth = 1;\n    private int mHeight = 1;\n    private int mScaleLocation = -1;\n    private int mMaxDistLocation = -1;\n\n    public CLASSTOKEN() { }\n\n    @Override\n    public void setSize(int width, int height) {\n        super.setSize(width, height);\n        mWidth = width;\n        mHeight = height;\n    }\n\n    @NonNull\n    @Override\n    public String getFragmentShader() {\n        return FRAGMENT_SHADER;\n    }\n\n    @Override\n    public void onCreate(int programHandle) {\n        super.onCreate(programHandle);\n        mScaleLocation = GLES20.glGetUniformLocation(programHandle, \"scale\");\n        Egloo.checkGlProgramLocation(mScaleLocation, \"scale\");\n        mMaxDistLocation = GLES20.glGetUniformLocation(programHandle, \"inv_max_dist\");\n        Egloo.checkGlProgramLocation(mMaxDistLocation, \"inv_max_dist\");\n    }\n\n    @Override\n    public void onDestroy() {\n        super.onDestroy();\n        mScaleLocation = -1;\n        mMaxDistLocation = -1;\n    }\n\n    @Override\n    protected void onPreDraw(long timestampUs, @NonNull float[] transformMatrix) {\n        super.onPreDraw(timestampUs, transformMatrix);\n        float[] scale = new float[2];\n        if (mWidth > mHeight) {\n            scale[0] = 1f;\n            scale[1] = ((float) mHeight) / mWidth;\n        } else {\n            scale[0] = ((float) mWidth) / mHeight;\n            scale[1] = 1f;\n        }\n        GLES20.glUniform2fv(mScaleLocation, 1, scale, 0);\n        Egloo.checkGlError(\"glUniform2fv\");\n\n        float maxDist = ((float) Math.sqrt(scale[0] * scale[0] + scale[1] * scale[1])) * 0.5f;\n        float invMaxDist = 1F / maxDist;\n        GLES20.glUniform1f(mMaxDistLocation, invMaxDist);\n        Egloo.checkGlError(\"glUniform1f\");\n\n    }\n", "target": "documentary filter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/overlay/OverlayLayout.java:OverlayLayout:0", "source": "\n\n    private static final String TAG = CLASSTOKEN.class.getSimpleName();\n    private static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    @VisibleForTesting Target currentTarget = Target.PREVIEW;\n\n    private boolean mHardwareCanvasEnabled;\n\n    /**\n     * We set {@link #setWillNotDraw(boolean)} to false even if we don't draw anything.\n     * This ensures that the View system will call {@link #draw(Canvas)} on us instead\n     * of short-circuiting to {@link #dispatchDraw(Canvas)}.\n     *\n     * That would be a problem for us since we use {@link #draw(Canvas)} to understand if\n     * we are currently drawing on the preview or not.\n     *\n     * @param context a context\n     */\n    public CLASSTOKEN(@NonNull Context context) {\n        super(context);\n        setWillNotDraw(false);\n    }\n\n    /**\n     * Returns true if this {@link AttributeSet} belongs to an overlay.\n     * @param set an attribute set\n     * @return true if overlay\n     */\n    public boolean isOverlay(@Nullable AttributeSet set) {\n        if (set == null) return false;\n        TypedArray a = getContext().obtainStyledAttributes(set, R.styleable.CameraView_Layout);\n        boolean isOverlay =\n                a.hasValue(R.styleable.CameraView_Layout_layout_drawOnPreview)\n                || a.hasValue(R.styleable.CameraView_Layout_layout_drawOnPictureSnapshot)\n                || a.hasValue(R.styleable.CameraView_Layout_layout_drawOnVideoSnapshot);\n        a.recycle();\n        return isOverlay;\n    }\n\n    /**\n     * Returns true if this {@link ViewGroup.LayoutParams} belongs to an overlay.\n     * @param params a layout params\n     * @return true if overlay\n     */\n    public boolean isOverlay(@NonNull ViewGroup.LayoutParams params) {\n        return params instanceof LayoutParams;\n    }\n\n    /**\n     * Generates our own overlay layout params.\n     * @param attrs input attrs\n     * @return our params\n     */\n    @Override\n    public CLASSTOKEN.LayoutParams generateLayoutParams(AttributeSet attrs) {\n        return new LayoutParams(getContext(), attrs);\n    }\n\n    /**\n     * This is called by the View hierarchy, so at this point we are\n     * likely drawing on the preview.\n     * @param canvas View canvas\n     */\n    @SuppressLint(\"MissingSuperCall\")\n    @Override\n    public void draw(Canvas canvas) {\n        LOG.i(\"normal draw called.\");\n        if (drawsOn(Target.PREVIEW)) {\n            drawOn(Target.PREVIEW, canvas);\n        }\n    }\n\n    @Override\n    public boolean drawsOn(@NonNull Target target) {\n        for (int i = 0; i < getChildCount(); i++) {\n            LayoutParams params = (LayoutParams) getChildAt(i).getLayoutParams();\n            if (params.drawsOn(target)) return true;\n        }\n        return false;\n    }\n\n    @Override\n    public void setHardwareCanvasEnabled(boolean on) {\n        mHardwareCanvasEnabled = on;\n    }\n\n    @Override\n    public boolean getHardwareCanvasEnabled() {\n        return mHardwareCanvasEnabled;\n    }\n\n    /**\n     * For {@link Target#PREVIEW}, this method is called by the View hierarchy. We will\n     * just forward the call to super.\n     *\n     * For {@link Target#PICTURE_SNAPSHOT} and {@link Target#VIDEO_SNAPSHOT},\n     * this method is called by the overlay drawer. We call {@link #dispatchDraw(Canvas)}\n     * to draw our children only.\n     *\n     * @param target the draw target\n     * @param canvas the canvas\n     */\n    @Override\n    public void drawOn(@NonNull Target target, @NonNull Canvas canvas) {\n        synchronized (this) {\n            currentTarget = target;\n            switch (target) {\n                case PREVIEW:\n                    super.draw(canvas);\n                    break;\n                case VIDEO_SNAPSHOT:\n                case PICTURE_SNAPSHOT:\n                    canvas.save();\n                    // The input canvas size is that of the preview stream, cropped to match\n                    // the view aspect ratio (this op is done by picture & video recorder).\n                    // So the aspect ratio is guaranteed to be the same, but we might have\n                    // to apply some scale (typically > 1).\n                    float widthScale = canvas.getWidth() / (float) getWidth();\n                    float heightScale = canvas.getHeight() / (float) getHeight();\n                    LOG.v(\"draw\",\n                            \"target:\", target,\n                            \"canvas:\", canvas.getWidth() + \"x\" + canvas.getHeight(),\n                            \"view:\", getWidth() + \"x\" + getHeight(),\n                            \"widthScale:\", widthScale,\n                            \"heightScale:\", heightScale,\n                            \"hardwareCanvasMode:\", mHardwareCanvasEnabled\n                    );\n                    canvas.scale(widthScale, heightScale);\n                    dispatchDraw(canvas);\n                    canvas.restore();\n                    break;\n            }\n        }\n    }\n\n    /**\n     * We end up here in all three cases, and should filter out\n     * views that are not meant to be drawn on that specific surface.\n     */\n    @Override\n    protected boolean drawChild(Canvas canvas, View child, long drawingTime) {\n        LayoutParams params = (LayoutParams) child.getLayoutParams();\n        if (params.drawsOn(currentTarget)) {\n            LOG.v(\"Performing drawing for view:\", child.getClass().getSimpleName(),\n                    \"target:\", currentTarget,\n                    \"params:\", params);\n            return doDrawChild(canvas, child, drawingTime);\n        } else {\n            LOG.v(\"Skipping drawing for view:\", child.getClass().getSimpleName(),\n                    \"target:\", currentTarget,\n                    \"params:\", params);\n            return false;\n        }\n    }\n\n    @VisibleForTesting\n    boolean doDrawChild(Canvas canvas, View child, long drawingTime) {\n        return super.drawChild(canvas, child, drawingTime);\n    }\n\n    @SuppressWarnings(\"WeakerAccess\")\n    public static class LayoutParams extends FrameLayout.LayoutParams {\n\n        @SuppressWarnings(\"unused\")\n        public boolean drawOnPreview = false;\n        public boolean drawOnPictureSnapshot = false;\n        public boolean drawOnVideoSnapshot = false;\n\n        public LayoutParams(int width, int height) {\n            super(width, height);\n        }\n\n        public LayoutParams(@NonNull Context context, @NonNull AttributeSet attrs) {\n            super(context, attrs);\n            TypedArray a = context.obtainStyledAttributes(attrs, R.styleable.CameraView_Layout);\n            try {\n                drawOnPreview = a.getBoolean(R.styleable.CameraView_Layout_layout_drawOnPreview,\n                        false);\n                drawOnPictureSnapshot = a.getBoolean(\n                        R.styleable.CameraView_Layout_layout_drawOnPictureSnapshot, false);\n                drawOnVideoSnapshot = a.getBoolean(\n                        R.styleable.CameraView_Layout_layout_drawOnVideoSnapshot, false);\n            } finally {\n                a.recycle();\n            }\n        }\n\n        @VisibleForTesting\n        boolean drawsOn(@NonNull Target target) {\n            return ((target == Target.PREVIEW && drawOnPreview)\n                    || (target == Target.VIDEO_SNAPSHOT && drawOnVideoSnapshot)\n                    || (target == Target.PICTURE_SNAPSHOT && drawOnPictureSnapshot));\n        }\n\n        @NonNull\n        @Override\n        public String toString() {\n            return getClass().getName() + \"[\"\n                    + \"drawOnPreview:\" + drawOnPreview\n                    + \",drawOnPictureSnapshot:\" + drawOnPictureSnapshot\n                    + \",drawOnVideoSnapshot:\" + drawOnVideoSnapshot\n                    + \"]\";\n        }\n    }\n", "target": "overlay layout"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/picture/Snapshot2PictureRecorder.java:Snapshot2PictureRecorder:0", "source": "\n\n    private final static long LOCK_TIMEOUT = 2500;\n\n    private class FlashAction extends BaseAction {\n\n        @Override\n        protected void onStart(@NonNull ActionHolder holder) {\n            super.onStart(holder);\n            LOG.i(\"FlashAction:\", \"Parameters locked, opening torch.\");\n            holder.getBuilder(this).set(CaptureRequest.FLASH_MODE,\n                    CaptureRequest.FLASH_MODE_TORCH);\n            holder.getBuilder(this).set(CaptureRequest.CONTROL_AE_MODE,\n                    CaptureRequest.CONTROL_AE_MODE_ON);\n            holder.applyBuilder(this);\n        }\n\n        @Override\n        public void onCaptureCompleted(@NonNull ActionHolder holder,\n                                       @NonNull CaptureRequest request,\n                                       @NonNull TotalCaptureResult result) {\n            super.onCaptureCompleted(holder, request, result);\n            Integer flashState = result.get(CaptureResult.FLASH_STATE);\n            if (flashState == null) {\n                LOG.w(\"FlashAction:\", \"Waiting flash, but flashState is null!\",\n                        \"Taking snapshot.\");\n                setState(STATE_COMPLETED);\n            } else if (flashState == CaptureResult.FLASH_STATE_FIRED) {\n                LOG.i(\"FlashAction:\", \"Waiting flash and we have FIRED state!\",\n                        \"Taking snapshot.\");\n                setState(STATE_COMPLETED);\n            } else {\n                LOG.i(\"FlashAction:\", \"Waiting flash but flashState is\",\n                        flashState, \". Waiting...\");\n            }\n        }\n    }\n\n    private class ResetFlashAction extends BaseAction {\n\n        @Override\n        protected void onStart(@NonNull ActionHolder holder) {\n            super.onStart(holder);\n            try {\n                // See Camera2Engine.setFlash() comments: turning TORCH off has bugs and we must do\n                // as follows.\n                LOG.i(\"ResetFlashAction:\", \"Reverting the flash changes.\");\n                CaptureRequest.Builder builder = holder.getBuilder(this);\n                builder.set(CaptureRequest.CONTROL_AE_MODE, CaptureRequest.CONTROL_AE_MODE_ON);\n                builder.set(CaptureRequest.FLASH_MODE, CaptureResult.FLASH_MODE_OFF);\n                holder.applyBuilder(this, builder);\n                builder.set(CaptureRequest.CONTROL_AE_MODE, mOriginalAeMode);\n                builder.set(CaptureRequest.FLASH_MODE, mOriginalFlashMode);\n                holder.applyBuilder(this);\n            } catch (CameraAccessException ignore) {}\n        }\n    }\n\n    private final Action mAction;\n    private final ActionHolder mHolder;\n    private final boolean mActionNeeded;\n    private Integer mOriginalAeMode;\n    private Integer mOriginalFlashMode;\n\n    public CLASSTOKEN(@NonNull PictureResult.Stub stub,\n                                    @NonNull Camera2Engine engine,\n                                    @NonNull RendererCameraPreview preview,\n                                    @NonNull AspectRatio outputRatio) {\n        super(stub, engine, preview, outputRatio, engine.getOverlay());\n        mHolder = engine;\n\n        mAction = Actions.sequence(\n                Actions.timeout(LOCK_TIMEOUT, new LockAction()),\n                new FlashAction());\n        mAction.addCallback(new CompletionCallback() {\n            @Override\n            protected void onActionCompleted(@NonNull Action action) {\n                LOG.i(\"Taking picture with super.take().\");\n                CLASSTOKEN.super.take();\n            }\n        });\n\n        CaptureResult lastResult = mHolder.getLastResult(mAction);\n        if (lastResult == null) {\n            LOG.w(\"Picture snapshot requested very early, before the first preview frame.\",\n                    \"Metering might not work as intended.\");\n        }\n        Integer aeState = lastResult == null ? null\n                : lastResult.get(CaptureResult.CONTROL_AE_STATE);\n        mActionNeeded = engine.getPictureSnapshotMetering()\n                && aeState != null\n                && aeState == CaptureResult.CONTROL_AE_STATE_FLASH_REQUIRED;\n        mOriginalAeMode = mHolder.getBuilder(mAction).get(CaptureRequest.CONTROL_AE_MODE);\n        mOriginalFlashMode = mHolder.getBuilder(mAction).get(CaptureRequest.FLASH_MODE);\n    }\n\n    @Override\n    public void take() {\n        if (!mActionNeeded) {\n            LOG.i(\"take:\", \"Engine does no metering or needs no flash.\",\n                    \"Taking fast snapshot.\");\n            super.take();\n        } else {\n            LOG.i(\"take:\", \"Engine needs flash. Starting action\");\n            mAction.start(mHolder);\n        }\n    }\n\n    @Override\n    protected void dispatchResult() {\n        // Revert our changes.\n        new ResetFlashAction().start(mHolder);\n        super.dispatchResult();\n    }\n", "target": "snapshot 2 picture recorder"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filters/GrainFilter.java:GrainFilter:0", "source": "\n\n    private final static Random RANDOM = new Random();\n    private final static String FRAGMENT_SHADER = \"#extension GL_OES_EGL_image_external : require\\n\"\n            + \"precision mediump float;\\n\"\n            + \"vec2 seed;\\n\"\n            + \"varying vec2 \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\";\\n\"\n            + \"uniform samplerExternalOES tex_sampler_0;\\n\"\n            + \"uniform samplerExternalOES tex_sampler_1;\\n\"\n            + \"uniform float scale;\\n\"\n            + \"uniform float stepX;\\n\"\n            + \"uniform float stepY;\\n\"\n            + \"float rand(vec2 loc) {\\n\"\n            + \"  float theta1 = dot(loc, vec2(0.9898, 0.233));\\n\"\n            + \"  float theta2 = dot(loc, vec2(12.0, 78.0));\\n\"\n            + \"  float value = cos(theta1) * sin(theta2) + sin(theta1) * cos(theta2);\\n\"\n            // keep value of part1 in range: (2^-14 to 2^14).\n            + \"  float temp = mod(197.0 * value, 1.0) + value;\\n\"\n            + \"  float part1 = mod(220.0 * temp, 1.0) + temp;\\n\"\n            + \"  float part2 = value * 0.5453;\\n\"\n            + \"  float part3 = cos(theta1 + theta2) * 0.43758;\\n\"\n            + \"  float sum = (part1 + part2 + part3);\\n\"\n            + \"  return fract(sum)*scale;\\n\"\n            + \"}\\n\"\n            + \"void main() {\\n\"\n            + \"  seed[0] = \" + RANDOM.nextFloat() + \";\\n\"\n            + \"  seed[1] = \" + RANDOM.nextFloat() + \";\\n\"\n            + \"  float noise = texture2D(tex_sampler_1, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME\n            + \" + vec2(-stepX, -stepY)).r * 0.224;\\n\"\n            + \"  noise += texture2D(tex_sampler_1, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME\n            + \" + vec2(-stepX, stepY)).r * 0.224;\\n\"\n            + \"  noise += texture2D(tex_sampler_1, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME\n            + \" + vec2(stepX, -stepY)).r * 0.224;\\n\"\n            + \"  noise += texture2D(tex_sampler_1, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME\n            + \" + vec2(stepX, stepY)).r * 0.224;\\n\"\n            + \"  noise += 0.4448;\\n\"\n            + \"  noise *= scale;\\n\"\n            + \"  vec4 color = texture2D(tex_sampler_0, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME\n            + \");\\n\"\n            + \"  float energy = 0.33333 * color.r + 0.33333 * color.g + 0.33333 * color.b;\\n\"\n            + \"  float mask = (1.0 - sqrt(energy));\\n\"\n            + \"  float weight = 1.0 - 1.333 * mask * noise;\\n\"\n            + \"  gl_FragColor = vec4(color.rgb * weight, color.a);\\n\"\n            + \"  gl_FragColor = gl_FragColor+vec4(rand(\"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME\n            + \" + seed), rand(\"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\" + seed),rand(\"\n            + DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\" + seed),1);\\n\"\n            + \"}\\n\";\n\n    private float strength = 0.5f;\n    private int width = 1;\n    private int height = 1;\n    private int strengthLocation = -1;\n    private int stepXLocation = -1;\n    private int stepYLocation = -1;\n\n    public CLASSTOKEN() { }\n\n    @Override\n    public void setSize(int width, int height) {\n        super.setSize(width, height);\n        this.width = width;\n        this.height = height;\n    }\n\n    /**\n     * Sets the current distortion strength.\n     * 0.0: no distortion.\n     * 1.0: maximum distortion.\n     *\n     * @param strength strength\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public void setStrength(float strength) {\n        if (strength < 0.0f) strength = 0.0f;\n        if (strength > 1.0f) strength = 1.0f;\n        this.strength = strength;\n    }\n\n    /**\n     * Returns the current strength.\n     *\n     * @see #setStrength(float)\n     * @return strength\n     */\n    @SuppressWarnings({\"unused\", \"WeakerAccess\"})\n    public float getStrength() {\n        return strength;\n    }\n\n    @Override\n    public void setParameter1(float value) {\n        setStrength(value);\n    }\n\n    @Override\n    public float getParameter1() {\n        return getStrength();\n    }\n\n    @NonNull\n    @Override\n    public String getFragmentShader() {\n        return FRAGMENT_SHADER;\n    }\n\n    @Override\n    public void onCreate(int programHandle) {\n        super.onCreate(programHandle);\n        strengthLocation = GLES20.glGetUniformLocation(programHandle, \"scale\");\n        Egloo.checkGlProgramLocation(strengthLocation, \"scale\");\n        stepXLocation = GLES20.glGetUniformLocation(programHandle, \"stepX\");\n        Egloo.checkGlProgramLocation(stepXLocation, \"stepX\");\n        stepYLocation = GLES20.glGetUniformLocation(programHandle, \"stepY\");\n        Egloo.checkGlProgramLocation(stepYLocation, \"stepY\");\n    }\n\n    @Override\n    public void onDestroy() {\n        super.onDestroy();\n        strengthLocation = -1;\n        stepXLocation = -1;\n        stepYLocation = -1;\n    }\n\n    @Override\n    protected void onPreDraw(long timestampUs, @NonNull float[] transformMatrix) {\n        super.onPreDraw(timestampUs, transformMatrix);\n        GLES20.glUniform1f(strengthLocation, strength);\n        Egloo.checkGlError(\"glUniform1f\");\n        GLES20.glUniform1f(stepXLocation, 0.5f / width);\n        Egloo.checkGlError(\"glUniform1f\");\n        GLES20.glUniform1f(stepYLocation, 0.5f / height);\n        Egloo.checkGlError(\"glUniform1f\");\n    }\n", "target": "grain filter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filter/NoFilter.java:NoFilter:0", "source": "\n\n    @NonNull\n    @Override\n    public String getFragmentShader() {\n        return createDefaultFragmentShader();\n    }\n", "target": "no filter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/encoding/VideoConfig.java:VideoConfig:0", "source": "\n    public int width;\n    public int height;\n    public int bitRate;\n    public int frameRate;\n    public int rotation;\n    public String mimeType;\n    public String encoder;\n\n    protected <C extends CLASSTOKEN> void copy(@NonNull C output) {\n        output.width = this.width;\n        output.height = this.height;\n        output.bitRate = this.bitRate;\n        output.frameRate = this.frameRate;\n        output.rotation = this.rotation;\n        output.mimeType = this.mimeType;\n        output.encoder = this.encoder;\n    }\n", "target": "video config"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/action/CompletionCallback.java:CompletionCallback:0", "source": "\n\n    @Override\n    public final void onActionStateChanged(@NonNull Action action, int state) {\n        if (state == Action.STATE_COMPLETED) {\n            onActionCompleted(action);\n        }\n    }\n\n    /**\n     * The given action has just reached the completed state.\n     * @param action action\n     */\n    protected abstract void onActionCompleted(@NonNull Action action);\n", "target": "completion callback"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/lock/ExposureLock.java:ExposureLock:0", "source": "\n\n    private final static String TAG = CLASSTOKEN.class.getSimpleName();\n    private final static CameraLogger LOG = CameraLogger.create(TAG);\n\n    @Override\n    protected boolean checkIsSupported(@NonNull ActionHolder holder) {\n        boolean isNotLegacy = readCharacteristic(\n                CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL, -1)\n                != CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY;\n        // Not sure we should check aeMode as well, probably all aeModes support locking,\n        // but this should not be a big issue since we're not even using different AE modes.\n        Integer aeMode = holder.getBuilder(this).get(CaptureRequest.CONTROL_AE_MODE);\n        boolean isAEOn = aeMode != null &&\n                (aeMode == CameraCharacteristics.CONTROL_AE_MODE_ON\n                        || aeMode == CameraCharacteristics.CONTROL_AE_MODE_ON_ALWAYS_FLASH\n                        || aeMode == CameraCharacteristics.CONTROL_AE_MODE_ON_AUTO_FLASH\n                        || aeMode == CameraCharacteristics.CONTROL_AE_MODE_ON_AUTO_FLASH_REDEYE\n                        || aeMode == 5\n                        /* CameraCharacteristics.CONTROL_AE_MODE_ON_EXTERNAL_FLASH, API 28 */);\n        boolean result = isNotLegacy && isAEOn;\n        LOG.i(\"checkIsSupported:\", result);\n        return result;\n    }\n\n    @Override\n    protected boolean checkShouldSkip(@NonNull ActionHolder holder) {\n        CaptureResult lastResult = holder.getLastResult(this);\n        if (lastResult != null) {\n            Integer aeState = lastResult.get(CaptureResult.CONTROL_AE_STATE);\n            boolean result = aeState != null && aeState == CaptureResult.CONTROL_AE_STATE_LOCKED;\n            LOG.i(\"checkShouldSkip:\", result);\n            return result;\n        } else {\n            LOG.i(\"checkShouldSkip: false - lastResult is null.\");\n            return false;\n        }\n    }\n\n    @Override\n    protected void onStarted(@NonNull ActionHolder holder) {\n        int cancelTrigger = Build.VERSION.SDK_INT >= 23\n                ? CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_CANCEL\n                : CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_IDLE;\n        holder.getBuilder(this).set(CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER,\n                cancelTrigger);\n        holder.getBuilder(this).set(CaptureRequest.CONTROL_AE_LOCK, true);\n        holder.applyBuilder(this);\n    }\n\n    @Override\n    public void onCaptureCompleted(@NonNull ActionHolder holder,\n                                   @NonNull CaptureRequest request,\n                                   @NonNull TotalCaptureResult result) {\n        super.onCaptureCompleted(holder, request, result);\n        Integer aeState = result.get(CaptureResult.CONTROL_AE_STATE);\n        LOG.i(\"processCapture:\", \"aeState:\", aeState);\n        if (aeState == null) return;\n        switch (aeState) {\n            case CaptureRequest.CONTROL_AE_STATE_LOCKED: {\n                setState(STATE_COMPLETED);\n                break;\n            }\n            case CaptureRequest.CONTROL_AE_STATE_PRECAPTURE:\n            case CaptureRequest.CONTROL_AE_STATE_CONVERGED:\n            case CaptureRequest.CONTROL_AE_STATE_INACTIVE:\n            case CaptureRequest.CONTROL_AE_STATE_SEARCHING:\n            case CaptureRequest.CONTROL_AE_STATE_FLASH_REQUIRED: {\n                // Wait...\n                break;\n            }\n        }\n    }\n", "target": "exposure lock"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/action/SequenceAction.java:SequenceAction:0", "source": "\n    // Need to be BaseAction so we can call onStart() instead of start()\n    private final List<BaseAction> actions;\n    private int runningAction = -1;\n\n    CLASSTOKEN(@NonNull List<BaseAction> actions) {\n        this.actions = actions;\n        increaseRunningAction();\n    }\n\n    private void increaseRunningAction() {\n        boolean first = runningAction == -1;\n        boolean last = runningAction == actions.size() - 1;\n        if (last) {\n            // This was the last action. We're done.\n            setState(STATE_COMPLETED);\n        } else {\n            runningAction++;\n            actions.get(runningAction).addCallback(new ActionCallback() {\n                @Override\n                public void onActionStateChanged(@NonNull Action action, int state) {\n                    if (state == STATE_COMPLETED) {\n                        action.removeCallback(this);\n                        increaseRunningAction();\n                    }\n                }\n            });\n            if (!first) {\n                actions.get(runningAction).onStart(getHolder());\n            }\n        }\n    }\n\n    @Override\n    protected void onStart(@NonNull ActionHolder holder) {\n        super.onStart(holder);\n        if (runningAction >= 0) {\n            actions.get(runningAction).onStart(holder);\n        }\n    }\n\n    @Override\n    protected void onAbort(@NonNull ActionHolder holder) {\n        super.onAbort(holder);\n        if (runningAction >= 0) {\n            // Previous actions have been completed already.\n            // Future actions will never start. So this is OK.\n            actions.get(runningAction).onAbort(holder);\n        }\n    }\n\n    @Override\n    public void onCaptureStarted(@NonNull ActionHolder holder, @NonNull CaptureRequest request) {\n        super.onCaptureStarted(holder, request);\n        if (runningAction >= 0) {\n            actions.get(runningAction).onCaptureStarted(holder, request);\n        }\n    }\n\n    @Override\n    public void onCaptureProgressed(@NonNull ActionHolder holder,\n                                    @NonNull CaptureRequest request,\n                                    @NonNull CaptureResult result) {\n        super.onCaptureProgressed(holder, request, result);\n        if (runningAction >= 0) {\n            actions.get(runningAction).onCaptureProgressed(holder, request, result);\n        }\n    }\n\n    @Override\n    public void onCaptureCompleted(@NonNull ActionHolder holder,\n                                   @NonNull CaptureRequest request,\n                                   @NonNull TotalCaptureResult result) {\n        super.onCaptureCompleted(holder, request, result);\n        if (runningAction >= 0) {\n            actions.get(runningAction).onCaptureCompleted(holder, request, result);\n        }\n    }\n", "target": "sequence action"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/meter/WhiteBalanceReset.java:WhiteBalanceReset:0", "source": "\n\n    private static final String TAG = CLASSTOKEN.class.getSimpleName();\n    private static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    @SuppressWarnings(\"WeakerAccess\")\n    public CLASSTOKEN() {\n        super(true);\n    }\n\n    @Override\n    protected void onStarted(@NonNull ActionHolder holder, @Nullable MeteringRectangle area) {\n        LOG.w(\"onStarted:\", \"with area:\", area);\n        int maxRegions = readCharacteristic(CameraCharacteristics.CONTROL_MAX_REGIONS_AWB,\n                0);\n        if (area != null && maxRegions > 0) {\n            holder.getBuilder(this).set(CaptureRequest.CONTROL_AWB_REGIONS,\n                    new MeteringRectangle[]{area});\n            holder.applyBuilder(this);\n        }\n        setState(STATE_COMPLETED);\n    }\n", "target": "white balance reset"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/encoding/TextureMediaEncoder.java:TextureMediaEncoder:0", "source": "\n\n    private static final String TAG = CLASSTOKEN.class.getSimpleName();\n    private static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    public final static String FRAME_EVENT = \"frame\";\n    public final static String FILTER_EVENT = \"filter\";\n\n    private int mTransformRotation;\n    private EglCore mEglCore;\n    private EglWindowSurface mWindow;\n    private GlTextureDrawer mDrawer;\n    private Pool<Frame> mFramePool = new Pool<>(Integer.MAX_VALUE, new Pool.Factory<Frame>() {\n        @Override\n        public Frame create() {\n            return new Frame();\n        }\n    });\n\n    private long mFirstTimeUs = Long.MIN_VALUE;\n\n    public CLASSTOKEN(@NonNull TextureConfig config) {\n        super(config.copy());\n    }\n\n    /**\n     * Should be acquired with {@link #acquireFrame()}, filled and then passed\n     * to {@link MediaEncoderEngine#notify(String, Object)} with {@link #FRAME_EVENT}.\n     */\n    public static class Frame {\n        private Frame() {}\n\n        /**\n         * Nanoseconds, in no meaningful time-base. Will be used for offsets only.\n         * Typically this comes from {@link SurfaceTexture#getTimestamp()}.\n         */\n        public long timestampNanos;\n\n        /**\n         * Milliseconds in the {@link System#currentTimeMillis()} reference.\n         * This is actually needed/read only for the first frame.\n         */\n        public long timestampMillis;\n\n        /**\n         * The transformation matrix for the base texture.\n         */\n        public float[] transform = new float[16];\n\n        private long timestampUs() {\n            return timestampNanos / 1000L;\n        }\n    }\n\n    /**\n     * Returns a new frame to be filled. See {@link Frame} for details.\n     * @return a new frame\n     */\n    @NonNull\n    public Frame acquireFrame() {\n        if (mFramePool.isEmpty()) {\n            throw new RuntimeException(\"Need more frames than this! \" +\n                    \"Please increase the pool size.\");\n        } else {\n            //noinspection ConstantConditions\n            return mFramePool.get();\n        }\n    }\n\n    @EncoderThread\n    @Override\n    protected void onPrepare(@NonNull MediaEncoderEngine.Controller controller, long maxLengthUs) {\n        // We rotate the texture using transformRotation. Pass rotation=0 to super so that\n        // no rotation metadata is written into the output file.\n        mTransformRotation = mConfig.rotation;\n        mConfig.rotation = 0;\n        super.onPrepare(controller, maxLengthUs);\n        mEglCore = new EglCore(mConfig.eglContext, EglCore.FLAG_RECORDABLE);\n        mWindow = new EglWindowSurface(mEglCore, mSurface, true);\n        mWindow.makeCurrent();\n        mDrawer = new GlTextureDrawer(mConfig.textureId);\n    }\n\n    /**\n     * Any number of pending events greater than 1 means that we should skip this frame.\n     * To avoid skipping too many frames, we'll use 2 for now, but this just means\n     * that we'll be drawing the same frame twice.\n     *\n     * When an event is posted, the textureId data has already been updated so we're\n     * too late to draw the old one and it should be skipped.\n     *\n     * This is especially important if we perform overlay drawing here, since that\n     * makes this class thread busy and slows down the event dispatching.\n     *\n     * @param timestampUs frame timestamp\n     * @return true to render\n     */\n    @Override\n    protected boolean shouldRenderFrame(long timestampUs) {\n        if (!super.shouldRenderFrame(timestampUs)) {\n            LOG.i(\"shouldRenderFrame - Dropping frame because of super()\");\n            return false;\n        } else if (mFrameNumber <= 10) {\n            // Always render the first few frames, or muxer fails.\n            return true;\n        } else if (getPendingEvents(FRAME_EVENT) > 2) {\n            LOG.i(\"shouldRenderFrame - Dropping, we already have too many pending events:\",\n                    getPendingEvents(FRAME_EVENT));\n            return false;\n        } else {\n            return true;\n        }\n    }\n\n    @EncoderThread\n    @Override\n    protected void onEvent(@NonNull String event, @Nullable Object data) {\n        switch (event) {\n            case FILTER_EVENT:\n                //noinspection ConstantConditions\n                onFilter((Filter) data);\n                break;\n            case FRAME_EVENT:\n                //noinspection ConstantConditions\n                onFrame((Frame) data);\n                break;\n        }\n    }\n\n    private void onFilter(@NonNull Filter filter) {\n        mDrawer.setFilter(filter);\n    }\n\n    private void onFrame(@NonNull Frame frame) {\n        if (!shouldRenderFrame(frame.timestampUs())) {\n            mFramePool.recycle(frame);\n            return;\n        }\n\n        // Notify we're got the first frame and its absolute time.\n        if (mFrameNumber == 1) {\n            notifyFirstFrameMillis(frame.timestampMillis);\n        }\n\n        // Notify we have reached the max length value.\n        if (mFirstTimeUs == Long.MIN_VALUE) mFirstTimeUs = frame.timestampUs();\n        if (!hasReachedMaxLength()) {\n            boolean didReachMaxLength = (frame.timestampUs() - mFirstTimeUs) > getMaxLengthUs();\n            if (didReachMaxLength) {\n                LOG.w(\"onEvent -\",\n                        \"frameNumber:\", mFrameNumber,\n                        \"timestampUs:\", frame.timestampUs(),\n                        \"firstTimeUs:\", mFirstTimeUs,\n                        \"- reached max length! deltaUs:\", frame.timestampUs() - mFirstTimeUs);\n                notifyMaxLengthReached();\n            }\n        }\n\n        // First, drain any previous data.\n        LOG.i(\"onEvent -\",\n                \"frameNumber:\", mFrameNumber,\n                \"timestampUs:\", frame.timestampUs(),\n                \"hasReachedMaxLength:\", hasReachedMaxLength(),\n                \"thread:\", Thread.currentThread(),\n                \"- draining.\");\n        drainOutput(false);\n\n        // Then draw on the surface.\n        LOG.i(\"onEvent -\",\n                \"frameNumber:\", mFrameNumber,\n                \"timestampUs:\", frame.timestampUs(),\n                \"hasReachedMaxLength:\", hasReachedMaxLength(),\n                \"thread:\", Thread.currentThread(),\n                \"- drawing.\");\n\n        // 1. We must scale this matrix like GlCameraPreview does, because it might have some\n        // cropping. Scaling takes place with respect to the (0, 0, 0) point, so we must apply\n        // a Translation to compensate.\n        float[] transform = frame.transform;\n        float scaleX = mConfig.scaleX;\n        float scaleY = mConfig.scaleY;\n        float scaleTranslX = (1F - scaleX) / 2F;\n        float scaleTranslY = (1F - scaleY) / 2F;\n        Matrix.translateM(transform, 0, scaleTranslX, scaleTranslY, 0);\n        Matrix.scaleM(transform, 0, scaleX, scaleY, 1);\n\n        // 2. We also must rotate this matrix. In GlCameraPreview it is not needed because it is\n        // a live stream, but the output video, must be correctly rotated based on the device\n        // rotation at the moment. Rotation also takes place with respect to the origin\n        // (the Z axis), so we must translate to origin, rotate, then back to where we were.\n        Matrix.translateM(transform, 0, 0.5F, 0.5F, 0);\n        Matrix.rotateM(transform, 0, mTransformRotation, 0, 0, 1);\n        Matrix.translateM(transform, 0, -0.5F, -0.5F, 0);\n\n        // 3. Do the same for overlays with their own rotation.\n        if (mConfig.hasOverlay()) {\n            mConfig.overlayDrawer.draw(mConfig.overlayTarget);\n            Matrix.translateM(mConfig.overlayDrawer.getTransform(),\n                    0, 0.5F, 0.5F, 0);\n            Matrix.rotateM(mConfig.overlayDrawer.getTransform(),\n                    0, mConfig.overlayRotation, 0, 0, 1);\n            Matrix.translateM(mConfig.overlayDrawer.getTransform(),\n                    0, -0.5F, -0.5F, 0);\n        }\n        LOG.i(\"onEvent -\",\n                \"frameNumber:\", mFrameNumber,\n                \"timestampUs:\", frame.timestampUs(),\n                \"hasReachedMaxLength:\", hasReachedMaxLength(),\n                \"thread:\", Thread.currentThread(),\n                \"- gl rendering.\");\n        mDrawer.setTextureTransform(transform);\n        mDrawer.draw(frame.timestampUs());\n        if (mConfig.hasOverlay()) {\n            mConfig.overlayDrawer.render(frame.timestampUs());\n        }\n        mWindow.setPresentationTime(frame.timestampNanos);\n        mWindow.swapBuffers();\n        mFramePool.recycle(frame);\n        LOG.i(\"onEvent -\",\n                \"frameNumber:\", mFrameNumber,\n                \"timestampUs:\", frame.timestampUs(),\n                \"hasReachedMaxLength:\", hasReachedMaxLength(),\n                \"thread:\", Thread.currentThread(),\n                \"- gl rendered.\");\n    }\n\n    @Override\n    protected void onStopped() {\n        super.onStopped();\n        mFramePool.clear();\n        if (mWindow != null) {\n            mWindow.release();\n            mWindow = null;\n        }\n        if (mDrawer != null) {\n            mDrawer.release();\n            mDrawer = null;\n        }\n        if (mEglCore != null) {\n            mEglCore.release();\n            mEglCore = null;\n        }\n    }\n", "target": "texture media encoder"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/markers/MarkerLayout.java:MarkerLayout:0", "source": "\n\n    public final static int TYPE_AUTOFOCUS = 1;\n\n    @SuppressLint(\"UseSparseArrays\")\n    private final HashMap<Integer, View> mViews = new HashMap<>();\n\n    public CLASSTOKEN(@NonNull Context context) {\n        super(context);\n    }\n\n    /**\n     * Notifies that a new marker was added, possibly replacing another one.\n     * @param type the marker type\n     * @param marker the marker\n     */\n    public void onMarker(int type, @Nullable Marker marker) {\n        // First check if we have a view for a previous marker of this type.\n        View oldView = mViews.get(type);\n        if (oldView != null) removeView(oldView);\n        // If new marker is null, we're done.\n        if (marker == null) return;\n        // Now see if we have a new view.\n        View newView = marker.onAttach(getContext(), this);\n        if (newView != null) {\n            mViews.put(type, newView);\n            addView(newView);\n        }\n    }\n\n    /**\n     * The event that should trigger the drawing is about to be dispatched to\n     * markers. If we have a valid View, cancel any animations on it and reposition\n     * it.\n     * @param type the event type\n     * @param points the position\n     */\n    public void onEvent(int type, @NonNull PointF[] points) {\n        View view = mViews.get(type);\n        if (view == null) return;\n        view.clearAnimation();\n        if (type == TYPE_AUTOFOCUS) {\n            // TODO can't be sure that getWidth and getHeight are available here.\n            PointF point = points[0];\n            float x = (int) (point.x - view.getWidth() / 2);\n            float y = (int) (point.y - view.getHeight() / 2);\n            view.setTranslationX(x);\n            view.setTranslationY(y);\n        }\n    }\n", "target": "marker layout"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/CameraListener.java:CameraListener:0", "source": "\n\n\n    /**\n     * Notifies that the camera was opened.\n     * The {@link CameraOptions} object collects all supported options by the current camera.\n     *\n     * @param options camera supported options\n     */\n    @UiThread\n    public void onCameraOpened(@NonNull CameraOptions options) { }\n\n\n    /**\n     * Notifies that the camera session was closed.\n     */\n    @UiThread\n    public void onCameraClosed() { }\n\n\n    /**\n     * Notifies about an error during the camera setup or configuration.\n     *\n     * At this point you should inspect the {@link CameraException} reason using\n     * {@link CameraException#getReason()} and see what should be done, if anything.\n     * If the error is unrecoverable, this is the right moment to show an error dialog, for example.\n     *\n     * @param exception the error\n     */\n    @UiThread\n    public void onCameraError(@NonNull CameraException exception) { }\n\n\n    /**\n     * Notifies that a picture previously captured with {@link CameraView#takePicture()}\n     * or {@link CameraView#takePictureSnapshot()} is ready to be shown or saved to file.\n     *\n     * If planning to show a bitmap, you can use\n     * {@link PictureResult#toBitmap(int, int, BitmapCallback)} to decode the byte array\n     * taking care about orientation and threading.\n     *\n     * @param result captured picture\n     */\n    @UiThread\n    public void onPictureTaken(@NonNull PictureResult result) { }\n\n\n    /**\n     * Notifies that a video capture has just ended.\n     *\n     * @param result the video result\n     */\n    @UiThread\n    public void onVideoTaken(@NonNull VideoResult result) { }\n\n\n    /**\n     * Notifies that the device was tilted or the window offset changed.\n     * The orientation passed is exactly the counter-clockwise rotation that a View should have,\n     * in order to appear correctly oriented to the user, considering the way she is\n     * holding the device, and the native activity orientation.\n     *\n     * This is meant to be used for aligning views (e.g. buttons) to the current camera viewport.\n     *\n     * @param orientation either 0, 90, 180 or 270\n     */\n    @UiThread\n    public void onOrientationChanged(int orientation) { }\n\n\n    /**\n     * Notifies that user interacted with the screen and started metering with a gesture,\n     * and touch metering routine is trying to focus around that area.\n     * This callback can be used to draw things on screen.\n     * Can also be triggered by {@link CameraView#startAutoFocus(float, float)}.\n     *\n     * @param point coordinates with respect to CameraView.getWidth() and CameraView.getHeight()\n     */\n    @UiThread\n    public void onAutoFocusStart(@NonNull PointF point) { }\n\n\n    /**\n     * Notifies that a touch metering event just ended, and the camera converged\n     * to a new focus, exposure and possibly white balance.\n     * This might succeed or not.\n     * Can also be triggered by {@link CameraView#startAutoFocus(float, float)}.\n     *\n     * @param successful whether metering succeeded\n     * @param point coordinates with respect to CameraView.getWidth() and CameraView.getHeight()\n     */\n    @UiThread\n    public void onAutoFocusEnd(boolean successful, @NonNull PointF point) { }\n\n\n    /**\n     * Notifies that a finger gesture just caused the camera zoom\n     * to be changed. This can be used to draw, for example, a seek bar.\n     *\n     * @param newValue the new zoom value\n     * @param bounds min and max bounds for newValue (fixed to 0 ... 1)\n     * @param fingers finger positions that caused the event, null if not caused by touch\n     */\n    @UiThread\n    public void onZoomChanged(float newValue,\n                              @NonNull float[] bounds,\n                              @Nullable PointF[] fingers) { }\n\n\n    /**\n     * Noitifies that a finger gesture just caused the camera exposure correction\n     * to be changed. This can be used to draw, for example, a seek bar.\n     *\n     * @param newValue the new correction value\n     * @param bounds min and max bounds for newValue, as returned by {@link CameraOptions}\n     * @param fingers finger positions that caused the event, null if not caused by touch\n     */\n    @UiThread\n    public void onExposureCorrectionChanged(float newValue,\n                                            @NonNull float[] bounds,\n                                            @Nullable PointF[] fingers) { }\n\n\n    /**\n     * Notifies that the actual video recording has started.\n     * This is the time when actual frames recording starts.\n     *\n     * This can be used to show some UI indicator for video recording or counting time.\n     *\n     * @see #onVideoRecordingEnd()\n     */\n    @UiThread\n    public void onVideoRecordingStart() {\n\n    }\n\n    /**\n     * Notifies that the actual video recording has ended.\n     * At this point recording has ended, though the file might still be processed.\n     * The {@link #onVideoTaken(VideoResult)} callback will be called soon.\n     *\n     * This can be used to remove UI indicators for video recording.\n     *\n     * @see #onVideoRecordingStart()\n     */\n    @UiThread\n    public void onVideoRecordingEnd() {\n\n    }\n\n    /**\n     * Notifies that the picture capture has started. Can be used to update the UI for visual\n     * confirmation or sound effects.\n     */\n    @UiThread\n    public void onPictureShutter() {\n\n    }\n    \n", "target": "camera listener"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/Camera1Engine.java:Camera1Engine:0", "source": "\n    private static final String JOB_FOCUS_RESET = \"focus reset\";\n    private static final String JOB_FOCUS_END = \"focus end\";\n\n    private static final int PREVIEW_FORMAT = ImageFormat.NV21;\n    @VisibleForTesting static final int AUTOFOCUS_END_DELAY_MILLIS = 2500;\n\n    private final Camera1Mapper mMapper = Camera1Mapper.get();\n    private Camera mCamera;\n    @VisibleForTesting int mCameraId;\n\n    public CLASSTOKEN(@NonNull Callback callback) {\n        super(callback);\n    }\n\n    //region Utilities\n\n    @Override\n    public void onError(int error, Camera camera) {\n        String message = LOG.e(\"Internal Camera1 error.\", error);\n        Exception runtime = new RuntimeException(message);\n        int reason;\n        switch (error) {\n            case Camera.CAMERA_ERROR_SERVER_DIED:\n            case Camera.CAMERA_ERROR_EVICTED:\n                reason = CameraException.REASON_DISCONNECTED; break;\n            case Camera.CAMERA_ERROR_UNKNOWN: // Pass DISCONNECTED which is considered unrecoverable\n                reason = CameraException.REASON_DISCONNECTED; break;\n            default: reason = CameraException.REASON_UNKNOWN;\n        }\n        throw new CameraException(runtime, reason);\n    }\n\n    //endregion\n\n    //region Protected APIs\n\n    @EngineThread\n    @NonNull\n    @Override\n    protected List<Size> getPreviewStreamAvailableSizes() {\n        List<Camera.Size> sizes;\n        try {\n            sizes = mCamera.getParameters().getSupportedPreviewSizes();\n        } catch (Exception e) {\n            LOG.e(\"getPreviewStreamAvailableSizes:\", \"Failed to compute preview size. Camera params is empty\");\n            throw new CameraException(e, CameraException.REASON_FAILED_TO_START_PREVIEW);\n        }\n        List<Size> result = new ArrayList<>(sizes.size());\n        for (Camera.Size size : sizes) {\n            Size add = new Size(size.width, size.height);\n            if (!result.contains(add)) result.add(add);\n        }\n        LOG.i(\"getPreviewStreamAvailableSizes:\", result);\n        return result;\n    }\n\n    @EngineThread\n    @NonNull\n    @Override\n    protected List<Size> getFrameProcessingAvailableSizes() {\n        // We don't choose the frame processing size.\n        // It comes from the preview stream.\n        return Collections.singletonList(mPreviewStreamSize);\n    }\n\n    @EngineThread\n    @Override\n    protected void onPreviewStreamSizeChanged() {\n        restartPreview();\n    }\n\n    @EngineThread\n    @Override\n    protected boolean collectCameraInfo(@NonNull Facing facing) {\n        int internalFacing = mMapper.mapFacing(facing);\n        LOG.i(\"collectCameraInfo\",\n                \"Facing:\", facing,\n                \"Internal:\", internalFacing,\n                \"Cameras:\", Camera.getNumberOfCameras());\n        Camera.CameraInfo cameraInfo = new Camera.CameraInfo();\n        for (int i = 0, count = Camera.getNumberOfCameras(); i < count; i++) {\n            Camera.getCameraInfo(i, cameraInfo);\n            if (cameraInfo.facing == internalFacing) {\n                getAngles().setSensorOffset(facing, cameraInfo.orientation);\n                mCameraId = i;\n                return true;\n            }\n        }\n        return false;\n    }\n\n    //endregion\n\n    //region Start\n\n    @NonNull\n    @EngineThread\n    @Override\n    protected Task<CameraOptions> onStartEngine() {\n        try {\n            mCamera = Camera.open(mCameraId);\n        } catch (Exception e) {\n            LOG.e(\"onStartEngine:\", \"Failed to connect. Maybe in use by another app?\");\n            throw new CameraException(e, CameraException.REASON_FAILED_TO_CONNECT);\n        }\n        if (mCamera == null) {\n            LOG.e(\"onStartEngine:\", \"Failed to connect. Camera is null, maybe in use by another app or already released?\");\n            throw new CameraException(CameraException.REASON_FAILED_TO_CONNECT);\n        }\n        mCamera.setErrorCallback(this);\n\n        // Set parameters that might have been set before the camera was opened.\n        LOG.i(\"onStartEngine:\", \"Applying default parameters.\");\n        try {\n            Camera.Parameters params = mCamera.getParameters();\n            mCameraOptions = new Camera1Options(params, mCameraId,\n                    getAngles().flip(Reference.SENSOR, Reference.VIEW));\n            applyAllParameters(params);\n            mCamera.setParameters(params);\n        } catch (Exception e) {\n            LOG.e(\"onStartEngine:\", \"Failed to connect. Problem with camera params\");\n            throw new CameraException(e, CameraException.REASON_FAILED_TO_CONNECT);\n        }\n        try {\n            mCamera.setDisplayOrientation(getAngles().offset(Reference.SENSOR, Reference.VIEW,\n                    Axis.ABSOLUTE)); // <- not allowed during preview\n        } catch (Exception e) {\n            LOG.e(\"onStartEngine:\", \"Failed to connect. Can't set display orientation, maybe preview already exists?\");\n            throw new CameraException(CameraException.REASON_FAILED_TO_CONNECT);\n        }\n        LOG.i(\"onStartEngine:\", \"Ended\");\n        return Tasks.forResult(mCameraOptions);\n    }\n\n    @EngineThread\n    @NonNull\n    @Override\n    protected Task<Void> onStartBind() {\n        LOG.i(\"onStartBind:\", \"Started\");\n        try {\n            if (mPreview.getOutputClass() == SurfaceHolder.class) {\n                mCamera.setPreviewDisplay((SurfaceHolder) mPreview.getOutput());\n            } else if (mPreview.getOutputClass() == SurfaceTexture.class) {\n                mCamera.setPreviewTexture((SurfaceTexture) mPreview.getOutput());\n            } else {\n                throw new RuntimeException(\"Unknown CameraPreview output class.\");\n            }\n        } catch (IOException e) {\n            LOG.e(\"onStartBind:\", \"Failed to bind.\", e);\n            throw new CameraException(e, CameraException.REASON_FAILED_TO_START_PREVIEW);\n        }\n\n        mCaptureSize = computeCaptureSize();\n        mPreviewStreamSize = computePreviewStreamSize();\n        LOG.i(\"onStartBind:\", \"Returning\");\n        return Tasks.forResult(null);\n    }\n\n    @EngineThread\n    @NonNull\n    @Override\n    protected Task<Void> onStartPreview() {\n        LOG.i(\"onStartPreview\", \"Dispatching onCameraPreviewStreamSizeChanged.\");\n        getCallback().onCameraPreviewStreamSizeChanged();\n\n        Size previewSize = getPreviewStreamSize(Reference.VIEW);\n        if (previewSize == null) {\n            throw new IllegalStateException(\"previewStreamSize should not be null at this point.\");\n        }\n        mPreview.setStreamSize(previewSize.getWidth(), previewSize.getHeight());\n        mPreview.setDrawRotation(0);\n\n        Camera.Parameters params;\n        try {\n            params = mCamera.getParameters();\n        } catch (Exception e) {\n            LOG.e(\"onStartPreview:\", \"Failed to get params from camera. Maybe low level problem with camera or camera has already released?\");\n            throw new CameraException(e, CameraException.REASON_FAILED_TO_START_PREVIEW);\n        }\n        // NV21 should be the default, but let's make sure, since YuvImage will only support this\n        // and a few others\n        params.setPreviewFormat(ImageFormat.NV21);\n        // setPreviewSize is not allowed during preview\n        params.setPreviewSize(mPreviewStreamSize.getWidth(), mPreviewStreamSize.getHeight());\n        if (getMode() == Mode.PICTURE) {\n            // setPictureSize is allowed during preview\n            params.setPictureSize(mCaptureSize.getWidth(), mCaptureSize.getHeight());\n        } else {\n            // mCaptureSize in this case is a video size. The available video sizes are not\n            // necessarily a subset of the picture sizes, so we can't use the mCaptureSize value:\n            // it might crash. However, the setPictureSize() passed here is useless : we don't allow\n            // HQ pictures in video mode.\n            // While this might be lifted in the future, for now, just use a picture capture size.\n            Size pictureSize = computeCaptureSize(Mode.PICTURE);\n            params.setPictureSize(pictureSize.getWidth(), pictureSize.getHeight());\n        }\n        try {\n            mCamera.setParameters(params);\n        } catch (Exception e) {\n            LOG.e(\"onStartPreview:\", \"Failed to set params for camera. Maybe incorrect parameter put in params?\");\n            throw new CameraException(e, CameraException.REASON_FAILED_TO_START_PREVIEW);\n        }\n\n        mCamera.setPreviewCallbackWithBuffer(null); // Release anything left\n        mCamera.setPreviewCallbackWithBuffer(this); // Add ourselves\n        getFrameManager().setUp(PREVIEW_FORMAT, mPreviewStreamSize, getAngles());\n\n        LOG.i(\"onStartPreview\", \"Starting preview with startPreview().\");\n        try {\n            mCamera.startPreview();\n        } catch (Exception e) {\n            LOG.e(\"onStartPreview\", \"Failed to start preview.\", e);\n            throw new CameraException(e, CameraException.REASON_FAILED_TO_START_PREVIEW);\n        }\n        LOG.i(\"onStartPreview\", \"Started preview.\");\n        return Tasks.forResult(null);\n    }\n\n    //endregion\n\n    //region Stop\n\n    @EngineThread\n    @NonNull\n    @Override\n    protected Task<Void> onStopPreview() {\n        LOG.i(\"onStopPreview:\", \"Started.\");\n        if (mVideoRecorder != null) {\n            mVideoRecorder.stop(true);\n            mVideoRecorder = null;\n        }\n        mPictureRecorder = null;\n        getFrameManager().release();\n        LOG.i(\"onStopPreview:\", \"Releasing preview buffers.\");\n        mCamera.setPreviewCallbackWithBuffer(null); // Release anything left\n        try {\n            LOG.i(\"onStopPreview:\", \"Stopping preview.\");\n            mCamera.stopPreview();\n            LOG.i(\"onStopPreview:\", \"Stopped preview.\");\n        } catch (Exception e) {\n            LOG.e(\"stopPreview\", \"Could not stop preview\", e);\n        }\n        return Tasks.forResult(null);\n    }\n\n    @EngineThread\n    @NonNull\n    @Override\n    protected Task<Void> onStopBind() {\n        mPreviewStreamSize = null;\n        mCaptureSize = null;\n        try {\n            if (mPreview.getOutputClass() == SurfaceHolder.class) {\n                mCamera.setPreviewDisplay(null);\n            } else if (mPreview.getOutputClass() == SurfaceTexture.class) {\n                mCamera.setPreviewTexture(null);\n            } else {\n                throw new RuntimeException(\"Unknown CameraPreview output class.\");\n            }\n        } catch (IOException e) {\n            // NOTE: when this happens, the next onStopEngine() call hangs on camera.release(),\n            // Not sure for how long. This causes the destroy() flow to fail the timeout.\n            LOG.e(\"onStopBind\", \"Could not release surface\", e);\n        }\n        return Tasks.forResult(null);\n    }\n\n    @EngineThread\n    @NonNull\n    @Override\n    protected Task<Void> onStopEngine() {\n        LOG.i(\"onStopEngine:\", \"About to clean up.\");\n        getOrchestrator().remove(JOB_FOCUS_RESET);\n        getOrchestrator().remove(JOB_FOCUS_END);\n        if (mCamera != null) {\n            try {\n                LOG.i(\"onStopEngine:\", \"Clean up.\", \"Releasing camera.\");\n                // Just like Camera2Engine, this call can hang (at least on emulators) and if\n                // we don't find a way around the lock, it leaves the camera in a bad state.\n                // This is anticipated by the exception in onStopBind() (see above).\n                //\n                // 12:29:32.163 E Camera3-Device: Camera 0: clearStreamingRequest: Device has encountered a serious error\u001b[0m\n                // 12:29:32.163 E Camera2-StreamingProcessor: stopStream: Camera 0: Can't clear stream request: Function not implemented (-38)\u001b[0m\n                // 12:29:32.163 E Camera2Client: stopPreviewL: Camera 0: Can't stop streaming: Function not implemented (-38)\u001b[0m\n                // 12:29:32.273 E Camera2-StreamingProcessor: deletePreviewStream: Unable to delete old preview stream: Device or resource busy (-16)\u001b[0m\n                // 12:29:32.274 E Camera2-CallbackProcessor: deleteStream: Unable to delete callback stream: Device or resource busy (-16)\u001b[0m\n                // 12:29:32.274 E Camera3-Device: Camera 0: disconnect: Shutting down in an error state\u001b[0m\n                //\n                // I believe there is a thread deadlock due to this call internally waiting to\n                // dispatch some callback to us (pending captures, ...), but the callback thread\n                // is blocked here. We try to workaround this in CameraEngine.destroy().\n                mCamera.release();\n                LOG.i(\"onStopEngine:\", \"Clean up.\", \"Released camera.\");\n            } catch (Exception e) {\n                LOG.w(\"onStopEngine:\", \"Clean up.\", \"Exception while releasing camera.\", e);\n            }\n            mCamera = null;\n            mCameraOptions = null;\n        }\n        mVideoRecorder = null;\n        mCameraOptions = null;\n        mCamera = null;\n        LOG.w(\"onStopEngine:\", \"Clean up.\", \"Returning.\");\n        return Tasks.forResult(null);\n    }\n\n    //endregion\n\n    //region Pictures\n\n    @EngineThread\n    @Override\n    protected void onTakePicture(@NonNull PictureResult.Stub stub, boolean doMetering) {\n        LOG.i(\"onTakePicture:\", \"executing.\");\n        stub.rotation = getAngles().offset(Reference.SENSOR, Reference.OUTPUT,\n                Axis.RELATIVE_TO_SENSOR);\n        stub.size = getPictureSize(Reference.OUTPUT);\n        mPictureRecorder = new Full1PictureRecorder(stub, CLASSTOKEN.this, mCamera);\n        mPictureRecorder.take();\n        LOG.i(\"onTakePicture:\", \"executed.\");\n    }\n\n    @EngineThread\n    @Override\n    protected void onTakePictureSnapshot(@NonNull PictureResult.Stub stub,\n                                         @NonNull AspectRatio outputRatio,\n                                         boolean doMetering) {\n        LOG.i(\"onTakePictureSnapshot:\", \"executing.\");\n        // Not the real size: it will be cropped to match the view ratio\n        stub.size = getUncroppedSnapshotSize(Reference.OUTPUT);\n        if (mPreview instanceof RendererCameraPreview && Build.VERSION.SDK_INT >= Build.VERSION_CODES.KITKAT) {\n            stub.rotation = getAngles().offset(Reference.VIEW, Reference.OUTPUT, Axis.ABSOLUTE);\n            mPictureRecorder = new SnapshotGlPictureRecorder(stub, this,\n                    (RendererCameraPreview) mPreview, outputRatio, getOverlay());\n        } else {\n            stub.rotation = getAngles().offset(Reference.SENSOR, Reference.OUTPUT, Axis.RELATIVE_TO_SENSOR);\n            mPictureRecorder = new Snapshot1PictureRecorder(stub, this, mCamera, outputRatio);\n        }\n        mPictureRecorder.take();\n        LOG.i(\"onTakePictureSnapshot:\", \"executed.\");\n    }\n\n    //endregion\n\n    //region Videos\n\n    @EngineThread\n    @Override\n    protected void onTakeVideo(@NonNull VideoResult.Stub stub) {\n        stub.rotation = getAngles().offset(Reference.SENSOR, Reference.OUTPUT,\n                Axis.RELATIVE_TO_SENSOR);\n        stub.size = getAngles().flip(Reference.SENSOR, Reference.OUTPUT) ? mCaptureSize.flip()\n                : mCaptureSize;\n        // Unlock the camera and start recording.\n        try {\n            mCamera.unlock();\n        } catch (Exception e) {\n            // If this failed, we are unlikely able to record the video.\n            // Dispatch an error.\n            onVideoResult(null, e);\n            return;\n        }\n        mVideoRecorder = new Full1VideoRecorder(CLASSTOKEN.this, mCamera, mCameraId);\n        mVideoRecorder.start(stub);\n    }\n\n    @SuppressLint(\"NewApi\")\n    @EngineThread\n    @Override\n    protected void onTakeVideoSnapshot(@NonNull VideoResult.Stub stub,\n                                       @NonNull AspectRatio outputRatio) {\n        if (!(mPreview instanceof RendererCameraPreview)) {\n            throw new IllegalStateException(\"Video snapshots are only supported with GL_SURFACE.\");\n        }\n        if (Build.VERSION.SDK_INT < Build.VERSION_CODES.JELLY_BEAN_MR2) {\n            throw new IllegalStateException(\"Video snapshots are only supported on API 18+.\");\n        }\n        RendererCameraPreview glPreview = (RendererCameraPreview) mPreview;\n        Size outputSize = getUncroppedSnapshotSize(Reference.OUTPUT);\n        if (outputSize == null) {\n            throw new IllegalStateException(\"outputSize should not be null.\");\n        }\n        Rect outputCrop = CropHelper.computeCrop(outputSize, outputRatio);\n        outputSize = new Size(outputCrop.width(), outputCrop.height());\n        stub.size = outputSize;\n        // Vertical:               0   (270-0-0)\n        // Left (unlocked):        0   (270-90-270)\n        // Right (unlocked):       0   (270-270-90)\n        // Upside down (unlocked): 0   (270-180-180)\n        // Left (locked):          270 (270-0-270)\n        // Right (locked):         90  (270-0-90)\n        // Upside down (locked):   180 (270-0-180)\n        // The correct formula seems to be deviceOrientation+displayOffset,\n        // which means offset(Reference.VIEW, Reference.OUTPUT, Axis.ABSOLUTE).\n        stub.rotation = getAngles().offset(Reference.VIEW, Reference.OUTPUT, Axis.ABSOLUTE);\n        stub.videoFrameRate = Math.round(mPreviewFrameRate);\n        LOG.i(\"onTakeVideoSnapshot\", \"rotation:\", stub.rotation, \"size:\", stub.size);\n\n        // Start.\n        mVideoRecorder = new SnapshotVideoRecorder(CLASSTOKEN.this, glPreview, getOverlay());\n        mVideoRecorder.start(stub);\n    }\n\n    @Override\n    public void onVideoResult(@Nullable VideoResult.Stub result, @Nullable Exception exception) {\n        super.onVideoResult(result, exception);\n        if (result == null) {\n            // Something went wrong, lock the camera again.\n            mCamera.lock();\n        }\n    }\n\n    //endregion\n\n    //region Parameters\n\n    private void applyAllParameters(@NonNull Camera.Parameters params) {\n        params.setRecordingHint(getMode() == Mode.VIDEO);\n        applyDefaultFocus(params);\n        applyFlash(params, Flash.OFF);\n        applyLocation(params, null);\n        applyWhiteBalance(params, WhiteBalance.AUTO);\n        applyHdr(params, Hdr.OFF);\n        applyZoom(params, 0F);\n        applyExposureCorrection(params, 0F);\n        applyPlaySounds(mPlaySounds);\n        applyPreviewFrameRate(params, 0F);\n    }\n\n    private void applyDefaultFocus(@NonNull Camera.Parameters params) {\n        List<String> modes = params.getSupportedFocusModes();\n\n        if (getMode() == Mode.VIDEO &&\n                modes.contains(Camera.Parameters.FOCUS_MODE_CONTINUOUS_VIDEO)) {\n            params.setFocusMode(Camera.Parameters.FOCUS_MODE_CONTINUOUS_VIDEO);\n            return;\n        }\n\n        if (modes.contains(Camera.Parameters.FOCUS_MODE_CONTINUOUS_PICTURE)) {\n            params.setFocusMode(Camera.Parameters.FOCUS_MODE_CONTINUOUS_PICTURE);\n            return;\n        }\n\n        if (modes.contains(Camera.Parameters.FOCUS_MODE_INFINITY)) {\n            params.setFocusMode(Camera.Parameters.FOCUS_MODE_INFINITY);\n            return;\n        }\n\n        if (modes.contains(Camera.Parameters.FOCUS_MODE_FIXED)) {\n            params.setFocusMode(Camera.Parameters.FOCUS_MODE_FIXED);\n            //noinspection UnnecessaryReturnStatement\n            return;\n        }\n    }\n\n    @Override\n    public void setFlash(@NonNull Flash flash) {\n        final Flash old = mFlash;\n        mFlash = flash;\n        mFlashTask = getOrchestrator().scheduleStateful(\"flash (\" + flash + \")\",\n                CameraState.ENGINE,\n                new Runnable() {\n            @Override\n            public void run() {\n                Camera.Parameters params = mCamera.getParameters();\n                if (applyFlash(params, old)) mCamera.setParameters(params);\n            }\n        });\n    }\n\n    private boolean applyFlash(@NonNull Camera.Parameters params, @NonNull Flash oldFlash) {\n        if (mCameraOptions.supports(mFlash)) {\n            params.setFlashMode(mMapper.mapFlash(mFlash));\n            return true;\n        }\n        mFlash = oldFlash;\n        return false;\n    }\n\n    @Override\n    public void setLocation(@Nullable Location location) {\n        final Location oldLocation = mLocation;\n        mLocation = location;\n        mLocationTask = getOrchestrator().scheduleStateful(\"location\",\n                CameraState.ENGINE,\n                new Runnable() {\n            @Override\n            public void run() {\n                Camera.Parameters params = mCamera.getParameters();\n                if (applyLocation(params, oldLocation)) mCamera.setParameters(params);\n            }\n        });\n    }\n\n    private boolean applyLocation(@NonNull Camera.Parameters params,\n                                  @SuppressWarnings(\"unused\") @Nullable Location oldLocation) {\n        if (mLocation != null) {\n            params.setGpsLatitude(mLocation.getLatitude());\n            params.setGpsLongitude(mLocation.getLongitude());\n            params.setGpsAltitude(mLocation.getAltitude());\n            params.setGpsTimestamp(mLocation.getTime());\n            params.setGpsProcessingMethod(mLocation.getProvider());\n        }\n        return true;\n    }\n\n    @Override\n    public void setWhiteBalance(@NonNull WhiteBalance whiteBalance) {\n        final WhiteBalance old = mWhiteBalance;\n        mWhiteBalance = whiteBalance;\n        mWhiteBalanceTask = getOrchestrator().scheduleStateful(\n                \"white balance (\" + whiteBalance + \")\",\n                CameraState.ENGINE,\n                new Runnable() {\n            @Override\n            public void run() {\n                Camera.Parameters params = mCamera.getParameters();\n                if (applyWhiteBalance(params, old)) mCamera.setParameters(params);\n            }\n        });\n    }\n\n    private boolean applyWhiteBalance(@NonNull Camera.Parameters params,\n                                      @NonNull WhiteBalance oldWhiteBalance) {\n        if (mCameraOptions.supports(mWhiteBalance)) {\n            // If this lock key is present, the engine can throw when applying the\n            // parameters, not sure why. Since we never lock it, this should be\n            // harmless for the rest of the engine.\n            params.setWhiteBalance(mMapper.mapWhiteBalance(mWhiteBalance));\n            params.remove(\"auto-whitebalance-lock\");\n            return true;\n        }\n        mWhiteBalance = oldWhiteBalance;\n        return false;\n    }\n\n    @Override\n    public void setHdr(@NonNull Hdr hdr) {\n        final Hdr old = mHdr;\n        mHdr = hdr;\n        mHdrTask = getOrchestrator().scheduleStateful(\"hdr (\" + hdr + \")\",\n                CameraState.ENGINE,\n                new Runnable() {\n            @Override\n            public void run() {\n                Camera.Parameters params = mCamera.getParameters();\n                if (applyHdr(params, old)) mCamera.setParameters(params);\n            }\n        });\n    }\n\n    private boolean applyHdr(@NonNull Camera.Parameters params, @NonNull Hdr oldHdr) {\n        if (mCameraOptions.supports(mHdr)) {\n            params.setSceneMode(mMapper.mapHdr(mHdr));\n            return true;\n        }\n        mHdr = oldHdr;\n        return false;\n    }\n\n    @Override\n    public void setZoom(final float zoom, @Nullable final PointF[] points, final boolean notify) {\n        final float old = mZoomValue;\n        mZoomValue = zoom;\n        // Zoom requests can be high frequency (e.g. linked to touch events), let's trim the oldest.\n        getOrchestrator().trim(\"zoom\", ALLOWED_ZOOM_OPS);\n        mZoomTask = getOrchestrator().scheduleStateful(\"zoom\",\n                CameraState.ENGINE,\n                new Runnable() {\n            @Override\n            public void run() {\n                Camera.Parameters params = mCamera.getParameters();\n                if (applyZoom(params, old)) {\n                    mCamera.setParameters(params);\n                    if (notify) {\n                        getCallback().dispatchOnZoomChanged(mZoomValue, points);\n                    }\n                }\n            }\n        });\n    }\n\n    private boolean applyZoom(@NonNull Camera.Parameters params, float oldZoom) {\n        if (mCameraOptions.isZoomSupported()) {\n            float max = params.getMaxZoom();\n            params.setZoom((int) (mZoomValue * max));\n            mCamera.setParameters(params);\n            return true;\n        }\n        mZoomValue = oldZoom;\n        return false;\n    }\n\n    @Override\n    public void setExposureCorrection(final float EVvalue, @NonNull final float[] bounds,\n                                      @Nullable final PointF[] points, final boolean notify) {\n        final float old = mExposureCorrectionValue;\n        mExposureCorrectionValue = EVvalue;\n        // EV requests can be high frequency (e.g. linked to touch events), let's trim the oldest.\n        getOrchestrator().trim(\"exposure correction\", ALLOWED_EV_OPS);\n        mExposureCorrectionTask = getOrchestrator().scheduleStateful(\n                \"exposure correction\",\n                CameraState.ENGINE,\n                new Runnable() {\n            @Override\n            public void run() {\n                Camera.Parameters params = mCamera.getParameters();\n                if (applyExposureCorrection(params, old)) {\n                    mCamera.setParameters(params);\n                    if (notify) {\n                        getCallback().dispatchOnExposureCorrectionChanged(mExposureCorrectionValue,\n                                bounds, points);\n                    }\n                }\n            }\n        });\n    }\n\n    private boolean applyExposureCorrection(@NonNull Camera.Parameters params,\n                                            float oldExposureCorrection) {\n        if (mCameraOptions.isExposureCorrectionSupported()) {\n            // Just make sure we're inside boundaries.\n            float max = mCameraOptions.getExposureCorrectionMaxValue();\n            float min = mCameraOptions.getExposureCorrectionMinValue();\n            float val = mExposureCorrectionValue;\n            val = val < min ? min : val > max ? max : val; // cap\n            mExposureCorrectionValue = val;\n            // Apply.\n            int indexValue = (int) (mExposureCorrectionValue\n                    / params.getExposureCompensationStep());\n            params.setExposureCompensation(indexValue);\n            return true;\n        }\n        mExposureCorrectionValue = oldExposureCorrection;\n        return false;\n    }\n\n    @Override\n    public void setPlaySounds(boolean playSounds) {\n        final boolean old = mPlaySounds;\n        mPlaySounds = playSounds;\n        mPlaySoundsTask = getOrchestrator().scheduleStateful(\n                \"play sounds (\" + playSounds + \")\",\n                CameraState.ENGINE,\n                new Runnable() {\n            @Override\n            public void run() {\n                applyPlaySounds(old);\n            }\n        });\n    }\n\n    @SuppressWarnings(\"UnusedReturnValue\")\n    @TargetApi(17)\n    private boolean applyPlaySounds(boolean oldPlaySound) {\n        if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.JELLY_BEAN_MR1) {\n            Camera.CameraInfo info = new Camera.CameraInfo();\n            Camera.getCameraInfo(mCameraId, info);\n            if (info.canDisableShutterSound) {\n                try {\n                    // this method is documented to throw on some occasions. #377\n                    return mCamera.enableShutterSound(mPlaySounds);\n                } catch (RuntimeException exception) {\n                    return false;\n                }\n            }\n        }\n        if (mPlaySounds) {\n            return true;\n        }\n        mPlaySounds = oldPlaySound;\n        return false;\n    }\n\n    @Override\n    public void setPreviewFrameRate(float previewFrameRate) {\n        final float old = previewFrameRate;\n        mPreviewFrameRate = previewFrameRate;\n        mPreviewFrameRateTask = getOrchestrator().scheduleStateful(\n                \"preview fps (\" + previewFrameRate + \")\",\n                CameraState.ENGINE,\n                new Runnable() {\n            @Override\n            public void run() {\n                Camera.Parameters params = mCamera.getParameters();\n                if (applyPreviewFrameRate(params, old)) mCamera.setParameters(params);\n            }\n        });\n    }\n\n    private boolean applyPreviewFrameRate(@NonNull Camera.Parameters params,\n                                          float oldPreviewFrameRate) {\n        List<int[]> fpsRanges = params.getSupportedPreviewFpsRange();\n        sortRanges(fpsRanges);\n        if (mPreviewFrameRate == 0F) {\n            // 0F is a special value. Fallback to a reasonable default.\n            for (int[] fpsRange : fpsRanges) {\n                float lower = (float) fpsRange[0] / 1000F;\n                float upper = (float) fpsRange[1] / 1000F;\n                if ((lower <= 30F && 30F <= upper) || (lower <= 24F && 24F <= upper)) {\n                    params.setPreviewFpsRange(fpsRange[0], fpsRange[1]);\n                    return true;\n                }\n            }\n        } else {\n            // If out of boundaries, adjust it.\n            mPreviewFrameRate = Math.min(mPreviewFrameRate,\n                    mCameraOptions.getPreviewFrameRateMaxValue());\n            mPreviewFrameRate = Math.max(mPreviewFrameRate,\n                    mCameraOptions.getPreviewFrameRateMinValue());\n            for (int[] fpsRange : fpsRanges) {\n                float lower = (float) fpsRange[0] / 1000F;\n                float upper = (float) fpsRange[1] / 1000F;\n                float rate = Math.round(mPreviewFrameRate);\n                if (lower <= rate && rate <= upper) {\n                    params.setPreviewFpsRange(fpsRange[0], fpsRange[1]);\n                    return true;\n                }\n            }\n        }\n        mPreviewFrameRate = oldPreviewFrameRate;\n        return false;\n    }\n\n    private void sortRanges(List<int[]> fpsRanges) {\n        if (getPreviewFrameRateExact() && mPreviewFrameRate != 0F) { // sort by range width in ascending order\n            Collections.sort(fpsRanges, new Comparator<int[]>() {\n                @Override\n                public int compare(int[] range1, int[] range2) {\n                    return (range1[1] - range1[0]) - (range2[1] - range2[0]);\n                }\n            });\n        } else { // sort by range width in descending order\n            Collections.sort(fpsRanges, new Comparator<int[]>() {\n                @Override\n                public int compare(int[] range1, int[] range2) {\n                    return (range2[1] - range2[0]) - (range1[1] - range1[0]);\n                }\n            });\n        }\n    }\n\n    @Override\n    public void setPictureFormat(@NonNull PictureFormat pictureFormat) {\n        if (pictureFormat != PictureFormat.JPEG) {\n            throw new UnsupportedOperationException(\"Unsupported picture format: \" + pictureFormat);\n        }\n        mPictureFormat = pictureFormat;\n    }\n\n    //endregion\n\n    //region Frame Processing\n\n    @NonNull\n    @Override\n    protected FrameManager instantiateFrameManager(int poolSize) {\n        return new ByteBufferFrameManager(poolSize, this);\n    }\n\n    @NonNull\n    @Override\n    public ByteBufferFrameManager getFrameManager() {\n        return (ByteBufferFrameManager) super.getFrameManager();\n    }\n\n    @Override\n    public void setHasFrameProcessors(boolean hasFrameProcessors) {\n        // we don't care, FP is always on\n        mHasFrameProcessors = hasFrameProcessors;\n    }\n\n    @Override\n    public void setFrameProcessingFormat(int format) {\n        // Ignore input: we only support NV21.\n        mFrameProcessingFormat = ImageFormat.NV21;\n    }\n\n    @Override\n    public void onBufferAvailable(@NonNull byte[] buffer) {\n        if (getState().isAtLeast(CameraState.ENGINE)\n                && getTargetState().isAtLeast(CameraState.ENGINE)) {\n            mCamera.addCallbackBuffer(buffer);\n        }\n    }\n\n    @Override\n    public void onPreviewFrame(byte[] data, Camera camera) {\n        if (data == null) {\n            // Seen this happen in logs.\n            return;\n        }\n        Frame frame = getFrameManager().getFrame(data, System.currentTimeMillis());\n        if (frame != null) {\n            getCallback().dispatchFrame(frame);\n        }\n    }\n\n    //endregion\n\n    //region Auto Focus\n\n    @Override\n    public void startAutoFocus(@Nullable final Gesture gesture,\n                               @NonNull final MeteringRegions regions,\n                               @NonNull final PointF legacyPoint) {\n        getOrchestrator().scheduleStateful(\"auto focus\", CameraState.BIND, new Runnable() {\n            @Override\n            public void run() {\n                if (!mCameraOptions.isAutoFocusSupported()) return;\n                MeteringTransform<Camera.Area> transform = new Camera1MeteringTransform(\n                        getAngles(),\n                        getPreview().getSurfaceSize());\n                MeteringRegions transformed = regions.transform(transform);\n\n                Camera.Parameters params = mCamera.getParameters();\n                int maxAF = params.getMaxNumFocusAreas();\n                int maxAE = params.getMaxNumMeteringAreas();\n                if (maxAF > 0) params.setFocusAreas(transformed.get(maxAF, transform));\n                if (maxAE > 0) params.setMeteringAreas(transformed.get(maxAE, transform));\n                params.setFocusMode(Camera.Parameters.FOCUS_MODE_AUTO);\n                try {\n                    mCamera.setParameters(params);\n                } catch (RuntimeException re) {\n                    LOG.e(\"startAutoFocus:\", \"Failed to set camera parameters\");\n                    throw new CameraException(re, CameraException.REASON_UNKNOWN);\n                }\n                getCallback().dispatchOnFocusStart(gesture, legacyPoint);\n\n                // The auto focus callback is not guaranteed to be called, but we really want it\n                // to be. So we remove the old runnable if still present and post a new one.\n                getOrchestrator().remove(JOB_FOCUS_END);\n                getOrchestrator().scheduleDelayed(JOB_FOCUS_END, true, AUTOFOCUS_END_DELAY_MILLIS,\n                        new Runnable() {\n                    @Override\n                    public void run() {\n                        getCallback().dispatchOnFocusEnd(gesture, false, legacyPoint);\n                    }\n                });\n\n                // Wrapping autoFocus in a try catch to handle some device specific exceptions,\n                // see See https://github.com/natario1/CameraView/issues/181.\n                try {\n                    mCamera.autoFocus(new Camera.AutoFocusCallback() {\n                        @Override\n                        public void onAutoFocus(boolean success, Camera camera) {\n                            getOrchestrator().remove(JOB_FOCUS_END);\n                            getOrchestrator().remove(JOB_FOCUS_RESET);\n                            getCallback().dispatchOnFocusEnd(gesture, success, legacyPoint);\n                            if (shouldResetAutoFocus()) {\n                                getOrchestrator().scheduleStatefulDelayed(\n                                        JOB_FOCUS_RESET,\n                                        CameraState.ENGINE,\n                                        getAutoFocusResetDelay(),\n                                        new Runnable() {\n                                    @Override\n                                    public void run() {\n                                        mCamera.cancelAutoFocus();\n                                        Camera.Parameters params = mCamera.getParameters();\n                                        int maxAF = params.getMaxNumFocusAreas();\n                                        int maxAE = params.getMaxNumMeteringAreas();\n                                        if (maxAF > 0) params.setFocusAreas(null);\n                                        if (maxAE > 0) params.setMeteringAreas(null);\n                                        applyDefaultFocus(params); // Revert to internal focus.\n                                        mCamera.setParameters(params);\n                                    }\n                                });\n                            }\n                        }\n                    });\n                } catch (RuntimeException e) {\n                    LOG.e(\"startAutoFocus:\", \"Error calling autoFocus\", e);\n                    // Let the mFocusEndRunnable do its job. (could remove it and quickly dispatch\n                    // onFocusEnd here, but let's make it simpler).\n                }\n            }\n        });\n    }\n\n    //endregion\n", "target": "camera 1 engine"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filters/TemperatureFilter.java:TemperatureFilter:0", "source": "\n\n    private final static String FRAGMENT_SHADER = \"#extension GL_OES_EGL_image_external : require\\n\"\n            + \"precision mediump float;\\n\"\n            + \"uniform samplerExternalOES sTexture;\\n\"\n            + \"uniform float scale;\\n\"\n            + \"varying vec2 \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\";\\n\"\n            + \"void main() {\\n\"\n            + \"  vec4 color = texture2D(sTexture, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\");\\n\"\n            + \"  vec3 new_color = color.rgb;\\n\"\n            + \"  new_color.r = color.r + color.r * ( 1.0 - color.r) * scale;\\n\"\n            + \"  new_color.b = color.b - color.b * ( 1.0 - color.b) * scale;\\n\"\n            + \"  if (scale > 0.0) { \\n\"\n            + \"    new_color.g = color.g + color.g * ( 1.0 - color.g) * scale * 0.25;\\n\"\n            + \"  }\\n\"\n            + \"  float max_value = max(new_color.r, max(new_color.g, new_color.b));\\n\"\n            + \"  if (max_value > 1.0) { \\n\"\n            + \"     new_color /= max_value;\\n\"\n            + \"  } \\n\"\n            + \"  gl_FragColor = vec4(new_color, color.a);\\n\"\n            + \"}\\n\";\n\n    private float scale = 1F; // -1...1\n    private int scaleLocation = -1;\n\n    public CLASSTOKEN() { }\n\n    /**\n     * Sets the new temperature value:\n     * -1.0: cool colors\n     * 0.0: no change\n     * 1.0: warm colors\n     *\n     * @param value new value\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public void setTemperature(float value) {\n        if (value < -1F) value = -1F;\n        if (value > 1F) value = 1F;\n        this.scale = value;\n    }\n\n    /**\n     * Returns the current temperature.\n     *\n     * @see #setTemperature(float)\n     * @return temperature\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public float getTemperature() {\n        return scale;\n    }\n\n    @Override\n    public void setParameter1(float value) {\n        setTemperature((2F * value - 1F));\n    }\n\n    @Override\n    public float getParameter1() {\n        return (getTemperature() + 1F) / 2F;\n    }\n\n    @NonNull\n    @Override\n    public String getFragmentShader() {\n        return FRAGMENT_SHADER;\n    }\n\n    @Override\n    public void onCreate(int programHandle) {\n        super.onCreate(programHandle);\n        scaleLocation = GLES20.glGetUniformLocation(programHandle, \"scale\");\n        Egloo.checkGlProgramLocation(scaleLocation, \"scale\");\n    }\n\n    @Override\n    public void onDestroy() {\n        super.onDestroy();\n        scaleLocation = -1;\n    }\n\n    @Override\n    protected void onPreDraw(long timestampUs, @NonNull float[] transformMatrix) {\n        super.onPreDraw(timestampUs, transformMatrix);\n        GLES20.glUniform1f(scaleLocation, scale);\n        Egloo.checkGlError(\"glUniform1f\");\n    }\n", "target": "temperature filter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/frame/FrameManager.java:FrameManager:0", "source": "\n\n    private static final String TAG = CLASSTOKEN.class.getSimpleName();\n    protected static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    private final int mPoolSize;\n    private int mFrameBytes = -1;\n    private Size mFrameSize = null;\n    private int mFrameFormat = -1;\n    private final Class<T> mFrameDataClass;\n    private LinkedBlockingQueue<Frame> mFrameQueue;\n    private Angles mAngles;\n\n\n    /**\n     * Construct a new frame manager.\n     * The construction must be followed by an {@link #setUp(int, Size, Angles)} call\n     * as soon as the parameters are known.\n     *\n     * @param poolSize the size of the backing pool.\n     */\n    protected CLASSTOKEN(int poolSize, @NonNull Class<T> dataClass) {\n        mPoolSize = poolSize;\n        mFrameDataClass = dataClass;\n        mFrameQueue = new LinkedBlockingQueue<>(mPoolSize);\n    }\n\n    /**\n     * Returns the pool size.\n     * @return pool size\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public final int getPoolSize() {\n        return mPoolSize;\n    }\n\n    /**\n     * Returns the frame size in bytes.\n     * @return frame size in bytes\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public final int getFrameBytes() {\n        return mFrameBytes;\n    }\n\n    /**\n     * Returns the frame data class.\n     * @return frame data class\n     */\n    public final Class<T> getFrameDataClass() {\n        return mFrameDataClass;\n    }\n\n    /**\n     * Allocates a {@link #mPoolSize} number of buffers. Should be called once\n     * the preview size and the image format value are known.\n     *\n     * This method can be called again after {@link #release()} has been called.\n     *  @param format the image format\n     * @param size the frame size\n     * @param angles angle object\n     */\n    public void setUp(int format, @NonNull Size size, @NonNull Angles angles) {\n        if (isSetUp()) {\n            // TODO throw or just reconfigure?\n        }\n        mFrameSize = size;\n        mFrameFormat = format;\n        int bitsPerPixel = ImageFormat.getBitsPerPixel(format);\n        long sizeInBits = size.getHeight() * size.getWidth() * bitsPerPixel;\n        mFrameBytes = (int) Math.ceil(sizeInBits / 8.0d);\n        for (int i = 0; i < getPoolSize(); i++) {\n            mFrameQueue.offer(new Frame(this));\n        }\n        mAngles = angles;\n    }\n\n    /**\n     * Returns true after {@link #setUp(int, Size, Angles)}\n     * but before {@link #release()}.\n     * Returns false otherwise.\n     *\n     * @return true if set up\n     */\n    protected boolean isSetUp() {\n        return mFrameSize != null;\n    }\n\n    /**\n     * Returns a new Frame for the given data. This must be called\n     * - after {@link #setUp(int, Size, Angles)}, which sets the buffer size\n     * - after the T data has been filled\n     *\n     * @param data data\n     * @param time timestamp\n     * @return a new frame\n     */\n    @Nullable\n    public Frame getFrame(@NonNull T data, long time) {\n        if (!isSetUp()) {\n            throw new IllegalStateException(\"Can't call getFrame() after releasing \" +\n                    \"or before setUp.\");\n        }\n\n        Frame frame = mFrameQueue.poll();\n        if (frame != null) {\n            LOG.v(\"getFrame for time:\", time, \"RECYCLING.\");\n            int userRotation = mAngles.offset(Reference.SENSOR, Reference.OUTPUT,\n                    Axis.RELATIVE_TO_SENSOR);\n            int viewRotation = mAngles.offset(Reference.SENSOR, Reference.VIEW,\n                    Axis.RELATIVE_TO_SENSOR);\n            frame.setContent(data, time, userRotation, viewRotation, mFrameSize, mFrameFormat);\n            return frame;\n        } else {\n            LOG.i(\"getFrame for time:\", time, \"NOT AVAILABLE.\");\n            onFrameDataReleased(data, false);\n            return null;\n        }\n    }\n\n    /**\n     * Called by child frames when they are released.\n     * @param frame the released frame\n     */\n    void onFrameReleased(@NonNull Frame frame, @NonNull T data) {\n        if (!isSetUp()) return;\n        // If frame queue is full, let's drop everything.\n        // If frame queue accepts this frame, let's recycle the buffer as well.\n        boolean recycled = mFrameQueue.offer(frame);\n        onFrameDataReleased(data, recycled);\n    }\n\n    /**\n     * Called when a Frame was released and its data is now available.\n     * This might be called from old Frames that belong to an old 'setUp'\n     * of this CLASSTOKEN instance. So the buffer size might be different,\n     * for instance.\n     * @param data data\n     * @param recycled recycled\n     */\n    protected abstract void onFrameDataReleased(@NonNull T data, boolean recycled);\n\n    @NonNull\n    final T cloneFrameData(@NonNull T data) {\n        return onCloneFrameData(data);\n    }\n\n    @NonNull\n    protected abstract T onCloneFrameData(@NonNull T data);\n\n    /**\n     * Releases all frames controlled by this manager and\n     * clears the pool.\n     */\n    public void release() {\n        if (!isSetUp()) {\n            LOG.w(\"release called twice. Ignoring.\");\n            return;\n        }\n\n        LOG.i(\"release: Clearing the frame and buffer queue.\");\n        mFrameQueue.clear();\n        mFrameBytes = -1;\n        mFrameSize = null;\n        mFrameFormat = -1;\n        mAngles = null;\n    }\n", "target": "frame manager"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filters/BrightnessFilter.java:BrightnessFilter:0", "source": "\n\n    private final static String FRAGMENT_SHADER = \"#extension GL_OES_EGL_image_external : require\\n\"\n            + \"precision mediump float;\\n\"\n            + \"uniform samplerExternalOES sTexture;\\n\"\n            + \"uniform float brightness;\\n\"\n            + \"varying vec2 \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\";\\n\"\n            + \"void main() {\\n\"\n            + \"  vec4 color = texture2D(sTexture, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\");\\n\"\n            + \"  gl_FragColor = brightness * color;\\n\"\n            + \"}\\n\";\n\n    private float brightness = 2.0f; // 1.0F...2.0F\n    private int brightnessLocation = -1;\n\n\n    public CLASSTOKEN() { }\n\n    /**\n     * Sets the brightness adjustment.\n     * 1.0: normal brightness.\n     * 2.0: high brightness.\n     *\n     * @param brightness brightness.\n     */\n    @SuppressWarnings({\"WeakerAccess\", \"unused\"})\n    public void setBrightness(float brightness) {\n        if (brightness < 1.0f) brightness = 1.0f;\n        if (brightness > 2.0f) brightness = 2.0f;\n        this.brightness = brightness;\n    }\n\n    /**\n     * Returns the current brightness.\n     *\n     * @see #setBrightness(float)\n     * @return brightness\n     */\n    @SuppressWarnings({\"unused\", \"WeakerAccess\"})\n    public float getBrightness() {\n        return brightness;\n    }\n\n    @Override\n    public void setParameter1(float value) {\n        // parameter is 0...1, brightness is 1...2.\n        setBrightness(value + 1);\n    }\n\n    @Override\n    public float getParameter1() {\n        // parameter is 0...1, brightness is 1...2.\n        return getBrightness() - 1F;\n    }\n\n    @NonNull\n    @Override\n    public String getFragmentShader() {\n        return FRAGMENT_SHADER;\n    }\n\n    @Override\n    public void onCreate(int programHandle) {\n        super.onCreate(programHandle);\n        brightnessLocation = GLES20.glGetUniformLocation(programHandle, \"brightness\");\n        Egloo.checkGlProgramLocation(brightnessLocation, \"brightness\");\n    }\n\n    @Override\n    public void onDestroy() {\n        super.onDestroy();\n        brightnessLocation = -1;\n    }\n\n    @Override\n    protected void onPreDraw(long timestampUs, @NonNull float[] transformMatrix) {\n        super.onPreDraw(timestampUs, transformMatrix);\n        GLES20.glUniform1f(brightnessLocation, brightness);\n        Egloo.checkGlError(\"glUniform1f\");\n    }\n", "target": "brightness filter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/preview/TextureCameraPreview.java:TextureCameraPreview:0", "source": "\n\n    private View mRootView;\n\n    public CLASSTOKEN(@NonNull Context context, @NonNull ViewGroup parent) {\n        super(context, parent);\n    }\n\n    @NonNull\n    @Override\n    protected TextureView onCreateView(@NonNull Context context, @NonNull ViewGroup parent) {\n        View root = LayoutInflater.from(context).inflate(R.layout.cameraview_texture_view, parent,\n                false);\n        parent.addView(root, 0);\n        TextureView texture = root.findViewById(R.id.texture_view);\n        texture.setSurfaceTextureListener(new TextureView.SurfaceTextureListener() {\n\n            @Override\n            public void onSurfaceTextureAvailable(SurfaceTexture surface, int width, int height) {\n                dispatchOnSurfaceAvailable(width, height);\n            }\n\n            @Override\n            public void onSurfaceTextureSizeChanged(SurfaceTexture surface, int width, int height) {\n                dispatchOnSurfaceSizeChanged(width, height);\n            }\n\n            @Override\n            public boolean onSurfaceTextureDestroyed(SurfaceTexture surface) {\n                dispatchOnSurfaceDestroyed();\n                return true;\n            }\n\n            @Override\n            public void onSurfaceTextureUpdated(SurfaceTexture surface) {\n            }\n        });\n        mRootView = root;\n        return texture;\n    }\n\n    @NonNull\n    @Override\n    public View getRootView() {\n        return mRootView;\n    }\n\n    @NonNull\n    @Override\n    public Class<SurfaceTexture> getOutputClass() {\n        return SurfaceTexture.class;\n    }\n\n    @NonNull\n    @Override\n    public SurfaceTexture getOutput() {\n        return getView().getSurfaceTexture();\n    }\n\n    @Override\n    public boolean supportsCropping() {\n        return true;\n    }\n\n    @Override\n    protected void crop(@Nullable final CropCallback callback) {\n        getView().post(new Runnable() {\n            @Override\n            public void run() {\n                if (mInputStreamHeight == 0 || mInputStreamWidth == 0 ||\n                        mOutputSurfaceHeight == 0 || mOutputSurfaceWidth == 0) {\n                    if (callback != null) callback.onCrop();\n                    return;\n                }\n                float scaleX = 1f, scaleY = 1f;\n                AspectRatio current = AspectRatio.of(mOutputSurfaceWidth, mOutputSurfaceHeight);\n                AspectRatio target = AspectRatio.of(mInputStreamWidth, mInputStreamHeight);\n                if (current.toFloat() >= target.toFloat()) {\n                    // We are too short. Must increase height.\n                    scaleY = current.toFloat() / target.toFloat();\n                } else {\n                    // We must increase width.\n                    scaleX = target.toFloat() / current.toFloat();\n                }\n\n                getView().setScaleX(scaleX);\n                getView().setScaleY(scaleY);\n\n                mCropping = scaleX > 1.02f || scaleY > 1.02f;\n                LOG.i(\"crop:\", \"applied scaleX=\", scaleX);\n                LOG.i(\"crop:\", \"applied scaleY=\", scaleY);\n                if (callback != null) callback.onCrop();\n            }\n        });\n    }\n\n    @Override\n    public void setDrawRotation(final int drawRotation) {\n        super.setDrawRotation(drawRotation);\n        final TaskCompletionSource<Void> task = new TaskCompletionSource<>();\n        getView().post(new Runnable() {\n            @Override\n            public void run() {\n                Matrix matrix = new Matrix();\n                // Output surface coordinates\n                float outputCenterX = mOutputSurfaceWidth / 2F;\n                float outputCenterY = mOutputSurfaceHeight / 2F;\n                boolean flip = drawRotation % 180 != 0;\n                // If dimensions are swapped, we must also do extra work to flip\n                // the two dimensions, using the view width and height (to support cropping).\n                if (flip) {\n                    float scaleX = (float) mOutputSurfaceHeight / mOutputSurfaceWidth;\n                    matrix.postScale(scaleX, 1F / scaleX, outputCenterX, outputCenterY);\n                }\n                matrix.postRotate((float) drawRotation, outputCenterX, outputCenterY);\n                getView().setTransform(matrix);\n                task.setResult(null);\n            }\n        });\n        try {\n            Tasks.await(task.getTask());\n        } catch (InterruptedException | ExecutionException ignore) { }\n    }\n", "target": "texture camera preview"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/picture/FullPictureRecorder.java:FullPictureRecorder:0", "source": "\n    private static final String TAG = CLASSTOKEN.class.getSimpleName();\n    protected static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    public CLASSTOKEN(@NonNull PictureResult.Stub stub,\n                               @Nullable PictureResultListener listener) {\n        super(stub, listener);\n    }\n", "target": "full picture recorder"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/CameraEngine.java:CrashExceptionHandler:1", "source": "\n        @Override\n        public void uncaughtException(@NonNull Thread thread, @NonNull Throwable throwable) {\n            handleException(throwable, true);\n        }\n    ", "target": "crash exception handler"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filters/SharpnessFilter.java:SharpnessFilter:0", "source": "\n\n    private final static String FRAGMENT_SHADER = \"#extension GL_OES_EGL_image_external : require\\n\"\n            + \"precision mediump float;\\n\"\n            + \"uniform samplerExternalOES sTexture;\\n\"\n            + \"uniform float scale;\\n\"\n            + \"uniform float stepsizeX;\\n\"\n            + \"uniform float stepsizeY;\\n\"\n            + \"varying vec2 \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\";\\n\"\n            + \"void main() {\\n\"\n            + \"  vec3 nbr_color = vec3(0.0, 0.0, 0.0);\\n\"\n            + \"  vec2 coord;\\n\"\n            + \"  vec4 color = texture2D(sTexture, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\");\\n\"\n            + \"  coord.x = \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\".x - 0.5 * stepsizeX;\\n\"\n            + \"  coord.y = \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\".y - stepsizeY;\\n\"\n            + \"  nbr_color += texture2D(sTexture, coord).rgb - color.rgb;\\n\"\n            + \"  coord.x = \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\".x - stepsizeX;\\n\"\n            + \"  coord.y = \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\".y + 0.5 * stepsizeY;\\n\"\n            + \"  nbr_color += texture2D(sTexture, coord).rgb - color.rgb;\\n\"\n            + \"  coord.x = \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\".x + stepsizeX;\\n\"\n            + \"  coord.y = \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\".y - 0.5 * stepsizeY;\\n\"\n            + \"  nbr_color += texture2D(sTexture, coord).rgb - color.rgb;\\n\"\n            + \"  coord.x = \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\".x + stepsizeX;\\n\"\n            + \"  coord.y = \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\".y + 0.5 * stepsizeY;\\n\"\n            + \"  nbr_color += texture2D(sTexture, coord).rgb - color.rgb;\\n\"\n            + \"  gl_FragColor = vec4(color.rgb - 2.0 * scale * nbr_color, color.a);\\n\"\n            + \"}\\n\";\n\n    private float scale = 0.5f;\n    private int width = 1;\n    private int height = 1;\n    private int scaleLocation = -1;\n    private int stepSizeXLocation = -1;\n    private int stepSizeYLocation = -1;\n\n    public CLASSTOKEN() { }\n\n    @Override\n    public void setSize(int width, int height) {\n        super.setSize(width, height);\n        this.width = width;\n        this.height = height;\n    }\n\n    /**\n     * Sets the current sharpness value:\n     * 0.0: no change.\n     * 1.0: maximum sharpness.\n     *\n     * @param value new sharpness\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public void setSharpness(float value) {\n        if (value < 0.0f) value = 0.0f;\n        if (value > 1.0f) value = 1.0f;\n        this.scale = value;\n    }\n\n    /**\n     * Returns the current sharpness.\n     *\n     * @see #setSharpness(float)\n     * @return sharpness\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public float getSharpness() {\n        return scale;\n    }\n\n    @Override\n    public void setParameter1(float value) {\n        setSharpness(value);\n    }\n\n    @Override\n    public float getParameter1() {\n        return getSharpness();\n    }\n\n    @NonNull\n    @Override\n    public String getFragmentShader() {\n        return FRAGMENT_SHADER;\n    }\n\n    @Override\n    public void onCreate(int programHandle) {\n        super.onCreate(programHandle);\n        scaleLocation = GLES20.glGetUniformLocation(programHandle, \"scale\");\n        Egloo.checkGlProgramLocation(scaleLocation, \"scale\");\n        stepSizeXLocation = GLES20.glGetUniformLocation(programHandle, \"stepsizeX\");\n        Egloo.checkGlProgramLocation(stepSizeXLocation, \"stepsizeX\");\n        stepSizeYLocation = GLES20.glGetUniformLocation(programHandle, \"stepsizeY\");\n        Egloo.checkGlProgramLocation(stepSizeYLocation, \"stepsizeY\");\n    }\n\n    @Override\n    public void onDestroy() {\n        super.onDestroy();\n        scaleLocation = -1;\n        stepSizeXLocation = -1;\n        stepSizeYLocation = -1;\n    }\n\n    @Override\n    protected void onPreDraw(long timestampUs, @NonNull float[] transformMatrix) {\n        super.onPreDraw(timestampUs, transformMatrix);\n        GLES20.glUniform1f(scaleLocation, scale);\n        Egloo.checkGlError(\"glUniform1f\");\n        GLES20.glUniform1f(stepSizeXLocation, 1.0F / width);\n        Egloo.checkGlError(\"glUniform1f\");\n        GLES20.glUniform1f(stepSizeYLocation, 1.0F / height);\n        Egloo.checkGlError(\"glUniform1f\");\n    }\n", "target": "sharpness filter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/preview/SurfaceCameraPreview.java:SurfaceCameraPreview:0", "source": "\n\n    private final static CameraLogger LOG\n            = CameraLogger.create(CLASSTOKEN.class.getSimpleName());\n\n    private boolean mDispatched;\n    private View mRootView;\n\n    public CLASSTOKEN(@NonNull Context context, @NonNull ViewGroup parent) {\n        super(context, parent);\n    }\n\n    @NonNull\n    @Override\n    protected SurfaceView onCreateView(@NonNull Context context, @NonNull ViewGroup parent) {\n        View root = LayoutInflater.from(context).inflate(R.layout.cameraview_surface_view, parent,\n                false);\n        parent.addView(root, 0);\n        SurfaceView surfaceView = root.findViewById(R.id.surface_view);\n        final SurfaceHolder holder = surfaceView.getHolder();\n        holder.setType(SurfaceHolder.SURFACE_TYPE_PUSH_BUFFERS);\n        holder.addCallback(new SurfaceHolder.Callback() {\n\n            @Override\n            public void surfaceCreated(SurfaceHolder holder) {\n                // This is too early to call anything.\n                // surfaceChanged is guaranteed to be called after, with exact dimensions.\n                LOG.i(\"callback: surfaceCreated.\");\n            }\n\n            @Override\n            public void surfaceChanged(SurfaceHolder holder, int format, int width, int height) {\n                LOG.i(\"callback:\", \"surfaceChanged\",\n                        \"w:\", width,\n                        \"h:\", height,\n                        \"dispatched:\", mDispatched);\n                if (!mDispatched) {\n                    dispatchOnSurfaceAvailable(width, height);\n                    mDispatched = true;\n                } else {\n                    dispatchOnSurfaceSizeChanged(width, height);\n                }\n            }\n\n            @Override\n            public void surfaceDestroyed(SurfaceHolder holder) {\n                LOG.i(\"callback: surfaceDestroyed\");\n                dispatchOnSurfaceDestroyed();\n                mDispatched = false;\n            }\n        });\n        mRootView = root;\n        return surfaceView;\n    }\n\n    @NonNull\n    @Override\n    public View getRootView() {\n        return mRootView;\n    }\n\n    @NonNull\n    @Override\n    public SurfaceHolder getOutput() {\n        return getView().getHolder();\n    }\n\n    @NonNull\n    @Override\n    public Class<SurfaceHolder> getOutputClass() {\n        return SurfaceHolder.class;\n    }\n\n\n", "target": "surface camera preview"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/size/AspectRatio.java:AspectRatio:0", "source": "\n\n    @VisibleForTesting final static HashMap<String, CLASSTOKEN> sCache\n            = new HashMap<>(16);\n\n    /**\n     * Creates an aspect ratio for the given size.\n     * @param size the size\n     * @return a (possibly cached) aspect ratio\n     */\n    @NonNull\n    public static CLASSTOKEN of(@NonNull Size size) {\n        return CLASSTOKEN.of(size.getWidth(), size.getHeight());\n    }\n\n    /**\n     * Creates an aspect ratio with the given values.\n     * @param x the width\n     * @param y the height\n     * @return a (possibly cached) aspect ratio\n     */\n    @NonNull\n    public static CLASSTOKEN of(int x, int y) {\n        int gcd = gcd(x, y);\n        if (gcd > 0) x /= gcd;\n        if (gcd > 0) y /= gcd;\n        String key = x + \":\" + y;\n        CLASSTOKEN cached = sCache.get(key);\n        if (cached == null) {\n            cached = new CLASSTOKEN(x, y);\n            sCache.put(key, cached);\n        }\n        return cached;\n    }\n\n    /**\n     * Parses an aspect ratio string, for example those previously obtained\n     * with {@link #toString()}.\n     *\n     * @param string a string of the format x:y where x and y are integers\n     * @return a (possibly cached) aspect ratio\n     */\n    @NonNull\n    @SuppressWarnings(\"WeakerAccess\")\n    public static CLASSTOKEN parse(@NonNull String string) {\n        String[] parts = string.split(\":\");\n        if (parts.length != 2) {\n            throw new NumberFormatException(\"Illegal CLASSTOKEN string. Must be x:y\");\n        }\n        int x = Integer.parseInt(parts[0]);\n        int y = Integer.parseInt(parts[1]);\n        return of(x, y);\n    }\n\n    private final int mX;\n    private final int mY;\n\n    private CLASSTOKEN(int x, int y) {\n        mX = x;\n        mY = y;\n    }\n\n    public int getX() {\n        return mX;\n    }\n\n    public int getY() {\n        return mY;\n    }\n\n    public boolean matches(@NonNull Size size) {\n        return equals(CLASSTOKEN.of(size));\n    }\n\n    public boolean matches(@NonNull Size size, float tolerance) {\n        return Math.abs(toFloat() - CLASSTOKEN.of(size).toFloat()) <= tolerance;\n    }\n\n    @Override\n    public boolean equals(Object o) {\n        if (o == null) {\n            return false;\n        }\n        if (this == o) {\n            return true;\n        }\n        if (o instanceof CLASSTOKEN) {\n            return toFloat() == ((CLASSTOKEN) o).toFloat();\n        }\n        return false;\n    }\n\n    @NonNull\n    @Override\n    public String toString() {\n        return mX + \":\" + mY;\n    }\n\n    public float toFloat() {\n        return (float) mX / mY;\n    }\n\n    @Override\n    public int hashCode() {\n        return Float.floatToIntBits(toFloat());\n    }\n\n    @Override\n    public int compareTo(@NonNull CLASSTOKEN another) {\n        return Float.compare(toFloat(), another.toFloat());\n    }\n\n    /**\n     * Returns a flipped aspect ratio, which means inverting its dimensions.\n     * @return a flipped aspect ratio\n     */\n    @SuppressWarnings(\"SuspiciousNameCombination\")\n    @NonNull\n    public CLASSTOKEN flip() {\n        return CLASSTOKEN.of(mY, mX);\n    }\n\n    // Note: gcd(0,X) = gcd(X,0) = X (even for X=0)\n    private static int gcd(int a, int b) {\n        while (b != 0) {\n            int c = b;\n            b = a % b;\n            a = c;\n        }\n        return a;\n    }\n", "target": "aspect ratio"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/meter/WhiteBalanceMeter.java:WhiteBalanceMeter:0", "source": "\n\n    private static final String TAG = CLASSTOKEN.class.getSimpleName();\n    private static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    public CLASSTOKEN(@NonNull List<MeteringRectangle> areas, boolean skipIfPossible) {\n        super(areas, skipIfPossible);\n    }\n\n    @Override\n    protected boolean checkIsSupported(@NonNull ActionHolder holder) {\n        boolean isNotLegacy = readCharacteristic(\n                CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL, -1)\n                != CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY;\n        Integer awbMode = holder.getBuilder(this).get(CaptureRequest.CONTROL_AWB_MODE);\n        boolean result = isNotLegacy\n                && awbMode != null\n                && awbMode == CaptureRequest.CONTROL_AWB_MODE_AUTO;\n        LOG.i(\"checkIsSupported:\", result);\n        return result;\n    }\n\n    @Override\n    protected boolean checkShouldSkip(@NonNull ActionHolder holder) {\n        CaptureResult lastResult = holder.getLastResult(this);\n        if (lastResult != null) {\n            Integer awbState = lastResult.get(CaptureResult.CONTROL_AWB_STATE);\n            boolean result = awbState != null\n                    && awbState == CaptureRequest.CONTROL_AWB_STATE_CONVERGED;\n            LOG.i(\"checkShouldSkip:\", result);\n            return result;\n        } else {\n            LOG.i(\"checkShouldSkip: false - lastResult is null.\");\n            return false;\n        }\n    }\n\n    @Override\n    protected void onStarted(@NonNull ActionHolder holder, @NonNull List<MeteringRectangle> areas) {\n        LOG.i(\"onStarted:\", \"with areas:\", areas);\n        int maxRegions = readCharacteristic(CameraCharacteristics.CONTROL_MAX_REGIONS_AWB,\n                0);\n        if (!areas.isEmpty() && maxRegions > 0) {\n            int max = Math.min(maxRegions, areas.size());\n            holder.getBuilder(this).set(CaptureRequest.CONTROL_AWB_REGIONS,\n                    areas.subList(0, max).toArray(new MeteringRectangle[]{}));\n            holder.applyBuilder(this);\n        }\n    }\n\n    @Override\n    public void onCaptureCompleted(@NonNull ActionHolder holder,\n                                   @NonNull CaptureRequest request,\n                                   @NonNull TotalCaptureResult result) {\n        super.onCaptureCompleted(holder, request, result);\n        Integer awbState = result.get(CaptureResult.CONTROL_AWB_STATE);\n        LOG.i(\"onCaptureCompleted:\", \"awbState:\", awbState);\n        if (awbState == null) return;\n\n        switch (awbState) {\n            case CaptureRequest.CONTROL_AWB_STATE_CONVERGED: {\n                setSuccessful(true);\n                setState(STATE_COMPLETED);\n                break;\n            }\n            case CaptureRequest.CONTROL_AWB_STATE_LOCKED: {\n                // Nothing we can do if AWB was locked.\n                setSuccessful(false);\n                setState(STATE_COMPLETED);\n                break;\n            }\n            case CaptureRequest.CONTROL_AWB_STATE_INACTIVE:\n            case CaptureRequest.CONTROL_AWB_STATE_SEARCHING: {\n                // Wait...\n                break;\n            }\n        }\n    }\n", "target": "white balance meter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/internal/DeviceEncoders.java:VideoException:1", "source": "\n        private CLASSTOKEN(@NonNull String message) {\n            super(message);\n        }\n    ", "target": "video exception"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/meter/BaseMeter.java:BaseMeter:0", "source": "\n\n    private final static String TAG = CLASSTOKEN.class.getSimpleName();\n    private final static CameraLogger LOG = CameraLogger.create(TAG);\n\n    private final List<MeteringRectangle> areas;\n    private boolean isSuccessful;\n    private boolean skipIfPossible;\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected CLASSTOKEN(@NonNull List<MeteringRectangle> areas, boolean skipIfPossible) {\n        this.areas = areas;\n        this.skipIfPossible = skipIfPossible;\n    }\n\n    @Override\n    protected final void onStart(@NonNull ActionHolder holder) {\n        super.onStart(holder);\n        boolean isSkipped = skipIfPossible && checkShouldSkip(holder);\n        boolean isSupported = checkIsSupported(holder);\n        if (isSupported && !isSkipped) {\n            LOG.i(\"onStart:\", \"supported and not skipped. Dispatching onStarted.\");\n            onStarted(holder, areas);\n        } else {\n            LOG.i(\"onStart:\", \"not supported or skipped. Dispatching COMPLETED state.\");\n            setSuccessful(true);\n            setState(STATE_COMPLETED);\n        }\n    }\n\n    protected abstract void onStarted(@NonNull ActionHolder holder,\n                                      @NonNull List<MeteringRectangle> areas);\n\n    protected abstract boolean checkShouldSkip(@NonNull ActionHolder holder);\n\n    protected abstract boolean checkIsSupported(@NonNull ActionHolder holder);\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected void setSuccessful(boolean successful) {\n        isSuccessful = successful;\n    }\n\n    @SuppressWarnings(\"WeakerAccess\")\n    public boolean isSuccessful() {\n        return isSuccessful;\n    }\n", "target": "base meter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/VideoResult.java:Stub:1", "source": "\n\n        CLASSTOKEN() {}\n\n        public boolean isSnapshot;\n        public Location location;\n        public int rotation;\n        public Size size;\n        public File file;\n        public FileDescriptor fileDescriptor;\n        public Facing facing;\n        public VideoCodec videoCodec;\n        public AudioCodec audioCodec;\n        public Audio audio;\n        public long maxSize;\n        public int maxDuration;\n        public int endReason;\n        public int videoBitRate;\n        public int videoFrameRate;\n        public int audioBitRate;\n    ", "target": "stub"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/picture/Snapshot1PictureRecorder.java:Snapshot1PictureRecorder:0", "source": "\n\n    private Camera1Engine mEngine1;\n    private Camera mCamera;\n    private AspectRatio mOutputRatio;\n    private int mFormat;\n\n    public CLASSTOKEN(\n            @NonNull PictureResult.Stub stub,\n            @NonNull Camera1Engine engine,\n            @NonNull Camera camera,\n            @NonNull AspectRatio outputRatio) {\n        super(stub, engine);\n        mEngine1 = engine;\n        mCamera = camera;\n        mOutputRatio = outputRatio;\n        mFormat = camera.getParameters().getPreviewFormat();\n    }\n\n    @Override\n    public void take() {\n        mCamera.setOneShotPreviewCallback(new Camera.PreviewCallback() {\n            @Override\n            public void onPreviewFrame(@NonNull final byte[] yuv, Camera camera) {\n                dispatchOnShutter(false);\n\n                // Got to rotate the preview frame, since byte[] data here does not include\n                // EXIF tags automatically set by camera. So either we add EXIF, or we rotate.\n                // Adding EXIF to a byte array, unfortunately, is hard.\n                final int sensorToOutput = mResult.rotation;\n                final Size outputSize = mResult.size;\n                final Size previewStreamSize = mEngine1.getPreviewStreamSize(Reference.SENSOR);\n                if (previewStreamSize == null) {\n                    throw new IllegalStateException(\"Preview stream size \" +\n                            \"should never be null here.\");\n                }\n                WorkerHandler.execute(new Runnable() {\n                    @Override\n                    public void run() {\n                        // Rotate the picture, because no one will write EXIF data,\n                        // then crop if needed. In both cases, transform yuv to jpeg.\n                        //noinspection deprecation\n                        byte[] data = RotationHelper.rotate(yuv, previewStreamSize, sensorToOutput);\n                        YuvImage yuv = new YuvImage(data, mFormat, outputSize.getWidth(),\n                                outputSize.getHeight(), null);\n\n                        ByteArrayOutputStream stream = new ByteArrayOutputStream();\n                        Rect outputRect = CropHelper.computeCrop(outputSize, mOutputRatio);\n                        yuv.compressToJpeg(outputRect, 90, stream);\n                        data = stream.toByteArray();\n\n                        mResult.data = data;\n                        mResult.size = new Size(outputRect.width(), outputRect.height());\n                        mResult.rotation = 0;\n                        dispatchResult();\n                    }\n                });\n\n                // It seems that the buffers are already cleared here, so we need to allocate again.\n                camera.setPreviewCallbackWithBuffer(null); // Release anything left\n                camera.setPreviewCallbackWithBuffer(mEngine1); // Add ourselves\n                mEngine1.getFrameManager().setUp(mFormat, previewStreamSize, mEngine1.getAngles());\n            }\n        });\n    }\n\n    @Override\n    protected void dispatchResult() {\n        mEngine1 = null;\n        mCamera = null;\n        mOutputRatio = null;\n        mFormat = 0;\n        super.dispatchResult();\n    }\n", "target": "snapshot 1 picture recorder"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/controls/ControlParser.java:ControlParser:0", "source": "\n\n    private int preview;\n    private int facing;\n    private int flash;\n    private int grid;\n    private int whiteBalance;\n    private int mode;\n    private int hdr;\n    private int audio;\n    private int videoCodec;\n    private int audioCodec;\n    private int engine;\n    private int pictureFormat;\n\n    public CLASSTOKEN(@NonNull Context context, @NonNull TypedArray array) {\n        preview = array.getInteger(R.styleable.CameraView_cameraPreview, Preview.DEFAULT.value());\n        facing = array.getInteger(R.styleable.CameraView_cameraFacing,\n                Facing.DEFAULT(context).value());\n        flash = array.getInteger(R.styleable.CameraView_cameraFlash, Flash.DEFAULT.value());\n        grid = array.getInteger(R.styleable.CameraView_cameraGrid, Grid.DEFAULT.value());\n        whiteBalance = array.getInteger(R.styleable.CameraView_cameraWhiteBalance,\n                WhiteBalance.DEFAULT.value());\n        mode = array.getInteger(R.styleable.CameraView_cameraMode, Mode.DEFAULT.value());\n        hdr = array.getInteger(R.styleable.CameraView_cameraHdr, Hdr.DEFAULT.value());\n        audio = array.getInteger(R.styleable.CameraView_cameraAudio, Audio.DEFAULT.value());\n        videoCodec = array.getInteger(R.styleable.CameraView_cameraVideoCodec,\n                VideoCodec.DEFAULT.value());\n        audioCodec = array.getInteger(R.styleable.CameraView_cameraAudioCodec,\n                AudioCodec.DEFAULT.value());\n        engine = array.getInteger(R.styleable.CameraView_cameraEngine, Engine.DEFAULT.value());\n        pictureFormat = array.getInteger(R.styleable.CameraView_cameraPictureFormat,\n                PictureFormat.DEFAULT.value());\n    }\n\n    @NonNull\n    public Preview getPreview() {\n        return Preview.fromValue(preview);\n    }\n\n    @NonNull\n    public Facing getFacing() {\n        //noinspection ConstantConditions\n        return Facing.fromValue(facing);\n    }\n\n    @NonNull\n    public Flash getFlash() {\n        return Flash.fromValue(flash);\n    }\n\n    @NonNull\n    public Grid getGrid() {\n        return Grid.fromValue(grid);\n    }\n\n    @NonNull\n    public Mode getMode() {\n        return Mode.fromValue(mode);\n    }\n\n    @NonNull\n    public WhiteBalance getWhiteBalance() {\n        return WhiteBalance.fromValue(whiteBalance);\n    }\n\n    @NonNull\n    public Hdr getHdr() {\n        return Hdr.fromValue(hdr);\n    }\n\n    @NonNull\n    public Audio getAudio() {\n        return Audio.fromValue(audio);\n    }\n\n    @NonNull\n    public AudioCodec getAudioCodec() {\n        return AudioCodec.fromValue(audioCodec);\n    }\n\n    @NonNull\n    public VideoCodec getVideoCodec() {\n        return VideoCodec.fromValue(videoCodec);\n    }\n\n    @NonNull\n    public Engine getEngine() {\n        return Engine.fromValue(engine);\n    }\n\n    @NonNull\n    public PictureFormat getPictureFormat() {\n        return PictureFormat.fromValue(pictureFormat);\n    }\n", "target": "control parser"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filters/DuotoneFilter.java:DuotoneFilter:0", "source": "\n\n    private final static String FRAGMENT_SHADER = \"#extension GL_OES_EGL_image_external : require\\n\"\n            + \"precision mediump float;\\n\"\n            + \"uniform samplerExternalOES sTexture;\\n\"\n            + \"uniform vec3 first;\\n\"\n            + \"uniform vec3 second;\\n\"\n            + \"varying vec2 \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\";\\n\"\n            + \"void main() {\\n\"\n            + \"  vec4 color = texture2D(sTexture, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\");\\n\"\n            + \"  float energy = (color.r + color.g + color.b) * 0.3333;\\n\"\n            + \"  vec3 new_color = (1.0 - energy) * first + energy * second;\\n\"\n            + \"  gl_FragColor = vec4(new_color.rgb, color.a);\\n\"\n            + \"}\\n\";\n\n    // Default values\n    private int mFirstColor = Color.MAGENTA;\n    private int mSecondColor = Color.YELLOW;\n    private int mFirstColorLocation = -1;\n    private int mSecondColorLocation = -1;\n\n    public CLASSTOKEN() { }\n\n    /**\n     * Sets the two duotone ARGB colors.\n     * @param firstColor first\n     * @param secondColor second\n     */\n    @SuppressWarnings({\"unused\"})\n    public void setColors(@ColorInt int firstColor, @ColorInt int secondColor) {\n        setFirstColor(firstColor);\n        setSecondColor(secondColor);\n    }\n\n    /**\n     * Sets the first of the duotone ARGB colors.\n     * Defaults to {@link Color#MAGENTA}.\n     *\n     * @param color first color\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public void setFirstColor(@ColorInt int color) {\n        // Remove any alpha.\n        mFirstColor = Color.rgb(Color.red(color), Color.green(color), Color.blue(color));\n    }\n\n    /**\n     * Sets the second of the duotone ARGB colors.\n     * Defaults to {@link Color#YELLOW}.\n     *\n     * @param color second color\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public void setSecondColor(@ColorInt int color) {\n        // Remove any alpha.\n        mSecondColor = Color.rgb(Color.red(color), Color.green(color), Color.blue(color));\n    }\n\n    /**\n     * Returns the first color.\n     *\n     * @see #setFirstColor(int)\n     * @return first\n     */\n    @SuppressWarnings({\"unused\", \"WeakerAccess\"})\n    @ColorInt\n    public int getFirstColor() {\n        return mFirstColor;\n    }\n\n    /**\n     * Returns the second color.\n     *\n     * @see #setSecondColor(int)\n     * @return second\n     */\n    @SuppressWarnings({\"unused\", \"WeakerAccess\"})\n    @ColorInt\n    public int getSecondColor() {\n        return mSecondColor;\n    }\n\n    @Override\n    public void setParameter1(float value) {\n        // no easy way to transform 0...1 into a color.\n        setFirstColor((int) (value * 0xFFFFFF));\n    }\n\n    @Override\n    public float getParameter1() {\n        int color = getFirstColor();\n        color = Color.argb(0, Color.red(color), Color.green(color), Color.blue(color));\n        return (float) color / 0xFFFFFF;\n    }\n\n    @Override\n    public void setParameter2(float value) {\n        // no easy way to transform 0...1 into a color.\n        setSecondColor((int) (value * 0xFFFFFF));\n    }\n\n    @Override\n    public float getParameter2() {\n        int color = getSecondColor();\n        color = Color.argb(0, Color.red(color), Color.green(color), Color.blue(color));\n        return (float) color / 0xFFFFFF;\n    }\n\n    @NonNull\n    @Override\n    public String getFragmentShader() {\n        return FRAGMENT_SHADER;\n    }\n\n    @Override\n    public void onCreate(int programHandle) {\n        super.onCreate(programHandle);\n        mFirstColorLocation = GLES20.glGetUniformLocation(programHandle, \"first\");\n        Egloo.checkGlProgramLocation(mFirstColorLocation, \"first\");\n        mSecondColorLocation = GLES20.glGetUniformLocation(programHandle, \"second\");\n        Egloo.checkGlProgramLocation(mSecondColorLocation, \"second\");\n    }\n\n    @Override\n    protected void onPreDraw(long timestampUs, @NonNull float[] transformMatrix) {\n        super.onPreDraw(timestampUs, transformMatrix);\n        float[] first = new float[]{\n                Color.red(mFirstColor) / 255f,\n                Color.green(mFirstColor) / 255f,\n                Color.blue(mFirstColor) / 255f\n        };\n        float[] second = new float[]{\n                Color.red(mSecondColor) / 255f,\n                Color.green(mSecondColor) / 255f,\n                Color.blue(mSecondColor) / 255f\n        };\n        GLES20.glUniform3fv(mFirstColorLocation, 1, first, 0);\n        Egloo.checkGlError(\"glUniform3fv\");\n        GLES20.glUniform3fv(mSecondColorLocation, 1, second, 0);\n        Egloo.checkGlError(\"glUniform3fv\");\n    }\n\n    @Override\n    public void onDestroy() {\n        super.onDestroy();\n        mFirstColorLocation = -1;\n        mSecondColorLocation = -1;\n    }\n", "target": "duotone filter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/meter/MeterAction.java:MeterAction:0", "source": "\n\n    private final static String TAG = CLASSTOKEN.class.getSimpleName();\n    private final static CameraLogger LOG = CameraLogger.create(TAG);\n\n    private List<BaseMeter> meters;\n    private BaseAction action;\n    private final MeteringRegions regions;\n    private final CameraEngine engine;\n    private final boolean skipIfPossible;\n\n    public CLASSTOKEN(@NonNull CameraEngine engine,\n                       @Nullable MeteringRegions regions,\n                       boolean skipIfPossible) {\n        this.regions = regions;\n        this.engine = engine;\n        this.skipIfPossible = skipIfPossible;\n    }\n\n    @NonNull\n    @Override\n    public BaseAction getAction() {\n        return action;\n    }\n\n    public boolean isSuccessful() {\n        for (BaseMeter meter : meters) {\n            if (!meter.isSuccessful()) {\n                LOG.i(\"isSuccessful:\", \"returning false.\");\n                return false;\n            }\n        }\n        LOG.i(\"isSuccessful:\", \"returning true.\");\n        return true;\n    }\n\n    @Override\n    protected void onStart(@NonNull ActionHolder holder) {\n        LOG.w(\"onStart:\", \"initializing.\");\n        initialize(holder);\n        LOG.w(\"onStart:\", \"initialized.\");\n        super.onStart(holder);\n    }\n\n    private void initialize(@NonNull ActionHolder holder) {\n        List<MeteringRectangle> areas = new ArrayList<>();\n        if (regions != null) {\n            MeteringTransform<MeteringRectangle> transform = new Camera2MeteringTransform(\n                    engine.getAngles(),\n                    engine.getPreview().getSurfaceSize(),\n                    engine.getPreviewStreamSize(Reference.VIEW),\n                    engine.getPreview().isCropping(),\n                    holder.getCharacteristics(this),\n                    holder.getBuilder(this)\n            );\n            MeteringRegions transformed = regions.transform(transform);\n            areas = transformed.get(Integer.MAX_VALUE, transform);\n        }\n\n        BaseMeter ae = new ExposureMeter(areas, skipIfPossible);\n        BaseMeter af = new FocusMeter(areas, skipIfPossible);\n        BaseMeter awb = new WhiteBalanceMeter(areas, skipIfPossible);\n        meters = Arrays.asList(ae, af, awb);\n        action = Actions.together(ae, af, awb);\n    }\n", "target": "meter action"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/Full1VideoRecorder.java:Full1VideoRecorder:0", "source": "\n\n    private final Camera1Engine mEngine;\n    private final Camera mCamera;\n    private final int mCameraId;\n\n    public CLASSTOKEN(@NonNull Camera1Engine engine,\n                              @NonNull Camera camera, int cameraId) {\n        super(engine);\n        mCamera = camera;\n        mEngine = engine;\n        mCameraId = cameraId;\n    }\n\n    @Override\n    protected void applyVideoSource(@NonNull VideoResult.Stub stub,\n                                    @NonNull MediaRecorder mediaRecorder) {\n        mediaRecorder.setCamera(mCamera);\n        mediaRecorder.setVideoSource(MediaRecorder.VideoSource.CAMERA);\n    }\n\n    @NonNull\n    @Override\n    protected CamcorderProfile getCamcorderProfile(@NonNull VideoResult.Stub stub) {\n        // Get a profile of quality compatible with the chosen size.\n        Size size = stub.rotation % 180 != 0 ? stub.size.flip() : stub.size;\n        return CamcorderProfiles.get(mCameraId, size);\n    }\n\n    @Override\n    protected void onDispatchResult() {\n        // Restore frame processing.\n        mCamera.setPreviewCallbackWithBuffer(mEngine);\n        super.onDispatchResult();\n    }\n", "target": "full 1 video recorder"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/internal/DeviceEncoders.java:DeviceEncoders:0", "source": "\n\n    private final static String TAG = CLASSTOKEN.class.getSimpleName();\n    private final static CameraLogger LOG = CameraLogger.create(TAG);\n\n    @VisibleForTesting static boolean ENABLED = Build.VERSION.SDK_INT >= 21;\n\n    public final static int MODE_RESPECT_ORDER = 0;\n    public final static int MODE_PREFER_HARDWARE = 1;\n\n    /**\n     * Exception thrown when trying to find appropriate values\n     * for a video encoder.\n     */\n    public class VideoException extends RuntimeException {\n        private VideoException(@NonNull String message) {\n            super(message);\n        }\n    }\n\n    /**\n     * Exception thrown when trying to find appropriate values\n     * for an audio encoder. Currently never thrown.\n     */\n    public class AudioException extends RuntimeException {\n        private AudioException(@NonNull String message) {\n            super(message);\n        }\n    }\n\n    @SuppressWarnings(\"FieldCanBeLocal\")\n    private final MediaCodecInfo mVideoEncoder;\n    @SuppressWarnings(\"FieldCanBeLocal\")\n    private final MediaCodecInfo mAudioEncoder;\n    private final MediaCodecInfo.VideoCapabilities mVideoCapabilities;\n    private final MediaCodecInfo.AudioCapabilities mAudioCapabilities;\n\n    @SuppressLint(\"NewApi\")\n    public CLASSTOKEN(int mode,\n                          @NonNull String videoType,\n                          @NonNull String audioType,\n                          int videoOffset,\n                          int audioOffset) {\n        // We could still get a list of MediaCodecInfo for API >= 16, but it seems that the APIs\n        // for querying the availability of a specified MediaFormat were only added in 21 anyway.\n        if (ENABLED) {\n            List<MediaCodecInfo> encoders = getDeviceEncoders();\n            mVideoEncoder = findDeviceEncoder(encoders, videoType, mode, videoOffset);\n            LOG.i(\"Enabled. Found video encoder:\", mVideoEncoder.getName());\n            mAudioEncoder = findDeviceEncoder(encoders, audioType, mode, audioOffset);\n            LOG.i(\"Enabled. Found audio encoder:\", mAudioEncoder.getName());\n            mVideoCapabilities = mVideoEncoder.getCapabilitiesForType(videoType)\n                    .getVideoCapabilities();\n            mAudioCapabilities = mAudioEncoder.getCapabilitiesForType(audioType)\n                    .getAudioCapabilities();\n        } else {\n            mVideoEncoder = null;\n            mAudioEncoder = null;\n            mVideoCapabilities = null;\n            mAudioCapabilities = null;\n            LOG.i(\"Disabled.\");\n        }\n    }\n\n    /**\n     * Collects all the device encoders, which means excluding decoders.\n     * @return encoders\n     */\n    @NonNull\n    @SuppressLint(\"NewApi\")\n    @VisibleForTesting\n    List<MediaCodecInfo> getDeviceEncoders() {\n        ArrayList<MediaCodecInfo> results = new ArrayList<>();\n        MediaCodecInfo[] array = new MediaCodecList(MediaCodecList.REGULAR_CODECS).getCodecInfos();\n        for (MediaCodecInfo info : array) {\n            if (info.isEncoder()) results.add(info);\n        }\n        return results;\n    }\n\n    /**\n     * Whether an encoder is a hardware encoder or not. We don't have an API to check this,\n     * but we can follow what libstagefright does:\n     * https://android.googlesource.com/platform/frameworks/av/+/master/media/libstagefright/MediaCodecList.cpp#293\n     *\n     * @param encoder encoder\n     * @return true if hardware\n     */\n    @SuppressLint(\"NewApi\")\n    @VisibleForTesting\n    boolean isHardwareEncoder(@NonNull String encoder) {\n        encoder = encoder.toLowerCase();\n        boolean isSoftwareEncoder = encoder.startsWith(\"omx.google.\")\n                || encoder.startsWith(\"c2.android.\")\n                || (!encoder.startsWith(\"omx.\") && !encoder.startsWith(\"c2.\"));\n        return !isSoftwareEncoder;\n    }\n\n    /**\n     * Finds the encoder we'll be using, depending on the given mode flag:\n     * - {@link #MODE_RESPECT_ORDER} will just take the first of the list\n     * - {@link #MODE_PREFER_HARDWARE} will prefer hardware encoders\n     * Throws if we find no encoder for this type.\n     *\n     * @param encoders encoders\n     * @param mimeType mime type\n     * @param mode mode\n     * @return encoder\n     */\n    @SuppressLint(\"NewApi\")\n    @NonNull\n    @VisibleForTesting\n    MediaCodecInfo findDeviceEncoder(@NonNull List<MediaCodecInfo> encoders,\n                                     @NonNull String mimeType,\n                                     int mode,\n                                     int offset) {\n        ArrayList<MediaCodecInfo> results = new ArrayList<>();\n        for (MediaCodecInfo encoder : encoders) {\n            String[] types = encoder.getSupportedTypes();\n            for (String type : types) {\n                if (type.equalsIgnoreCase(mimeType)) {\n                    results.add(encoder);\n                    break;\n                }\n            }\n        }\n        LOG.i(\"findDeviceEncoder -\", \"type:\", mimeType, \"encoders:\", results.size());\n        if (mode == MODE_PREFER_HARDWARE) {\n            Collections.sort(results, new Comparator<MediaCodecInfo>() {\n                @Override\n                public int compare(MediaCodecInfo o1, MediaCodecInfo o2) {\n                    boolean hw1 = isHardwareEncoder(o1.getName());\n                    boolean hw2 = isHardwareEncoder(o2.getName());\n                    return Boolean.compare(hw2, hw1);\n                }\n            });\n        }\n        if (results.size() < offset + 1) {\n            // This should not be a VideoException or AudioException - we want the process\n            // to crash here.\n            throw new RuntimeException(\"No encoders for type:\" + mimeType);\n        }\n        return results.get(offset);\n    }\n\n    /**\n     * Returns a video size supported by the device encoders.\n     * Throws if input width or height are out of the supported boundaries.\n     *\n     * @param size input size\n     * @return adjusted size\n     */\n    @SuppressLint(\"NewApi\")\n    @NonNull\n    public Size getSupportedVideoSize(@NonNull Size size) {\n        if (!ENABLED) return size;\n        int width = size.getWidth();\n        int height = size.getHeight();\n        double aspect = (double) width / height;\n        LOG.i(\"getSupportedVideoSize - started. width:\", width, \"height:\", height);\n\n        // If width is too large, scale down, but keep aspect ratio.\n        if (mVideoCapabilities.getSupportedWidths().getUpper() < width) {\n            width = mVideoCapabilities.getSupportedWidths().getUpper();\n            height = (int) Math.round(width / aspect);\n            LOG.i(\"getSupportedVideoSize - exceeds maxWidth! width:\", width,\n                    \"height:\", height);\n        }\n\n        // If height is too large, scale down, but keep aspect ratio.\n        if (mVideoCapabilities.getSupportedHeights().getUpper() < height) {\n            height = mVideoCapabilities.getSupportedHeights().getUpper();\n            width = (int) Math.round(aspect * height);\n            LOG.i(\"getSupportedVideoSize - exceeds maxHeight! width:\", width,\n                    \"height:\", height);\n        }\n\n        // Adjust the alignment.\n        while (width % mVideoCapabilities.getWidthAlignment() != 0) width--;\n        while (height % mVideoCapabilities.getHeightAlignment() != 0) height--;\n        LOG.i(\"getSupportedVideoSize - aligned. width:\", width, \"height:\", height);\n\n        // It's still possible that we're BELOW the lower.\n        if (!mVideoCapabilities.getSupportedWidths().contains(width)) {\n            throw new VideoException(\"Width not supported after adjustment.\" +\n                    \" Desired:\" + width +\n                    \" Range:\" + mVideoCapabilities.getSupportedWidths());\n        }\n        if (!mVideoCapabilities.getSupportedHeights().contains(height)) {\n            throw new VideoException(\"Height not supported after adjustment.\" +\n                    \" Desired:\" + height +\n                    \" Range:\" + mVideoCapabilities.getSupportedHeights());\n        }\n\n        // We cannot change the aspect ratio, but the max block count might also be the\n        // issue. Try to find a width that contains a height that would accept our AR.\n        try {\n            if (!mVideoCapabilities.getSupportedHeightsFor(width).contains(height)) {\n                int candidateWidth = width;\n                int minWidth = mVideoCapabilities.getSupportedWidths().getLower();\n                int widthAlignment = mVideoCapabilities.getWidthAlignment();\n                while (candidateWidth >= minWidth) {\n                    // Reduce by 32 and realign just in case, then check if our AR is now\n                    // supported. If it is, restart from scratch to go through the other checks.\n                    candidateWidth -= 32;\n                    while (candidateWidth % widthAlignment != 0) candidateWidth--;\n                    int candidateHeight = (int) Math.round(candidateWidth / aspect);\n                    if (mVideoCapabilities.getSupportedHeightsFor(candidateWidth)\n                            .contains(candidateHeight)) {\n                        LOG.w(\"getSupportedVideoSize - restarting with smaller size.\");\n                        return getSupportedVideoSize(new Size(candidateWidth, candidateHeight));\n                    }\n                }\n            }\n        } catch (IllegalArgumentException ignore) {}\n\n        // It's still possible that we're unsupported for other reasons.\n        if (!mVideoCapabilities.isSizeSupported(width, height)) {\n            throw new VideoException(\"Size not supported for unknown reason.\" +\n                    \" Might be an aspect ratio issue.\" +\n                    \" Desired size:\" + new Size(width, height));\n        }\n        return new Size(width, height);\n    }\n\n    /**\n     * Returns a video bit rate supported by the device encoders.\n     * This means adjusting the input bit rate if needed, to match encoder constraints.\n     *\n     * @param bitRate input rate\n     * @return adjusted rate\n     */\n    @SuppressLint(\"NewApi\")\n    public int getSupportedVideoBitRate(int bitRate) {\n        if (!ENABLED) return bitRate;\n        int newBitRate = mVideoCapabilities.getBitrateRange().clamp(bitRate);\n        LOG.i(\"getSupportedVideoBitRate -\",\n                \"inputRate:\", bitRate,\n                \"adjustedRate:\", newBitRate);\n        return newBitRate;\n    }\n\n    /**\n     * Returns a video frame rate supported by the device encoders.\n     * This means adjusting the input frame rate if needed, to match encoder constraints.\n     *\n     * @param frameRate input rate\n     * @return adjusted rate\n     */\n    @SuppressLint(\"NewApi\")\n    public int getSupportedVideoFrameRate(@NonNull Size size, int frameRate) {\n        if (!ENABLED) return frameRate;\n        int newFrameRate = (int) (double) mVideoCapabilities\n                .getSupportedFrameRatesFor(size.getWidth(), size.getHeight())\n                .clamp((double) frameRate);\n        LOG.i(\"getSupportedVideoFrameRate -\",\n                \"inputRate:\", frameRate,\n                \"adjustedRate:\", newFrameRate);\n        return newFrameRate;\n    }\n\n    /**\n     * Returns an audio bit rate supported by the device encoders.\n     * This means adjusting the input bit rate if needed, to match encoder constraints.\n     *\n     * @param bitRate input rate\n     * @return adjusted rate\n     */\n    @SuppressLint(\"NewApi\")\n    public int getSupportedAudioBitRate(int bitRate) {\n        if (!ENABLED) return bitRate;\n        int newBitRate = mAudioCapabilities.getBitrateRange().clamp(bitRate);\n        LOG.i(\"getSupportedAudioBitRate -\",\n                \"inputRate:\", bitRate,\n                \"adjustedRate:\", newBitRate);\n        return newBitRate;\n    }\n\n\n    // Won't do this for audio sample rate. As far as I remember, the value we're using,\n    // 44.1kHz, is guaranteed to be available, and it's not configurable.\n\n    /**\n     * Returns the name of the video encoder if we were able to determine one.\n     * @return encoder name\n     */\n    @SuppressLint(\"NewApi\")\n    @Nullable\n    public String getVideoEncoder() {\n        if (mVideoEncoder != null) {\n            return mVideoEncoder.getName();\n        } else {\n            return null;\n        }\n    }\n\n    /**\n     * Returns the name of the audio encoder if we were able to determine one.\n     * @return encoder name\n     */\n    @SuppressLint(\"NewApi\")\n    @Nullable\n    public String getAudioEncoder() {\n        if (mAudioEncoder != null) {\n            return mAudioEncoder.getName();\n        } else {\n            return null;\n        }\n    }\n\n    @SuppressLint(\"NewApi\")\n    public void tryConfigureVideo(@NonNull String mimeType,\n                                  @NonNull Size size,\n                                  int frameRate,\n                                  int bitRate) {\n        if (mVideoEncoder != null) {\n            MediaCodec codec = null;\n            try {\n                MediaFormat format = MediaFormat.createVideoFormat(mimeType, size.getWidth(),\n                        size.getHeight());\n                format.setInteger(MediaFormat.KEY_COLOR_FORMAT,\n                        MediaCodecInfo.CodecCapabilities.COLOR_FormatSurface);\n                format.setInteger(MediaFormat.KEY_BIT_RATE, bitRate);\n                format.setInteger(MediaFormat.KEY_FRAME_RATE, frameRate);\n                format.setInteger(MediaFormat.KEY_I_FRAME_INTERVAL, 1);\n                codec = MediaCodec.createByCodecName(mVideoEncoder.getName());\n                codec.configure(format, null, null,\n                        MediaCodec.CONFIGURE_FLAG_ENCODE);\n            } catch (Exception e) {\n                throw new VideoException(\"Failed to configure video codec: \" + e.getMessage());\n            } finally {\n                if (codec != null) {\n                    try {\n                        codec.release();\n                    } catch (Exception ignore) {}\n                }\n            }\n        }\n    }\n\n    @SuppressLint(\"NewApi\")\n    public void tryConfigureAudio(@NonNull String mimeType,\n                                  int bitRate,\n                                  int sampleRate,\n                                  int channels) {\n        if (mAudioEncoder != null) {\n            MediaCodec codec = null;\n            try {\n                final MediaFormat format = MediaFormat.createAudioFormat(mimeType, sampleRate,\n                        channels);\n                int channelMask = channels == 2 ? AudioFormat.CHANNEL_IN_STEREO\n                        : AudioFormat.CHANNEL_IN_MONO;\n                format.setInteger(MediaFormat.KEY_CHANNEL_MASK, channelMask);\n                format.setInteger(MediaFormat.KEY_BIT_RATE, bitRate);\n\n                codec = MediaCodec.createByCodecName(mAudioEncoder.getName());\n                codec.configure(format, null, null,\n                        MediaCodec.CONFIGURE_FLAG_ENCODE);\n            } catch (Exception e) {\n                throw new AudioException(\"Failed to configure video audio: \" + e.getMessage());\n            } finally {\n                if (codec != null) {\n                    try {\n                        codec.release();\n                    } catch (Exception ignore) {}\n                }\n            }\n        }\n    }\n\n", "target": "device encoders"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/metering/Camera2MeteringTransform.java:Camera2MeteringTransform:0", "source": "\n\n    protected static final String TAG = CLASSTOKEN.class.getSimpleName();\n    protected static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    private final Angles angles;\n    private final Size previewSize;\n    private final Size previewStreamSize;\n    private final boolean previewIsCropping;\n    private final CameraCharacteristics characteristics;\n    private final CaptureRequest.Builder builder;\n\n    public CLASSTOKEN(@NonNull Angles angles,\n                                    @NonNull Size previewSize,\n                                    @NonNull Size previewStreamSize,\n                                    boolean previewIsCropping,\n                                    @NonNull CameraCharacteristics characteristics,\n                                    @NonNull CaptureRequest.Builder builder) {\n        this.angles = angles;\n        this.previewSize = previewSize;\n        this.previewStreamSize = previewStreamSize;\n        this.previewIsCropping = previewIsCropping;\n        this.characteristics = characteristics;\n        this.builder = builder;\n    }\n\n    @NonNull\n    @Override\n    public MeteringRectangle transformMeteringRegion(@NonNull RectF region, int weight) {\n        Rect round = new Rect();\n        region.round(round);\n        return new MeteringRectangle(round, weight);\n    }\n\n    @NonNull\n    @Override\n    public PointF transformMeteringPoint(@NonNull PointF point) {\n        // This is a good Q/A. https://stackoverflow.com/a/33181620/4288782\n        // At first, the point is relative to the View system and does not account\n        // our own cropping. Will keep updating these two below.\n        final PointF referencePoint = new PointF(point.x, point.y);\n        Size referenceSize = previewSize;\n\n        // 1. Account for cropping.\n        // This will enlarge the preview size so that aspect ratio matches.\n        referenceSize = applyPreviewCropping(referenceSize, referencePoint);\n\n        // 2. Scale to the preview stream coordinates.\n        // This will move to the preview stream coordinates by scaling.\n        referenceSize = applyPreviewScale(referenceSize, referencePoint);\n\n        // 3. Rotate to the stream coordinate system.\n        // This leaves us with sensor stream coordinates.\n        referenceSize = applyPreviewToSensorRotation(referenceSize, referencePoint);\n\n        // 4. Move to the crop region coordinate system.\n        // The crop region is the union of all currently active streams.\n        referenceSize = applyCropRegionCoordinates(referenceSize, referencePoint);\n\n        // 5. Move to the active array coordinate system.\n        referenceSize = applyActiveArrayCoordinates(referenceSize, referencePoint);\n        LOG.i(\"input:\", point, \"output (before clipping):\", referencePoint);\n\n        // 6. Probably not needed, but make sure we clip.\n        if (referencePoint.x < 0) referencePoint.x = 0;\n        if (referencePoint.y < 0) referencePoint.y = 0;\n        if (referencePoint.x > referenceSize.getWidth()) referencePoint.x = referenceSize.getWidth();\n        if (referencePoint.y > referenceSize.getHeight()) referencePoint.y = referenceSize.getHeight();\n        LOG.i(\"input:\", point, \"output (after clipping):\", referencePoint);\n        return referencePoint;\n    }\n\n\n    @SuppressWarnings(\"UnnecessaryLocalVariable\")\n    @NonNull\n    private Size applyPreviewCropping(@NonNull Size referenceSize, @NonNull PointF referencePoint) {\n        Size previewStreamSize = this.previewStreamSize;\n        Size previewSurfaceSize = referenceSize;\n        int referenceWidth = previewSurfaceSize.getWidth();\n        int referenceHeight = previewSurfaceSize.getHeight();\n        AspectRatio previewStreamAspectRatio = AspectRatio.of(previewStreamSize);\n        AspectRatio previewSurfaceAspectRatio = AspectRatio.of(previewSurfaceSize);\n        if (previewIsCropping) {\n            if (previewStreamAspectRatio.toFloat() > previewSurfaceAspectRatio.toFloat()) {\n                // Stream is larger. The x coordinate must be increased: a touch on the left side\n                // of the surface is not on the left size of stream (it's more to the right).\n                float scale = previewStreamAspectRatio.toFloat()\n                        / previewSurfaceAspectRatio.toFloat();\n                referencePoint.x += previewSurfaceSize.getWidth() * (scale - 1F) / 2F;\n                referenceWidth = Math.round(previewSurfaceSize.getWidth() * scale);\n            } else {\n                // Stream is taller. The y coordinate must be increased: a touch on the top side\n                // of the surface is not on the top size of stream (it's a bit lower).\n                float scale = previewSurfaceAspectRatio.toFloat()\n                        / previewStreamAspectRatio.toFloat();\n                referencePoint.y += previewSurfaceSize.getHeight() * (scale - 1F) / 2F;\n                referenceHeight = Math.round(previewSurfaceSize.getHeight() * scale);\n            }\n        }\n        return new Size(referenceWidth, referenceHeight);\n    }\n\n    @NonNull\n    private Size applyPreviewScale(@NonNull Size referenceSize, @NonNull PointF referencePoint) {\n        // The referenceSize how has the same aspect ratio of the previewStreamSize, but they\n        // can still have different size (that is, a scale operation is needed).\n        Size previewStreamSize = this.previewStreamSize;\n        referencePoint.x *= (float) previewStreamSize.getWidth() / referenceSize.getWidth();\n        referencePoint.y *= (float) previewStreamSize.getHeight() / referenceSize.getHeight();\n        return previewStreamSize;\n    }\n\n    @SuppressWarnings(\"SuspiciousNameCombination\")\n    @NonNull\n    private Size applyPreviewToSensorRotation(@NonNull Size referenceSize,\n                                              @NonNull PointF referencePoint) {\n        // Not elegant, but the sin/cos way was failing for some reason.\n        int angle = angles.offset(Reference.SENSOR, Reference.VIEW, Axis.ABSOLUTE);\n        boolean flip = angle % 180 != 0;\n        float tempX = referencePoint.x;\n        float tempY = referencePoint.y;\n        if (angle == 0) {\n            referencePoint.x = tempX;\n            referencePoint.y = tempY;\n        } else if (angle == 90) {\n            referencePoint.x = tempY;\n            referencePoint.y = referenceSize.getWidth() - tempX;\n        } else if (angle == 180) {\n            referencePoint.x = referenceSize.getWidth() - tempX;\n            referencePoint.y = referenceSize.getHeight() - tempY;\n        } else if (angle == 270) {\n            referencePoint.x = referenceSize.getHeight() - tempY;\n            referencePoint.y = tempX;\n        } else {\n            throw new IllegalStateException(\"Unexpected angle \" + angle);\n        }\n        return flip ? referenceSize.flip() : referenceSize;\n    }\n\n    @NonNull\n    private Size applyCropRegionCoordinates(@NonNull Size referenceSize,\n                                            @NonNull PointF referencePoint) {\n        // The input point and size refer to the stream rect.\n        // The stream rect is part of the 'crop region', as described below.\n        // https://source.android.com/devices/camera/camera3_crop_reprocess.html\n        Rect cropRect = builder.get(CaptureRequest.SCALER_CROP_REGION);\n        // For now we don't care about x and y position. Rect should not be null, but let's be safe.\n        int cropRectWidth = cropRect == null ? referenceSize.getWidth() : cropRect.width();\n        int cropRectHeight = cropRect == null ? referenceSize.getHeight() : cropRect.height();\n        // The stream is always centered inside the crop region, and one of the dimensions\n        // should always match. We just increase the other one.\n        referencePoint.x += (cropRectWidth - referenceSize.getWidth()) / 2F;\n        referencePoint.y += (cropRectHeight - referenceSize.getHeight()) / 2F;\n        return new Size(cropRectWidth, cropRectHeight);\n    }\n\n    @NonNull\n    private Size applyActiveArrayCoordinates(@NonNull Size referenceSize,\n                                             @NonNull PointF referencePoint) {\n        // The input point and size refer to the scaler crop region.\n        // We can query for the crop region position inside the active array, so this is easy.\n        Rect cropRect = builder.get(CaptureRequest.SCALER_CROP_REGION);\n        referencePoint.x += cropRect == null ? 0 : cropRect.left;\n        referencePoint.y += cropRect == null ? 0 : cropRect.top;\n        // Finally, get the active rect width and height from characteristics.\n        Rect activeRect = characteristics.get(CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE);\n        if (activeRect == null) { // Should never happen\n            activeRect = new Rect(0, 0, referenceSize.getWidth(),\n                    referenceSize.getHeight());\n        }\n        return new Size(activeRect.width(), activeRect.height());\n    }\n", "target": "camera 2 metering transform"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/picture/Full1PictureRecorder.java:Full1PictureRecorder:0", "source": "\n\n    private final Camera mCamera;\n    private final Camera1Engine mEngine;\n\n    public CLASSTOKEN(@NonNull PictureResult.Stub stub,\n                                @NonNull Camera1Engine engine,\n                                @NonNull Camera camera) {\n        super(stub, engine);\n        mEngine = engine;\n        mCamera = camera;\n\n        // We set the rotation to the camera parameters, but we don't know if the result will be\n        // already rotated with 0 exif, or original with non zero exif. we will have to read EXIF.\n        Camera.Parameters params = mCamera.getParameters();\n        params.setRotation(mResult.rotation);\n        mCamera.setParameters(params);\n    }\n\n    @Override\n    public void take() {\n        LOG.i(\"take() called.\");\n        // Stopping the preview callback is important on older APIs / emulators,\n        // or takePicture can hang and leave the camera in a bad state.\n        mCamera.setPreviewCallbackWithBuffer(null);\n        mEngine.getFrameManager().release();\n        try {\n            mCamera.takePicture(\n                    new Camera.ShutterCallback() {\n                        @Override\n                        public void onShutter() {\n                            LOG.i(\"take(): got onShutter callback.\");\n                            dispatchOnShutter(true);\n                        }\n                    },\n                    null,\n                    null,\n                    new Camera.PictureCallback() {\n                        @Override\n                        public void onPictureTaken(byte[] data, final Camera camera) {\n                            LOG.i(\"take(): got picture callback.\");\n                            int exifRotation;\n                            try {\n                                ExifInterface exif = new ExifInterface(new ByteArrayInputStream(data));\n                                int exifOrientation = exif.getAttributeInt(\n                                        ExifInterface.TAG_ORIENTATION,\n                                        ExifInterface.ORIENTATION_NORMAL);\n                                exifRotation = ExifHelper.getOrientation(exifOrientation);\n                            } catch (IOException e) {\n                                exifRotation = 0;\n                            }\n                            mResult.data = data;\n                            mResult.rotation = exifRotation;\n                            LOG.i(\"take(): starting preview again. \", Thread.currentThread());\n\n                            // It's possible that by the time this callback is invoked, we're not previewing\n                            // anymore, so check before restarting preview.\n                            if (mEngine.getState().isAtLeast(CameraState.PREVIEW)) {\n                                camera.setPreviewCallbackWithBuffer(mEngine);\n                                Size previewStreamSize = mEngine.getPreviewStreamSize(Reference.SENSOR);\n                                if (previewStreamSize == null) {\n                                    throw new IllegalStateException(\"Preview stream size \" +\n                                            \"should never be null here.\");\n                                }\n                                // Need to re-setup the frame manager, otherwise no frames are processed\n                                // after takePicture() is called\n                                mEngine.getFrameManager().setUp(\n                                        mEngine.getFrameProcessingFormat(),\n                                        previewStreamSize,\n                                        mEngine.getAngles()\n                                );\n                                camera.startPreview();\n                            }\n                            dispatchResult();\n                        }\n                    }\n            );\n            LOG.i(\"take() returned.\");\n        } catch (Exception e) {\n            mError = e;\n            dispatchResult();\n        }\n    }\n\n    @Override\n    protected void dispatchResult() {\n        LOG.i(\"dispatching result. Thread:\", Thread.currentThread());\n        super.dispatchResult();\n    }\n", "target": "full 1 picture recorder"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filters/VignetteFilter.java:VignetteFilter:0", "source": "\n\n    private final static String FRAGMENT_SHADER = \"#extension GL_OES_EGL_image_external : require\\n\"\n            + \"precision mediump float;\\n\"\n            + \"uniform samplerExternalOES sTexture;\\n\"\n            + \"uniform float range;\\n\"\n            + \"uniform float inv_max_dist;\\n\"\n            + \"uniform float shade;\\n\"\n            + \"uniform vec2 scale;\\n\"\n            + \"varying vec2 \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\";\\n\"\n            + \"void main() {\\n\"\n            + \"  const float slope = 20.0;\\n\"\n            + \"  vec2 coord = \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\" - vec2(0.5, 0.5);\\n\"\n            + \"  float dist = length(coord * scale);\\n\"\n            + \"  float lumen = shade / (1.0 + exp((dist * inv_max_dist - range) * slope)) \"\n            + \"+ (1.0 - shade);\\n\"\n            + \"  vec4 color = texture2D(sTexture, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\");\\n\"\n            + \"  gl_FragColor = vec4(color.rgb * lumen, color.a);\\n\"\n            + \"}\\n\";\n\n    private float mScale = 0.85f; // 0...1\n    private float mShade = 0.5f; // 0...1\n    private int mWidth = 1;\n    private int mHeight = 1;\n\n    private int mRangeLocation = -1;\n    private int mMaxDistLocation = -1;\n    private int mShadeLocation = -1;\n    private int mScaleLocation = -1;\n\n    public CLASSTOKEN() { }\n\n    @Override\n    public void setSize(int width, int height) {\n        super.setSize(width, height);\n        mWidth = width;\n        mHeight = height;\n    }\n\n    /**\n     * Sets the vignette effect scale (0.0 - 1.0).\n     * @param scale new scale\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public void setVignetteScale(float scale) {\n        if (scale < 0.0f) scale = 0.0f;\n        if (scale > 1.0f) scale = 1.0f;\n        mScale = scale;\n    }\n\n    /**\n     * Sets the vignette effect shade (0.0 - 1.0).\n     * @param shade new shade\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public void setVignetteShade(float shade) {\n        if (shade < 0.0f) shade = 0.0f;\n        if (shade > 1.0f) shade = 1.0f;\n        this.mShade = shade;\n    }\n\n    /**\n     * Gets the current vignette scale.\n     *\n     * @see #setVignetteScale(float)\n     * @return scale\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public float getVignetteScale() {\n        return mScale;\n    }\n\n    /**\n     * Gets the current vignette shade.\n     *\n     * @see #setVignetteShade(float)\n     * @return shade\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public float getVignetteShade() {\n        return mShade;\n    }\n\n\n    @Override\n    public void setParameter1(float value) {\n        setVignetteScale(value);\n    }\n\n    @Override\n    public float getParameter1() {\n        return getVignetteScale();\n    }\n\n    @Override\n    public void setParameter2(float value) {\n        setVignetteShade(value);\n    }\n\n    @Override\n    public float getParameter2() {\n        return getVignetteShade();\n    }\n\n    @NonNull\n    @Override\n    public String getFragmentShader() {\n        return FRAGMENT_SHADER;\n    }\n\n    @Override\n    public void onCreate(int programHandle) {\n        super.onCreate(programHandle);\n        mRangeLocation = GLES20.glGetUniformLocation(programHandle, \"range\");\n        Egloo.checkGlProgramLocation(mRangeLocation, \"range\");\n        mMaxDistLocation = GLES20.glGetUniformLocation(programHandle, \"inv_max_dist\");\n        Egloo.checkGlProgramLocation(mMaxDistLocation, \"inv_max_dist\");\n        mShadeLocation = GLES20.glGetUniformLocation(programHandle, \"shade\");\n        Egloo.checkGlProgramLocation(mShadeLocation, \"shade\");\n        mScaleLocation = GLES20.glGetUniformLocation(programHandle, \"scale\");\n        Egloo.checkGlProgramLocation(mScaleLocation, \"scale\");\n    }\n\n    @Override\n    public void onDestroy() {\n        super.onDestroy();\n        mRangeLocation = -1;\n        mMaxDistLocation = -1;\n        mShadeLocation = -1;\n        mScaleLocation = -1;\n    }\n\n    @Override\n    protected void onPreDraw(long timestampUs, @NonNull float[] transformMatrix) {\n        super.onPreDraw(timestampUs, transformMatrix);\n        float[] scale = new float[2];\n        if (mWidth > mHeight) {\n            scale[0] = 1f;\n            scale[1] = ((float) mHeight) / mWidth;\n        } else {\n            scale[0] = ((float) mWidth) / mHeight;\n            scale[1] = 1f;\n        }\n        GLES20.glUniform2fv(mScaleLocation, 1, scale, 0);\n        Egloo.checkGlError(\"glUniform2fv\");\n\n        float maxDist = ((float) Math.sqrt(scale[0] * scale[0] + scale[1] * scale[1])) * 0.5f;\n        GLES20.glUniform1f(mMaxDistLocation, 1F / maxDist);\n        Egloo.checkGlError(\"glUniform1f\");\n\n        GLES20.glUniform1f(mShadeLocation, mShade);\n        Egloo.checkGlError(\"glUniform1f\");\n\n        // The 'range' is between 1.3 to 0.6. When scale is zero then range is 1.3\n        // which means no vignette at all because the luminousity difference is\n        // less than 1/256 and will cause nothing.\n        float range = (1.30f - (float) Math.sqrt(mScale) * 0.7f);\n        GLES20.glUniform1f(mRangeLocation, range);\n        Egloo.checkGlError(\"glUniform1f\");\n    }\n", "target": "vignette filter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filters/BlackAndWhiteFilter.java:BlackAndWhiteFilter:0", "source": "\n\n    private final static String FRAGMENT_SHADER = \"#extension GL_OES_EGL_image_external : require\\n\"\n            + \"precision mediump float;\\n\"\n            + \"varying vec2 \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\";\\n\"\n            + \"uniform samplerExternalOES sTexture;\\n\" + \"void main() {\\n\"\n            + \"  vec4 color = texture2D(sTexture, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\");\\n\"\n            + \"  float colorR = (color.r + color.g + color.b) / 3.0;\\n\"\n            + \"  float colorG = (color.r + color.g + color.b) / 3.0;\\n\"\n            + \"  float colorB = (color.r + color.g + color.b) / 3.0;\\n\"\n            + \"  gl_FragColor = vec4(colorR, colorG, colorB, color.a);\\n\"\n            + \"}\\n\";\n\n    public CLASSTOKEN() { }\n\n    @NonNull\n    @Override\n    public String getFragmentShader() {\n        return FRAGMENT_SHADER;\n    }\n", "target": "black and white filter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/orchestrator/CameraOrchestrator.java:Job:1", "source": "\n        public final String name;\n        public final TaskCompletionSource<T> source = new TaskCompletionSource<>();\n        public final Callable<Task<T>> scheduler;\n        public final boolean dispatchExceptions;\n        public final long startTime;\n\n        private CLASSTOKEN(@NonNull String name, @NonNull Callable<Task<T>> scheduler, boolean dispatchExceptions, long startTime) {\n            this.name = name;\n            this.scheduler = scheduler;\n            this.dispatchExceptions = dispatchExceptions;\n            this.startTime = startTime;\n        }\n    ", "target": "job"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/lock/LockAction.java:LockAction:0", "source": "\n\n    private final BaseAction action = Actions.together(\n            new ExposureLock(),\n            new FocusLock(),\n            new WhiteBalanceLock()\n    );\n\n    @NonNull\n    @Override\n    public BaseAction getAction() {\n        return action;\n    }\n", "target": "lock action"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/action/TogetherAction.java:TogetherAction:0", "source": "\n    // Need to be BaseAction so we can call onStart() instead of start()\n    private final List<BaseAction> actions;\n    private final List<BaseAction> runningActions;\n\n    CLASSTOKEN(@NonNull final List<BaseAction> actions) {\n        this.actions = new ArrayList<>(actions);\n        this.runningActions = new ArrayList<>(actions);\n        for (BaseAction action : actions) {\n            action.addCallback(new ActionCallback() {\n                @Override\n                public void onActionStateChanged(@NonNull Action action, int state) {\n                    if (state == STATE_COMPLETED) {\n                        //noinspection SuspiciousMethodCalls\n                        runningActions.remove(action);\n                    }\n                    if (runningActions.isEmpty()) {\n                        setState(STATE_COMPLETED);\n                    }\n                }\n            });\n        }\n    }\n\n    @Override\n    protected void onStart(@NonNull ActionHolder holder) {\n        super.onStart(holder);\n        for (BaseAction action : actions) {\n            if (!action.isCompleted()) action.onStart(holder);\n        }\n    }\n\n    @Override\n    protected void onAbort(@NonNull ActionHolder holder) {\n        super.onAbort(holder);\n        for (BaseAction action : actions) {\n            if (!action.isCompleted()) action.onAbort(holder);\n        }\n    }\n\n    @Override\n    public void onCaptureStarted(@NonNull ActionHolder holder, @NonNull CaptureRequest request) {\n        super.onCaptureStarted(holder, request);\n        for (BaseAction action : actions) {\n            if (!action.isCompleted()) action.onCaptureStarted(holder, request);\n        }\n    }\n\n    @Override\n    public void onCaptureProgressed(@NonNull ActionHolder holder,\n                                    @NonNull CaptureRequest request,\n                                    @NonNull CaptureResult result) {\n        super.onCaptureProgressed(holder, request, result);\n        for (BaseAction action : actions) {\n            if (!action.isCompleted()) action.onCaptureProgressed(holder, request, result);\n        }\n    }\n\n    @Override\n    public void onCaptureCompleted(@NonNull ActionHolder holder,\n                                   @NonNull CaptureRequest request,\n                                   @NonNull TotalCaptureResult result) {\n        super.onCaptureCompleted(holder, request, result);\n        for (BaseAction action : actions) {\n            if (!action.isCompleted()) action.onCaptureCompleted(holder, request, result);\n        }\n    }\n", "target": "together action"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/orchestrator/CameraOrchestrator.java:CameraOrchestrator:0", "source": "\n\n    protected static final String TAG = CLASSTOKEN.class.getSimpleName();\n    protected static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    public interface Callback {\n        @NonNull\n        WorkerHandler getJobWorker(@NonNull String job);\n        void handleJobException(@NonNull String job, @NonNull Exception exception);\n    }\n\n    protected static class Job<T> {\n        public final String name;\n        public final TaskCompletionSource<T> source = new TaskCompletionSource<>();\n        public final Callable<Task<T>> scheduler;\n        public final boolean dispatchExceptions;\n        public final long startTime;\n\n        private Job(@NonNull String name, @NonNull Callable<Task<T>> scheduler, boolean dispatchExceptions, long startTime) {\n            this.name = name;\n            this.scheduler = scheduler;\n            this.dispatchExceptions = dispatchExceptions;\n            this.startTime = startTime;\n        }\n    }\n\n    protected final Callback mCallback;\n    protected final ArrayDeque<Job<?>> mJobs = new ArrayDeque<>();\n    protected boolean mJobRunning = false;\n    protected final Object mJobsLock = new Object();\n\n    public CLASSTOKEN(@NonNull Callback callback) {\n        mCallback = callback;\n    }\n\n    @NonNull\n    public Task<Void> schedule(@NonNull String name,\n                               boolean dispatchExceptions,\n                               @NonNull Runnable job) {\n        return scheduleDelayed(name, dispatchExceptions, 0L, job);\n    }\n\n    @NonNull\n    public Task<Void> scheduleDelayed(@NonNull String name,\n                                      boolean dispatchExceptions,\n                                      long minDelay,\n                                      @NonNull final Runnable job) {\n        return scheduleInternal(name, dispatchExceptions, minDelay, new Callable<Task<Void>>() {\n            @Override\n            public Task<Void> call() {\n                job.run();\n                return Tasks.forResult(null);\n            }\n        });\n    }\n\n    @NonNull\n    public <T> Task<T> schedule(@NonNull String name,\n                                boolean dispatchExceptions,\n                                @NonNull Callable<Task<T>> scheduler) {\n        return scheduleInternal(name, dispatchExceptions, 0L, scheduler);\n    }\n\n    @NonNull\n    private <T> Task<T> scheduleInternal(@NonNull String name,\n                                         boolean dispatchExceptions,\n                                         long minDelay,\n                                         @NonNull Callable<Task<T>> scheduler) {\n        LOG.i(name.toUpperCase(), \"- Scheduling.\");\n        Job<T> job = new Job<>(name, scheduler, dispatchExceptions,\n                System.currentTimeMillis() + minDelay);\n        synchronized (mJobsLock) {\n            mJobs.addLast(job);\n            sync(minDelay);\n        }\n        return job.source.getTask();\n    }\n\n    @GuardedBy(\"mJobsLock\")\n    private void sync(long after) {\n        // Jumping on the message handler even if after = 0L should avoid StackOverflow errors.\n        mCallback.getJobWorker(\"_sync\").post(after, new Runnable() {\n            @SuppressWarnings(\"StatementWithEmptyBody\")\n            @Override\n            public void run() {\n                Job<?> job = null;\n                synchronized (mJobsLock) {\n                    if (mJobRunning) {\n                        // Do nothing, job will be picked in executed().\n                    } else {\n                        long now = System.currentTimeMillis();\n                        for (Job<?> candidate : mJobs) {\n                            if (candidate.startTime <= now) {\n                                job = candidate;\n                                break;\n                            }\n                        }\n                        if (job != null) {\n                            mJobRunning = true;\n                        }\n                    }\n                }\n                // This must be out of mJobsLock! See comments in execute().\n                if (job != null) execute(job);\n            }\n        });\n    }\n\n    // Since we use WorkerHandler.run(), the job can end up being executed on the current thread.\n    // For this reason, it's important that this method is never guarded by mJobsLock! Because\n    // all threads can be waiting on that, even the UI thread e.g. through scheduleInternal.\n    private <T> void execute(@NonNull final Job<T> job) {\n        final WorkerHandler worker = mCallback.getJobWorker(job.name);\n        worker.run(new Runnable() {\n            @Override\n            public void run() {\n                try {\n                    LOG.i(job.name.toUpperCase(), \"- Executing.\");\n                    Task<T> task = job.scheduler.call();\n                    onComplete(task, worker, new OnCompleteListener<T>() {\n                        @Override\n                        public void onComplete(@NonNull Task<T> task) {\n                            Exception e = task.getException();\n                            if (e != null) {\n                                LOG.w(job.name.toUpperCase(), \"- Finished with ERROR.\", e);\n                                if (job.dispatchExceptions) {\n                                    mCallback.handleJobException(job.name, e);\n                                }\n                                job.source.trySetException(e);\n                            } else if (task.isCanceled()) {\n                                LOG.i(job.name.toUpperCase(), \"- Finished because ABORTED.\");\n                                job.source.trySetException(new CancellationException());\n                            } else {\n                                LOG.i(job.name.toUpperCase(), \"- Finished.\");\n                                job.source.trySetResult(task.getResult());\n                            }\n                            synchronized (mJobsLock) {\n                                executed(job);\n                            }\n                        }\n                    });\n                } catch (Exception e) {\n                    LOG.i(job.name.toUpperCase(), \"- Finished with ERROR.\", e);\n                    if (job.dispatchExceptions) {\n                        mCallback.handleJobException(job.name, e);\n                    }\n                    job.source.trySetException(e);\n                    synchronized (mJobsLock) {\n                        executed(job);\n                    }\n                }\n            }\n        });\n    }\n\n    @GuardedBy(\"mJobsLock\")\n    private <T> void executed(Job<T> job) {\n        if (!mJobRunning) {\n            throw new IllegalStateException(\"mJobRunning was not true after completing job=\" + job.name);\n        }\n        mJobRunning = false;\n        mJobs.remove(job);\n        sync(0L);\n    }\n\n    public void remove(@NonNull String name) {\n        trim(name, 0);\n    }\n\n    public void trim(@NonNull String name, int allowed) {\n        synchronized (mJobsLock) {\n            List<Job<?>> scheduled = new ArrayList<>();\n            for (Job<?> job : mJobs) {\n                if (job.name.equals(name)) {\n                    scheduled.add(job);\n                }\n            }\n            LOG.v(\"trim: name=\", name, \"scheduled=\", scheduled.size(), \"allowed=\", allowed);\n            int existing = Math.max(scheduled.size() - allowed, 0);\n            if (existing > 0) {\n                // To remove the oldest ones first, we must reverse the list.\n                // Note that we will potentially remove a job that is being executed: we don't\n                // have a mechanism to cancel the ongoing execution, but it shouldn't be a problem.\n                Collections.reverse(scheduled);\n                scheduled = scheduled.subList(0, existing);\n                for (Job<?> job : scheduled) {\n                    mJobs.remove(job);\n                }\n            }\n        }\n    }\n\n    public void reset() {\n        synchronized (mJobsLock) {\n            Set<String> all = new HashSet<>();\n            for (Job<?> job : mJobs) {\n                all.add(job.name);\n            }\n            for (String job : all) {\n                remove(job);\n            }\n        }\n    }\n\n    private static <T> void onComplete(@NonNull final Task<T> task,\n                                       @NonNull WorkerHandler handler,\n                                       @NonNull final OnCompleteListener<T> listener) {\n        if (task.isComplete()) {\n            handler.run(new Runnable() {\n                @Override\n                public void run() {\n                    listener.onComplete(task);\n                }\n            });\n        } else {\n            task.addOnCompleteListener(handler.getExecutor(), listener);\n        }\n    }\n", "target": "camera orchestrator"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/meter/BaseReset.java:BaseReset:0", "source": "\n\n    private boolean resetArea;\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected CLASSTOKEN(boolean resetArea) {\n        this.resetArea = resetArea;\n    }\n\n    @Override\n    protected final void onStart(@NonNull ActionHolder holder) {\n        super.onStart(holder);\n        MeteringRectangle area = null;\n        if (resetArea) {\n            Rect rect = readCharacteristic(CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE,\n                    new Rect());\n            area = new MeteringRectangle(rect, MeteringRectangle.METERING_WEIGHT_DONT_CARE);\n        }\n        onStarted(holder, area);\n    }\n\n    protected abstract void onStarted(@NonNull ActionHolder holder,\n                                      @Nullable MeteringRectangle area);\n", "target": "base reset"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/size/SizeSelectors.java:AndSelector:2", "source": "\n\n        private SizeSelector[] values;\n\n        private CLASSTOKEN(@NonNull SizeSelector... values) {\n            this.values = values;\n        }\n\n        @Override\n        @NonNull\n        public List<Size> select(@NonNull List<Size> source) {\n            List<Size> temp = source;\n            for (SizeSelector selector : values) {\n                temp = selector.select(temp);\n            }\n            return temp;\n        }\n    ", "target": "and selector"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/picture/PictureRecorder.java:PictureRecorder:0", "source": "\n\n    /**\n     * Listens for picture recorder events.\n     */\n    public interface PictureResultListener {\n\n        /**\n         * The shutter was activated.\n         * @param didPlaySound whether a sound was played\n         */\n        void onPictureShutter(boolean didPlaySound);\n\n        /**\n         * Picture was taken or there was some error, if\n         * the result is null.\n         * @param result the result or null if there was some error\n         * @param error the error or null if there wasn't any\n         */\n        void onPictureResult(@Nullable PictureResult.Stub result, @Nullable Exception error);\n    }\n\n    @VisibleForTesting(otherwise = VisibleForTesting.PROTECTED) PictureResult.Stub mResult;\n    @VisibleForTesting PictureResultListener mListener;\n    @SuppressWarnings(\"WeakerAccess\")\n    protected Exception mError;\n\n    /**\n     * Creates a new picture recorder.\n     * @param stub a picture stub\n     * @param listener a listener\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public CLASSTOKEN(@NonNull PictureResult.Stub stub,\n                           @Nullable PictureResultListener listener) {\n        mResult = stub;\n        mListener = listener;\n    }\n\n    /**\n     * Takes a picture.\n     */\n    public abstract void take();\n\n    /**\n     * Subclasses can call this to notify that the shutter was activated,\n     * and whether it did play some sound or not.\n     * @param didPlaySound whether it played sounds\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    protected void dispatchOnShutter(boolean didPlaySound) {\n        if (mListener != null) mListener.onPictureShutter(didPlaySound);\n    }\n\n    /**\n     * Subclasses can call this to notify that the result was obtained,\n     * either with some error (null result) or with the actual stub, filled.\n     */\n    protected void dispatchResult() {\n        if (mListener != null) {\n            mListener.onPictureResult(mResult, mError);\n            mListener = null;\n            mResult = null;\n        }\n    }\n", "target": "picture recorder"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/meter/ExposureMeter.java:ExposureMeter:0", "source": "\n\n    private static final String TAG = CLASSTOKEN.class.getSimpleName();\n    private static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    private static final int STATE_WAITING_PRECAPTURE = 0;\n    private static final int STATE_WAITING_PRECAPTURE_END = 1;\n\n    private boolean mSupportsAreas = false;\n    private boolean mSupportsTrigger = false;\n\n    @SuppressWarnings(\"WeakerAccess\")\n    public CLASSTOKEN(@NonNull List<MeteringRectangle> areas, boolean skipIfPossible) {\n        super(areas, skipIfPossible);\n    }\n\n    @Override\n    protected boolean checkIsSupported(@NonNull ActionHolder holder) {\n        // In our case, this means checking if we support the AE precapture trigger.\n        boolean isLegacy = readCharacteristic(\n                CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL, -1)\n                == CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY;\n        Integer aeMode = holder.getBuilder(this).get(CaptureRequest.CONTROL_AE_MODE);\n        boolean isAEOn = aeMode != null &&\n                (aeMode == CameraCharacteristics.CONTROL_AE_MODE_ON\n                        || aeMode == CameraCharacteristics.CONTROL_AE_MODE_ON_ALWAYS_FLASH\n                        || aeMode == CameraCharacteristics.CONTROL_AE_MODE_ON_AUTO_FLASH\n                        || aeMode == CameraCharacteristics.CONTROL_AE_MODE_ON_AUTO_FLASH_REDEYE\n                        || aeMode == 5\n                        /* CameraCharacteristics.CONTROL_AE_MODE_ON_EXTERNAL_FLASH, API 28 */);\n        mSupportsTrigger = !isLegacy;\n        mSupportsAreas = readCharacteristic(CameraCharacteristics.CONTROL_MAX_REGIONS_AE,\n                0) > 0;\n        boolean result = isAEOn && (mSupportsTrigger || mSupportsAreas);\n        LOG.i(\"checkIsSupported:\", result,\n                \"trigger:\", mSupportsTrigger,\n                \"areas:\", mSupportsAreas);\n        return result;\n    }\n\n    @Override\n    protected boolean checkShouldSkip(@NonNull ActionHolder holder) {\n        CaptureResult lastResult = holder.getLastResult(this);\n        if (lastResult != null) {\n            Integer aeState = lastResult.get(CaptureResult.CONTROL_AE_STATE);\n            boolean result = aeState != null && aeState == CaptureResult.CONTROL_AE_STATE_CONVERGED;\n            LOG.i(\"checkShouldSkip:\", result);\n            return result;\n        } else {\n            LOG.i(\"checkShouldSkip: false - lastResult is null.\");\n            return false;\n        }\n    }\n\n    @Override\n    protected void onStarted(@NonNull ActionHolder holder, @NonNull List<MeteringRectangle> areas) {\n        LOG.i(\"onStarted:\", \"with areas:\", areas);\n\n        if (mSupportsAreas && !areas.isEmpty()) {\n            int max = readCharacteristic(CameraCharacteristics.CONTROL_MAX_REGIONS_AE, 0);\n            max = Math.min(max, areas.size());\n            holder.getBuilder(this).set(CaptureRequest.CONTROL_AE_REGIONS,\n                    areas.subList(0, max).toArray(new MeteringRectangle[]{}));\n        }\n\n        if (mSupportsTrigger) {\n            holder.getBuilder(this).set(\n                    CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER,\n                    CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_START);\n        }\n\n        // Apply\n        holder.applyBuilder(this);\n        if (mSupportsTrigger) {\n            setState(STATE_WAITING_PRECAPTURE);\n        } else {\n            setState(STATE_WAITING_PRECAPTURE_END);\n        }\n    }\n\n    @Override\n    protected void onCompleted(@NonNull ActionHolder holder) {\n        super.onCompleted(holder);\n        // Remove (but not apply) the risky parameter so it is not included in new requests.\n        // Documentation about this key says that this should be allowed.\n        holder.getBuilder(this).set(CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER, null);\n    }\n\n    @Override\n    public void onCaptureCompleted(@NonNull ActionHolder holder, @NonNull CaptureRequest request,\n                                   @NonNull TotalCaptureResult result) {\n        super.onCaptureCompleted(holder, request, result);\n        Integer aeState = result.get(CaptureResult.CONTROL_AE_STATE);\n        Integer aeTriggerState = result.get(CaptureResult.CONTROL_AE_PRECAPTURE_TRIGGER);\n        LOG.i(\"onCaptureCompleted:\", \"aeState:\", aeState, \"aeTriggerState:\", aeTriggerState);\n        if (aeState == null) return;\n\n        if (getState() == STATE_WAITING_PRECAPTURE) {\n            switch (aeState) {\n                case CaptureResult.CONTROL_AE_STATE_PRECAPTURE: {\n                    setState(STATE_WAITING_PRECAPTURE_END);\n                    break;\n                }\n                case CaptureResult.CONTROL_AE_STATE_CONVERGED:\n                case CaptureResult.CONTROL_AE_STATE_FLASH_REQUIRED: {\n                    // PRECAPTURE is a transient state. Being here might mean that precapture run\n                    // and was successful, OR that the trigger was not even received yet. To\n                    // distinguish, check the trigger state.\n                    if (aeTriggerState != null && aeTriggerState\n                            == CaptureResult.CONTROL_AE_PRECAPTURE_TRIGGER_START) {\n                        setSuccessful(true);\n                        setState(STATE_COMPLETED);\n                    }\n                    break;\n                }\n                case CaptureResult.CONTROL_AE_STATE_LOCKED: {\n                    // There's nothing we can do, AE was locked, triggers are ignored.\n                    setSuccessful(false);\n                    setState(STATE_COMPLETED);\n                    break;\n                }\n                case CaptureResult.CONTROL_AE_STATE_INACTIVE:\n                case CaptureResult.CONTROL_AE_STATE_SEARCHING: {\n                    // Wait...\n                    break;\n                }\n            }\n        }\n\n        if (getState() == STATE_WAITING_PRECAPTURE_END) {\n            switch (aeState) {\n                case CaptureResult.CONTROL_AE_STATE_CONVERGED:\n                case CaptureResult.CONTROL_AE_STATE_FLASH_REQUIRED: {\n                    setSuccessful(true);\n                    setState(STATE_COMPLETED);\n                    break;\n                }\n                case CaptureResult.CONTROL_AE_STATE_LOCKED: {\n                    // There's nothing we can do, AE was locked, triggers are ignored.\n                    setSuccessful(false);\n                    setState(STATE_COMPLETED);\n                    break;\n                }\n                case CaptureResult.CONTROL_AE_STATE_PRECAPTURE:\n                case CaptureResult.CONTROL_AE_STATE_INACTIVE:\n                case CaptureResult.CONTROL_AE_STATE_SEARCHING: {\n                    // Wait...\n                    break;\n                }\n            }\n        }\n    }\n", "target": "exposure meter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filters/SaturationFilter.java:SaturationFilter:0", "source": "\n\n    private final static String FRAGMENT_SHADER = \"#extension GL_OES_EGL_image_external : require\\n\"\n            + \"precision mediump float;\\n\"\n            + \"uniform samplerExternalOES sTexture;\\n\"\n            + \"uniform float scale;\\n\"\n            + \"uniform vec3 exponents;\\n\"\n            + \"float shift;\\n\"\n            + \"vec3 weights;\\n\"\n            + \"varying vec2 \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\";\\n\"\n            + \"void main() {\\n\"\n            + \"  weights[0] = \" + 2f / 8f + \";\\n\"\n            + \"  weights[1] = \" + 5f / 8f + \";\\n\"\n            + \"  weights[2] = \" + 1f / 8f + \";\\n\"\n            + \"  shift = \" + 1.0f / 255.0f + \";\\n\"\n            + \"  vec4 oldcolor = texture2D(sTexture, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME\n            + \");\\n\"\n            + \"  float kv = dot(oldcolor.rgb, weights) + shift;\\n\"\n            + \"  vec3 new_color = scale * oldcolor.rgb + (1.0 - scale) * kv;\\n\"\n            + \"  gl_FragColor = vec4(new_color, oldcolor.a);\\n\"\n            + \"  vec4 color = texture2D(sTexture, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME\n            + \");\\n\"\n            + \"  float de = dot(color.rgb, weights);\\n\"\n            + \"  float inv_de = 1.0 / de;\\n\"\n            + \"  vec3 verynew_color = de * pow(color.rgb * inv_de, exponents);\\n\"\n            + \"  float max_color = max(max(max(verynew_color.r, verynew_color.g), \"\n            + \"verynew_color.b), 1.0);\\n\"\n            + \"  gl_FragColor = gl_FragColor+vec4(verynew_color / max_color, color.a);\\n\"\n            + \"}\\n\";\n\n    private float scale = 1F; // -1...1\n    private int scaleLocation = -1;\n    private int exponentsLocation = -1;\n\n    public CLASSTOKEN() { }\n\n    /**\n     * Sets the saturation correction value:\n     * -1.0: fully desaturated, grayscale.\n     * 0.0: no change.\n     * +1.0: fully saturated.\n     *\n     * @param value new value\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public void setSaturation(float value) {\n        if (value < -1F) value = -1F;\n        if (value > 1F) value = 1F;\n        scale = value;\n    }\n\n    /**\n     * Returns the current saturation.\n     *\n     * @see #setSaturation(float)\n     * @return saturation\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public float getSaturation() {\n        return scale;\n    }\n\n    @Override\n    public void setParameter1(float value) {\n        setSaturation(2F * value - 1F);\n    }\n\n    @Override\n    public float getParameter1() {\n        return (getSaturation() + 1F) / 2F;\n    }\n\n    @NonNull\n    @Override\n    public String getFragmentShader() {\n        return FRAGMENT_SHADER;\n    }\n\n    @Override\n    public void onCreate(int programHandle) {\n        super.onCreate(programHandle);\n        scaleLocation = GLES20.glGetUniformLocation(programHandle, \"scale\");\n        Egloo.checkGlProgramLocation(scaleLocation, \"scale\");\n        exponentsLocation = GLES20.glGetUniformLocation(programHandle, \"exponents\");\n        Egloo.checkGlProgramLocation(exponentsLocation, \"exponents\");\n    }\n\n    @Override\n    public void onDestroy() {\n        super.onDestroy();\n        scaleLocation = -1;\n        exponentsLocation = -1;\n    }\n\n    @Override\n    protected void onPreDraw(long timestampUs, @NonNull float[] transformMatrix) {\n        super.onPreDraw(timestampUs, transformMatrix);\n        if (scale > 0.0f) {\n            GLES20.glUniform1f(scaleLocation, 0F);\n            Egloo.checkGlError(\"glUniform1f\");\n            GLES20.glUniform3f(exponentsLocation,\n                    (0.9f * scale) + 1.0f,\n                    (2.1f * scale) + 1.0f,\n                    (2.7f * scale) + 1.0f\n            );\n            Egloo.checkGlError(\"glUniform3f\");\n        } else {\n            GLES20.glUniform1f(scaleLocation, 1.0F + scale);\n            Egloo.checkGlError(\"glUniform1f\");\n            GLES20.glUniform3f(exponentsLocation, 0F, 0F, 0F);\n            Egloo.checkGlError(\"glUniform3f\");\n        }\n    }\n", "target": "saturation filter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/Camera2Engine.java:Camera2Engine:0", "source": "\n\n    private static final int FRAME_PROCESSING_FORMAT = ImageFormat.YUV_420_888;\n    @VisibleForTesting static final long METER_TIMEOUT = 5000;\n    private static final long METER_TIMEOUT_SHORT = 2500;\n\n    private final CameraManager mManager;\n    private String mCameraId;\n    private CameraDevice mCamera;\n    private CameraCharacteristics mCameraCharacteristics;\n    private CameraCaptureSession mSession;\n    private CaptureRequest.Builder mRepeatingRequestBuilder;\n    private TotalCaptureResult mLastRepeatingResult;\n    private final Camera2Mapper mMapper = Camera2Mapper.get();\n\n    // Frame processing\n    private ImageReader mFrameProcessingReader; // need this or the reader surface is collected\n    private Surface mFrameProcessingSurface;\n\n    // Preview\n    private Surface mPreviewStreamSurface;\n\n    // Video recording\n    // When takeVideo is called, we restart the session.\n    private VideoResult.Stub mFullVideoPendingStub;\n\n    // Picture capturing\n    private ImageReader mPictureReader;\n    private final boolean mPictureCaptureStopsPreview = false; // can be configurable at some point\n\n    // Actions\n    // Use COW to properly synchronize the list. We'll iterate much more than mutate\n    private final List<Action> mActions = new CopyOnWriteArrayList<>();\n    private MeterAction mMeterAction;\n\n    public CLASSTOKEN(Callback callback) {\n        super(callback);\n        mManager = (CameraManager) getCallback().getContext()\n                .getSystemService(Context.CAMERA_SERVICE);\n        new LogAction().start(this);\n    }\n\n    //region Utilities\n\n    @VisibleForTesting\n    @NonNull\n    <T> T readCharacteristic(@NonNull CameraCharacteristics.Key<T> key,\n                             @NonNull T fallback) {\n        return readCharacteristic(mCameraCharacteristics, key, fallback);\n    }\n\n    @NonNull\n    private <T> T readCharacteristic(@NonNull CameraCharacteristics characteristics,\n                             @NonNull CameraCharacteristics.Key<T> key,\n                             @NonNull T fallback) {\n        T value = characteristics.get(key);\n        return value == null ? fallback : value;\n    }\n\n    @NonNull\n    private CameraException createCameraException(@NonNull CameraAccessException exception) {\n        int reason;\n        switch (exception.getReason()) {\n            case CameraAccessException.CAMERA_DISABLED:\n            case CameraAccessException.CAMERA_IN_USE:\n            case CameraAccessException.MAX_CAMERAS_IN_USE: {\n                reason = CameraException.REASON_FAILED_TO_CONNECT;\n                break;\n            }\n            case CameraAccessException.CAMERA_ERROR:\n            case CameraAccessException.CAMERA_DISCONNECTED: {\n                reason = CameraException.REASON_DISCONNECTED;\n                break;\n            }\n            default: {\n                reason = CameraException.REASON_UNKNOWN;\n                break;\n            }\n        }\n        return new CameraException(exception, reason);\n    }\n\n    @NonNull\n    private CameraException createCameraException(int stateCallbackError) {\n        int reason;\n        switch (stateCallbackError) {\n            case CameraDevice.StateCallback.ERROR_CAMERA_DISABLED: // Device policy\n            case CameraDevice.StateCallback.ERROR_CAMERA_DEVICE: // Fatal error\n            case CameraDevice.StateCallback.ERROR_CAMERA_SERVICE: // Fatal error, might have to\n                // restart the device\n            case CameraDevice.StateCallback.ERROR_CAMERA_IN_USE:\n            case CameraDevice.StateCallback.ERROR_MAX_CAMERAS_IN_USE: {\n                reason = CameraException.REASON_FAILED_TO_CONNECT;\n                break;\n            }\n            default: {\n                reason = CameraException.REASON_UNKNOWN;\n                break;\n            }\n        }\n        return new CameraException(reason);\n    }\n\n    /**\n     * When creating a new builder, we want to\n     * - set it to {@link #mRepeatingRequestBuilder}, the current one\n     * - add a tag for the template just in case\n     * - apply all the current parameters\n     */\n    @SuppressWarnings(\"UnusedReturnValue\")\n    @NonNull\n    private CaptureRequest.Builder createRepeatingRequestBuilder(int template)\n            throws CameraAccessException {\n        CaptureRequest.Builder oldBuilder = mRepeatingRequestBuilder;\n        mRepeatingRequestBuilder = mCamera.createCaptureRequest(template);\n        mRepeatingRequestBuilder.setTag(template);\n        applyAllParameters(mRepeatingRequestBuilder, oldBuilder);\n        return mRepeatingRequestBuilder;\n    }\n\n    /**\n     * Sets up the repeating request builder with default surfaces and extra ones\n     * if needed (like a video recording surface).\n     */\n    private void addRepeatingRequestBuilderSurfaces(@NonNull Surface... extraSurfaces) {\n        mRepeatingRequestBuilder.addTarget(mPreviewStreamSurface);\n        if (mFrameProcessingSurface != null) {\n            mRepeatingRequestBuilder.addTarget(mFrameProcessingSurface);\n        }\n        for (Surface extraSurface : extraSurfaces) {\n            if (extraSurface == null) {\n                throw new IllegalArgumentException(\"Should not add a null surface.\");\n            }\n            mRepeatingRequestBuilder.addTarget(extraSurface);\n        }\n    }\n\n    /**\n     * Removes default surfaces from the repeating request builder.\n     */\n    private void removeRepeatingRequestBuilderSurfaces() {\n        mRepeatingRequestBuilder.removeTarget(mPreviewStreamSurface);\n        if (mFrameProcessingSurface != null) {\n            mRepeatingRequestBuilder.removeTarget(mFrameProcessingSurface);\n        }\n    }\n\n    /**\n     * Can be changed to select something different than {@link CameraDevice#TEMPLATE_PREVIEW}\n     * for the default repeating request.\n     * @return the default template for preview\n     */\n    protected int getRepeatingRequestDefaultTemplate() {\n        return CameraDevice.TEMPLATE_PREVIEW;\n    }\n\n    /**\n     * Applies the repeating request builder to the preview, assuming we actually have a preview\n     * running. Can be called after changing parameters to the builder.\n     *\n     * To apply a new builder (for example switch between TEMPLATE_PREVIEW and TEMPLATE_RECORD)\n     * it should be set before calling this method, for example by calling\n     * {@link #createRepeatingRequestBuilder(int)}.\n     */\n    @EngineThread\n    @SuppressWarnings(\"WeakerAccess\")\n    protected void applyRepeatingRequestBuilder() {\n        applyRepeatingRequestBuilder(true, CameraException.REASON_DISCONNECTED);\n    }\n\n    @EngineThread\n    private void applyRepeatingRequestBuilder(boolean checkStarted, int errorReason) {\n        if ((getState() == CameraState.PREVIEW && !isChangingState()) || !checkStarted) {\n            try {\n                mSession.setRepeatingRequest(mRepeatingRequestBuilder.build(),\n                        mRepeatingRequestCallback, null);\n            } catch (CameraAccessException e) {\n                throw new CameraException(e, errorReason);\n            } catch (IllegalStateException e) {\n                // mSession is invalid - has been closed. This is extremely worrying because\n                // it means that the session state and getPreviewState() are not synced.\n                // This probably signals an error in the setup/teardown synchronization.\n                LOG.e(\"applyRepeatingRequestBuilder: session is invalid!\", e,\n                        \"checkStarted:\", checkStarted,\n                        \"currentThread:\", Thread.currentThread().getName(),\n                        \"state:\", getState(),\n                        \"targetState:\", getTargetState());\n                throw new CameraException(CameraException.REASON_DISCONNECTED);\n            }\n        }\n    }\n\n    private final CameraCaptureSession.CaptureCallback mRepeatingRequestCallback\n            = new CameraCaptureSession.CaptureCallback() {\n        @Override\n        public void onCaptureStarted(@NonNull CameraCaptureSession session,\n                                     @NonNull CaptureRequest request,\n                                     long timestamp,\n                                     long frameNumber) {\n            for (Action action : mActions) {\n                action.onCaptureStarted(CLASSTOKEN.this, request);\n            }\n        }\n\n        @Override\n        public void onCaptureProgressed(@NonNull CameraCaptureSession session,\n                                        @NonNull CaptureRequest request,\n                                        @NonNull CaptureResult partialResult) {\n            for (Action action : mActions) {\n                action.onCaptureProgressed(CLASSTOKEN.this, request, partialResult);\n            }\n        }\n\n        @Override\n        public void onCaptureCompleted(@NonNull CameraCaptureSession session,\n                                       @NonNull CaptureRequest request,\n                                       @NonNull TotalCaptureResult result) {\n            mLastRepeatingResult = result;\n            for (Action action : mActions) {\n                action.onCaptureCompleted(CLASSTOKEN.this, request, result);\n            }\n        }\n    };\n\n    //endregion\n\n    //region Protected APIs\n\n    @EngineThread\n    @NonNull\n    @Override\n    protected List<Size> getPreviewStreamAvailableSizes() {\n        try {\n            CameraCharacteristics characteristics = mManager.getCameraCharacteristics(mCameraId);\n            StreamConfigurationMap streamMap =\n                    characteristics.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);\n            if (streamMap == null) {\n                throw new RuntimeException(\"StreamConfigurationMap is null. Should not happen.\");\n            }\n            // This works because our previews return either a SurfaceTexture or a SurfaceHolder,\n            // which are accepted class types by the getOutputSizes method.\n            android.util.Size[] sizes = streamMap.getOutputSizes(mPreview.getOutputClass());\n            List<Size> candidates = new ArrayList<>(sizes.length);\n            for (android.util.Size size : sizes) {\n                Size add = new Size(size.getWidth(), size.getHeight());\n                if (!candidates.contains(add)) candidates.add(add);\n            }\n            return candidates;\n        } catch (CameraAccessException e) {\n            throw createCameraException(e);\n        }\n    }\n\n    @EngineThread\n    @NonNull\n    @Override\n    protected List<Size> getFrameProcessingAvailableSizes() {\n        try {\n            CameraCharacteristics characteristics = mManager.getCameraCharacteristics(mCameraId);\n            StreamConfigurationMap streamMap =\n                    characteristics.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);\n            if (streamMap == null) {\n                throw new RuntimeException(\"StreamConfigurationMap is null. Should not happen.\");\n            }\n            android.util.Size[] sizes = streamMap.getOutputSizes(mFrameProcessingFormat);\n            List<Size> candidates = new ArrayList<>(sizes.length);\n            for (android.util.Size size : sizes) {\n                Size add = new Size(size.getWidth(), size.getHeight());\n                if (!candidates.contains(add)) candidates.add(add);\n            }\n            return candidates;\n        } catch (CameraAccessException e) {\n            throw createCameraException(e);\n        }\n    }\n\n    @EngineThread\n    @Override\n    protected void onPreviewStreamSizeChanged() {\n        LOG.i(\"onPreviewStreamSizeChanged:\", \"Calling restartBind().\");\n        restartBind();\n    }\n\n    @EngineThread\n    @Override\n    protected final boolean collectCameraInfo(@NonNull Facing facing) {\n        int internalFacing = mMapper.mapFacing(facing);\n        String[] cameraIds = null;\n        try {\n            cameraIds = mManager.getCameraIdList();\n        } catch (CameraAccessException e) {\n            // This should never happen, I don't see how it could crash here.\n            // However, let's launch an unrecoverable exception.\n            throw createCameraException(e);\n        }\n        LOG.i(\"collectCameraInfo\", \"Facing:\", facing,\n                \"Internal:\", internalFacing,\n                \"Cameras:\", cameraIds.length);\n        for (String cameraId : cameraIds) {\n            try {\n                CameraCharacteristics characteristics = mManager.getCameraCharacteristics(cameraId);\n                if (internalFacing == readCharacteristic(characteristics,\n                        CameraCharacteristics.LENS_FACING, -99)) {\n                    mCameraId = cameraId;\n                    int sensorOffset = readCharacteristic(characteristics,\n                            CameraCharacteristics.SENSOR_ORIENTATION, 0);\n                    getAngles().setSensorOffset(facing, sensorOffset);\n                    return true;\n                }\n            } catch (CameraAccessException ignore) {\n                // This specific camera has been disconnected.\n                // Keep searching in other camerIds.\n            }\n        }\n        return false;\n    }\n\n    //endregion\n\n    //region Start\n\n    @EngineThread\n    @SuppressLint(\"MissingPermission\")\n    @NonNull\n    @Override\n    protected Task<CameraOptions> onStartEngine() {\n        final TaskCompletionSource<CameraOptions> task = new TaskCompletionSource<>();\n        try {\n            // We have a valid camera for this Facing. Go on.\n            mManager.openCamera(mCameraId, new CameraDevice.StateCallback() {\n                @Override\n                public void onOpened(@NonNull CameraDevice camera) {\n                    mCamera = camera;\n\n                    // Set parameters that might have been set before the camera was opened.\n                    try {\n                        LOG.i(\"onStartEngine:\", \"Opened camera device.\");\n                        mCameraCharacteristics = mManager.getCameraCharacteristics(mCameraId);\n                        boolean flip = getAngles().flip(Reference.SENSOR, Reference.VIEW);\n                        int format;\n                        switch (mPictureFormat) {\n                            case JPEG: format = ImageFormat.JPEG; break;\n                            case DNG: format = ImageFormat.RAW_SENSOR; break;\n                            default: throw new IllegalArgumentException(\"Unknown format:\"\n                                    + mPictureFormat);\n                        }\n                        mCameraOptions = new Camera2Options(mManager, mCameraId, flip, format);\n                        createRepeatingRequestBuilder(getRepeatingRequestDefaultTemplate());\n                    } catch (CameraAccessException e) {\n                        task.trySetException(createCameraException(e));\n                        return;\n                    }\n                    task.trySetResult(mCameraOptions);\n                }\n\n                @Override\n                public void onDisconnected(@NonNull CameraDevice camera) {\n                    // Not sure if this is called INSTEAD of onOpened() or can be called after\n                    // as well. Cover both cases with an unrecoverable exception so that the\n                    // engine is properly destroyed.\n                    CameraException exception\n                            = new CameraException(CameraException.REASON_DISCONNECTED);\n                    if (!task.getTask().isComplete()) {\n                        task.trySetException(exception);\n                    } else {\n                        LOG.i(\"CameraDevice.StateCallback reported disconnection.\");\n                        throw exception;\n                    }\n                }\n\n                @Override\n                public void onError(@NonNull CameraDevice camera, int error) {\n                    if (!task.getTask().isComplete()) {\n                        task.trySetException(createCameraException(error));\n                    } else {\n                        // This happened while the engine is running. Throw unrecoverable exception\n                        // so that engine is properly destroyed.\n                        LOG.e(\"CameraDevice.StateCallback reported an error:\", error);\n                        throw new CameraException(CameraException.REASON_DISCONNECTED);\n                    }\n                }\n            }, null);\n        } catch (CameraAccessException e) {\n            throw createCameraException(e);\n        }\n        return task.getTask();\n    }\n\n    @EngineThread\n    @NonNull\n    @Override\n    protected Task<Void> onStartBind() {\n        LOG.i(\"onStartBind:\", \"Started\");\n        final TaskCompletionSource<Void> task = new TaskCompletionSource<>();\n\n        // Compute sizes.\n        // TODO preview stream should never be bigger than 1920x1080 as per\n        //  CameraDevice.createCaptureSession. This should probably be applied\n        //  before all the other external selectors, to treat it as a hard limit.\n        //  OR: pass an int into these functions to be able to take smaller dims\n        //  when session configuration fails\n        //  OR: both.\n        mCaptureSize = computeCaptureSize();\n        mPreviewStreamSize = computePreviewStreamSize();\n\n        // Deal with surfaces.\n        // In Camera2, instead of applying the size to the camera params object,\n        // we must resize our own surfaces and configure them before opening the session.\n        List<Surface> outputSurfaces = new ArrayList<>();\n\n        // 1. PREVIEW\n        // Create a preview surface with the correct size.\n        final Class outputClass = mPreview.getOutputClass();\n        final Object output = mPreview.getOutput();\n        if (outputClass == SurfaceHolder.class) {\n            try {\n                // This must be called from the UI thread...\n                LOG.i(\"onStartBind:\", \"Waiting on UI thread...\");\n                Tasks.await(Tasks.call(new Callable<Void>() {\n                    @Override\n                    public Void call() {\n                        ((SurfaceHolder) output).setFixedSize(\n                                mPreviewStreamSize.getWidth(),\n                                mPreviewStreamSize.getHeight());\n                        return null;\n                    }\n                }));\n            } catch (ExecutionException | InterruptedException e) {\n                throw new CameraException(e, CameraException.REASON_FAILED_TO_CONNECT);\n            }\n            mPreviewStreamSurface = ((SurfaceHolder) output).getSurface();\n        } else if (outputClass == SurfaceTexture.class) {\n            ((SurfaceTexture) output).setDefaultBufferSize(\n                    mPreviewStreamSize.getWidth(),\n                    mPreviewStreamSize.getHeight());\n            mPreviewStreamSurface = new Surface((SurfaceTexture) output);\n        } else {\n            throw new RuntimeException(\"Unknown CameraPreview output class.\");\n        }\n        outputSurfaces.add(mPreviewStreamSurface);\n\n        // 2. VIDEO RECORDING\n        if (getMode() == Mode.VIDEO) {\n            if (mFullVideoPendingStub != null) {\n                Full2VideoRecorder recorder = new Full2VideoRecorder(this, mCameraId);\n                try {\n                    outputSurfaces.add(recorder.createInputSurface(mFullVideoPendingStub));\n                } catch (Full2VideoRecorder.PrepareException e) {\n                    throw new CameraException(e, CameraException.REASON_FAILED_TO_CONNECT);\n                }\n                mVideoRecorder = recorder;\n            }\n        }\n\n        // 3. PICTURE RECORDING\n        // Format is supported, or it would have thrown in Camera2Options constructor.\n        if (getMode() == Mode.PICTURE) {\n            int format;\n            switch (mPictureFormat) {\n                case JPEG: format = ImageFormat.JPEG; break;\n                case DNG: format = ImageFormat.RAW_SENSOR; break;\n                default: throw new IllegalArgumentException(\"Unknown format:\" + mPictureFormat);\n            }\n            mPictureReader = ImageReader.newInstance(\n                    mCaptureSize.getWidth(),\n                    mCaptureSize.getHeight(),\n                    format, 2);\n            outputSurfaces.add(mPictureReader.getSurface());\n        }\n\n        // 4. FRAME PROCESSING\n        if (hasFrameProcessors()) {\n            mFrameProcessingSize = computeFrameProcessingSize();\n            // Hard to write down why, but in Camera2 we need a number of Frames that's one less\n            // than the number of Images. If we let all Images be part of Frames, thus letting all\n            // Images be used by processor at any given moment, the Camera2 output breaks.\n            // In fact, if there are no Images available, the sensor BLOCKS until it finds one,\n            // which is a big issue because processor times become a bottleneck for the preview.\n            // This is a design flaw in the ImageReader / sensor implementation, as they should\n            // simply DROP frames written to the surface if there are no Images available.\n            // Since this is not how things work, we ensure that one Image is always available here.\n            mFrameProcessingReader = ImageReader.newInstance(\n                    mFrameProcessingSize.getWidth(),\n                    mFrameProcessingSize.getHeight(),\n                    mFrameProcessingFormat,\n                    getFrameProcessingPoolSize() + 1);\n            mFrameProcessingReader.setOnImageAvailableListener(this,\n                    null);\n            mFrameProcessingSurface = mFrameProcessingReader.getSurface();\n            outputSurfaces.add(mFrameProcessingSurface);\n        } else {\n            mFrameProcessingReader = null;\n            mFrameProcessingSize = null;\n            mFrameProcessingSurface = null;\n        }\n\n        try {\n            // null handler means using the current looper which is totally ok.\n            mCamera.createCaptureSession(outputSurfaces, new CameraCaptureSession.StateCallback() {\n                @Override\n                public void onConfigured(@NonNull CameraCaptureSession session) {\n                    mSession = session;\n                    LOG.i(\"onStartBind:\", \"Completed\");\n                    task.trySetResult(null);\n                }\n\n                @Override\n                public void onConfigureFailed(@NonNull CameraCaptureSession session) {\n                    String message = LOG.e(\"onConfigureFailed! Session\", session);\n                    Throwable cause = new RuntimeException(message);\n                    if (!task.getTask().isComplete()) {\n                        task.trySetException(new CameraException(cause,\n                                CameraException.REASON_FAILED_TO_START_PREVIEW));\n                    } else {\n                        // Like onStartEngine.onError\n                        throw new CameraException(CameraException.REASON_DISCONNECTED);\n                    }\n                }\n\n                @Override\n                public void onReady(@NonNull CameraCaptureSession session) {\n                    super.onReady(session);\n                    LOG.i(\"CameraCaptureSession.StateCallback reported onReady.\");\n                }\n            }, null);\n        } catch (CameraAccessException e) {\n            throw createCameraException(e);\n        }\n        return task.getTask();\n    }\n\n    @EngineThread\n    @NonNull\n    @Override\n    protected Task<Void> onStartPreview() {\n        LOG.i(\"onStartPreview:\", \"Dispatching onCameraPreviewStreamSizeChanged.\");\n        getCallback().onCameraPreviewStreamSizeChanged();\n\n        Size previewSizeForView = getPreviewStreamSize(Reference.VIEW);\n        if (previewSizeForView == null) {\n            throw new IllegalStateException(\"previewStreamSize should not be null at this point.\");\n        }\n        mPreview.setStreamSize(previewSizeForView.getWidth(), previewSizeForView.getHeight());\n        mPreview.setDrawRotation(getAngles().offset(Reference.BASE, Reference.VIEW, Axis.ABSOLUTE));\n        if (hasFrameProcessors()) {\n            getFrameManager().setUp(mFrameProcessingFormat, mFrameProcessingSize, getAngles());\n        }\n\n        LOG.i(\"onStartPreview:\", \"Starting preview.\");\n        addRepeatingRequestBuilderSurfaces();\n        applyRepeatingRequestBuilder(false,\n                CameraException.REASON_FAILED_TO_START_PREVIEW);\n        LOG.i(\"onStartPreview:\", \"Started preview.\");\n\n        // Start delayed video if needed.\n        if (mFullVideoPendingStub != null) {\n            // Do not call takeVideo/onTakeVideo. It will reset some stub parameters that\n            // the recorder sets. Also we are posting so that doTakeVideo sees a started preview.\n            final VideoResult.Stub stub = mFullVideoPendingStub;\n            mFullVideoPendingStub = null;\n            getOrchestrator().scheduleStateful(\"do take video\", CameraState.PREVIEW,\n                    new Runnable() {\n                @Override\n                public void run() {\n                    doTakeVideo(stub);\n                }\n            });\n        }\n\n        // Wait for the first frame.\n        final TaskCompletionSource<Void> task = new TaskCompletionSource<>();\n        new BaseAction() {\n            @Override\n            public void onCaptureCompleted(@NonNull ActionHolder holder,\n                                           @NonNull CaptureRequest request,\n                                           @NonNull TotalCaptureResult result) {\n                super.onCaptureCompleted(holder, request, result);\n                setState(STATE_COMPLETED);\n                task.trySetResult(null);\n            }\n        }.start(this);\n        return task.getTask();\n    }\n\n    //endregion\n\n    //region Stop\n\n    @EngineThread\n    @NonNull\n    @Override\n    protected Task<Void> onStopPreview() {\n        LOG.i(\"onStopPreview:\", \"Started.\");\n        if (mVideoRecorder != null) {\n            // This should synchronously call onVideoResult that will reset the repeating builder\n            // to the PREVIEW template. This is very important.\n            mVideoRecorder.stop(true);\n            mVideoRecorder = null;\n        }\n        mPictureRecorder = null;\n        if (hasFrameProcessors()) {\n            getFrameManager().release();\n        }\n        // Removing the part below for now. It hangs on emulators and can take a lot of time\n        // in real devices, for benefits that I'm not 100% sure about.\n        if (false) {\n            try {\n                // Preferring abortCaptures() over stopRepeating(): it makes sure that all\n                // in-flight operations are discarded as fast as possible, which is what we want.\n                // NOTE: this call is asynchronous. Should find a good way to wait for the outcome.\n                LOG.i(\"onStopPreview:\", \"calling abortCaptures().\");\n                mSession.abortCaptures();\n                LOG.i(\"onStopPreview:\", \"called abortCaptures().\");\n            } catch (CameraAccessException e) {\n                // This tells us that we should stop everything. It's better to throw an\n                // unrecoverable exception rather than just swallow, so everything gets stopped.\n                LOG.w(\"onStopPreview:\", \"abortCaptures failed!\", e);\n                throw createCameraException(e);\n            } catch (IllegalStateException e) {\n                // This tells us that the session was already closed.\n                // Not sure if this can happen, but we can swallow it.\n            }\n        }\n        removeRepeatingRequestBuilderSurfaces();\n        mLastRepeatingResult = null;\n        LOG.i(\"onStopPreview:\", \"Returning.\");\n        return Tasks.forResult(null);\n    }\n\n    @EngineThread\n    @NonNull\n    @Override\n    protected Task<Void> onStopBind() {\n        LOG.i(\"onStopBind:\", \"About to clean up.\");\n        mFrameProcessingSurface = null;\n        mPreviewStreamSurface = null;\n        mPreviewStreamSize = null;\n        mCaptureSize = null;\n        mFrameProcessingSize = null;\n        if (mFrameProcessingReader != null) {\n            // WARNING: This call synchronously releases all Images and their underlying\n            // properties. This can cause issues if the Image is being used.\n            mFrameProcessingReader.close();\n            mFrameProcessingReader = null;\n        }\n        if (mPictureReader != null) {\n            mPictureReader.close();\n            mPictureReader = null;\n        }\n        mSession.close();\n        mSession = null;\n        LOG.i(\"onStopBind:\", \"Returning.\");\n        return Tasks.forResult(null);\n    }\n\n    @EngineThread\n    @NonNull\n    @Override\n    protected Task<Void> onStopEngine() {\n        try {\n            LOG.i(\"onStopEngine:\", \"Clean up.\", \"Releasing camera.\");\n            // Just like Camera1Engine, this call can hang (at least on emulators) and if\n            // we don't find a way around the lock, it leaves the camera in a bad state.\n            //\n            // 12:33:28.152  2888  5470 I CameraEngine: onStopEngine: Clean up. Releasing camera.\u001b[0m\n            // 12:33:29.476  1384  1555 E audio_hw_generic: pcm_write failed cannot write stream data: I/O error\u001b[0m\n            // 12:33:33.206  1512  3616 E Camera3-Device: Camera 0: waitUntilDrainedLocked: Error waiting for HAL to drain: Connection timed out (-110)\u001b[0m\n            // 12:33:33.242  1512  3616 E CameraDeviceClient: detachDevice: waitUntilDrained failed with code 0xffffff92\u001b[0m\n            // 12:33:33.243  1512  3616 E Camera3-Device: Camera 0: disconnect: Shutting down in an error state\u001b[0m\n            //\n            // I believe there is a thread deadlock due to this call internally waiting to\n            // dispatch some callback to us (pending captures, ...), but the callback thread\n            // is blocked here. We try to workaround this in CameraEngine.destroy().\n            mCamera.close();\n            LOG.i(\"onStopEngine:\", \"Clean up.\", \"Released camera.\");\n        } catch (Exception e) {\n            LOG.w(\"onStopEngine:\", \"Clean up.\", \"Exception while releasing camera.\", e);\n        }\n        mCamera = null;\n\n        // After engine is stopping, the repeating request builder will be null,\n        // so the ActionHolder.getBuilder() contract would be broken. Same for characteristics.\n        // This can cause crashes if some ongoing Action queries the holder. So we abort them.\n        LOG.i(\"onStopEngine:\", \"Aborting actions.\");\n        for (Action action : mActions) {\n            action.abort(this);\n        }\n\n        mCameraCharacteristics = null;\n        mCameraOptions = null;\n        mVideoRecorder = null;\n        mRepeatingRequestBuilder = null;\n        LOG.w(\"onStopEngine:\", \"Returning.\");\n        return Tasks.forResult(null);\n    }\n\n    //endregion\n\n    //region Pictures\n\n    @EngineThread\n    @Override\n    protected void onTakePictureSnapshot(@NonNull final PictureResult.Stub stub,\n                                         @NonNull final AspectRatio outputRatio,\n                                         boolean doMetering) {\n        if (doMetering) {\n            LOG.i(\"onTakePictureSnapshot:\", \"doMetering is true. Delaying.\");\n            Action action = Actions.timeout(METER_TIMEOUT_SHORT, createMeterAction(null));\n            action.addCallback(new CompletionCallback() {\n                @Override\n                protected void onActionCompleted(@NonNull Action action) {\n                    // This is called on any thread, so be careful.\n                    setPictureSnapshotMetering(false);\n                    takePictureSnapshot(stub);\n                    setPictureSnapshotMetering(true);\n                }\n            });\n            action.start(this);\n        } else {\n            LOG.i(\"onTakePictureSnapshot:\", \"doMetering is false. Performing.\");\n            if (!(mPreview instanceof RendererCameraPreview)) {\n                throw new RuntimeException(\"takePictureSnapshot with Camera2 is only \" +\n                        \"supported with Preview.GL_SURFACE\");\n            }\n            // stub.size is not the real size: it will be cropped to the given ratio\n            // stub.rotation will be set to 0 - we rotate the texture instead.\n            stub.size = getUncroppedSnapshotSize(Reference.OUTPUT);\n            stub.rotation = getAngles().offset(Reference.VIEW, Reference.OUTPUT, Axis.ABSOLUTE);\n            mPictureRecorder = new Snapshot2PictureRecorder(stub, this,\n                    (RendererCameraPreview) mPreview, outputRatio);\n            mPictureRecorder.take();\n        }\n    }\n\n    @EngineThread\n    @Override\n    protected void onTakePicture(@NonNull final PictureResult.Stub stub, boolean doMetering) {\n        if (doMetering) {\n            LOG.i(\"onTakePicture:\", \"doMetering is true. Delaying.\");\n            Action action = Actions.timeout(METER_TIMEOUT_SHORT, createMeterAction(null));\n            action.addCallback(new CompletionCallback() {\n                @Override\n                protected void onActionCompleted(@NonNull Action action) {\n                    // This is called on any thread, so be careful.\n                    setPictureMetering(false);\n                    takePicture(stub);\n                    setPictureMetering(true);\n                }\n            });\n            action.start(this);\n        } else {\n            LOG.i(\"onTakePicture:\", \"doMetering is false. Performing.\");\n            stub.rotation = getAngles().offset(Reference.SENSOR, Reference.OUTPUT,\n                    Axis.RELATIVE_TO_SENSOR);\n            stub.size = getPictureSize(Reference.OUTPUT);\n            try {\n                if (mPictureCaptureStopsPreview) {\n                    // These two are present in official samples and are probably meant to\n                    // speed things up? But from my tests, they actually make everything slower.\n                    // So this is disabled by default with a boolean flag. Maybe in the future\n                    // we can make this configurable as some people might want to stop the preview\n                    // while picture is being taken even if it increases the latency.\n                    mSession.stopRepeating();\n                    mSession.abortCaptures();\n                }\n                CaptureRequest.Builder builder\n                        = mCamera.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);\n                applyAllParameters(builder, mRepeatingRequestBuilder);\n                mPictureRecorder = new Full2PictureRecorder(stub, this, builder,\n                        mPictureReader);\n                mPictureRecorder.take();\n            } catch (CameraAccessException e) {\n                throw createCameraException(e);\n            }\n        }\n    }\n\n    @Override\n    public void onPictureResult(@Nullable PictureResult.Stub result, @Nullable Exception error) {\n        boolean fullPicture = mPictureRecorder instanceof Full2PictureRecorder;\n        super.onPictureResult(result, error);\n        if (fullPicture && mPictureCaptureStopsPreview) {\n            applyRepeatingRequestBuilder();\n        }\n\n        // Some picture recorders might lock metering, and we usually run a metering sequence\n        // before running the recorders. So, run an unlock/reset sequence if needed.\n        boolean unlock = (fullPicture && getPictureMetering())\n                || (!fullPicture && getPictureSnapshotMetering());\n        if (unlock) {\n            getOrchestrator().scheduleStateful(\"reset metering after picture\",\n                    CameraState.PREVIEW,\n                    new Runnable() {\n                @Override\n                public void run() {\n                    unlockAndResetMetering();\n                }\n            });\n        }\n    }\n\n    //endregion\n\n    //region Videos\n\n    @EngineThread\n    @Override\n    protected void onTakeVideo(@NonNull VideoResult.Stub stub) {\n        LOG.i(\"onTakeVideo\", \"called.\");\n        stub.rotation = getAngles().offset(Reference.SENSOR, Reference.OUTPUT,\n                Axis.RELATIVE_TO_SENSOR);\n        stub.size = getAngles().flip(Reference.SENSOR, Reference.OUTPUT) ?\n                mCaptureSize.flip() : mCaptureSize;\n        // We must restart the session at each time.\n        // Save the pending data and restart the session.\n        LOG.w(\"onTakeVideo\", \"calling restartBind.\");\n        mFullVideoPendingStub = stub;\n        restartBind();\n    }\n\n    private void doTakeVideo(@NonNull final VideoResult.Stub stub) {\n        if (!(mVideoRecorder instanceof Full2VideoRecorder)) {\n            throw new IllegalStateException(\"doTakeVideo called, but video recorder \" +\n                    \"is not a Full2VideoRecorder! \" + mVideoRecorder);\n        }\n        Full2VideoRecorder recorder = (Full2VideoRecorder) mVideoRecorder;\n        try {\n            createRepeatingRequestBuilder(CameraDevice.TEMPLATE_RECORD);\n            addRepeatingRequestBuilderSurfaces(recorder.getInputSurface());\n            applyRepeatingRequestBuilder(true, CameraException.REASON_DISCONNECTED);\n            mVideoRecorder.start(stub);\n        } catch (CameraAccessException e) {\n            onVideoResult(null, e);\n            throw createCameraException(e);\n        } catch (CameraException e) {\n            onVideoResult(null, e);\n            throw e;\n        }\n    }\n\n    @EngineThread\n    @Override\n    protected void onTakeVideoSnapshot(@NonNull VideoResult.Stub stub,\n                                       @NonNull AspectRatio outputRatio) {\n        if (!(mPreview instanceof RendererCameraPreview)) {\n            throw new IllegalStateException(\"Video snapshots are only supported with GL_SURFACE.\");\n        }\n        RendererCameraPreview glPreview = (RendererCameraPreview) mPreview;\n        Size outputSize = getUncroppedSnapshotSize(Reference.OUTPUT);\n        if (outputSize == null) {\n            throw new IllegalStateException(\"outputSize should not be null.\");\n        }\n        Rect outputCrop = CropHelper.computeCrop(outputSize, outputRatio);\n        outputSize = new Size(outputCrop.width(), outputCrop.height());\n        stub.size = outputSize;\n        stub.rotation = getAngles().offset(Reference.VIEW, Reference.OUTPUT, Axis.ABSOLUTE);\n        stub.videoFrameRate = Math.round(mPreviewFrameRate);\n        LOG.i(\"onTakeVideoSnapshot\", \"rotation:\", stub.rotation, \"size:\", stub.size);\n        mVideoRecorder = new SnapshotVideoRecorder(this, glPreview, getOverlay());\n        mVideoRecorder.start(stub);\n    }\n\n    /**\n     * When video ends we must stop the recorder and remove the recorder surface from\n     * camera outputs. This is done in onVideoResult. However, on some devices, order matters.\n     * If we stop the recorder and AFTER send camera frames to it, the camera will try to fill\n     * the recorder \"abandoned\" Surface and on some devices with a poor internal implementation\n     * (HW_LEVEL_LEGACY) this crashes. So if the conditions are met, we restore here. Issue #549.\n     */\n    @Override\n    public void onVideoRecordingEnd() {\n        super.onVideoRecordingEnd();\n        // SnapshotRecorder will invoke this on its own thread which is risky, but if it was a\n        // snapshot, this function does nothing so it's safe.\n        boolean needsIssue549Workaround = (mVideoRecorder instanceof Full2VideoRecorder) &&\n                (readCharacteristic(CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL, -1)\n                        == CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY);\n        if (needsIssue549Workaround) {\n            LOG.w(\"Applying the Issue549 workaround.\", Thread.currentThread());\n            maybeRestorePreviewTemplateAfterVideo();\n            LOG.w(\"Applied the Issue549 workaround. Sleeping...\");\n            try { Thread.sleep(600); } catch (InterruptedException ignore) {}\n            LOG.w(\"Applied the Issue549 workaround. Slept!\");\n        }\n    }\n\n    @Override\n    public void onVideoResult(@Nullable VideoResult.Stub result, @Nullable Exception exception) {\n        super.onVideoResult(result, exception);\n        // SnapshotRecorder will invoke this on its own thread, so let's post in our own thread\n        // and check camera state before trying to restore the preview. Engine might have been\n        // torn down in the engine thread while this was still being called.\n        getOrchestrator().scheduleStateful(\"restore preview template\", CameraState.BIND,\n                new Runnable() {\n            @Override\n            public void run() {\n                maybeRestorePreviewTemplateAfterVideo();\n            }\n        });\n    }\n\n    /**\n     * Video recorders might change the camera template to {@link CameraDevice#TEMPLATE_RECORD}.\n     * After the video is taken, we should restore the template preview, which also means that\n     * we'll remove any extra surface target that was added by the video recorder.\n     *\n     * This method avoids doing this twice by checking the request tag, as set by\n     * the {@link #createRepeatingRequestBuilder(int)} method.\n     */\n    @EngineThread\n    private void maybeRestorePreviewTemplateAfterVideo() {\n        int template = (int) mRepeatingRequestBuilder.build().getTag();\n        if (template != getRepeatingRequestDefaultTemplate()) {\n            try {\n                createRepeatingRequestBuilder(getRepeatingRequestDefaultTemplate());\n                addRepeatingRequestBuilderSurfaces();\n                applyRepeatingRequestBuilder();\n            } catch (CameraAccessException e) {\n                throw createCameraException(e);\n            }\n        }\n    }\n\n    //endregion\n\n    //region Parameters\n\n    private void applyAllParameters(@NonNull CaptureRequest.Builder builder,\n                                    @Nullable CaptureRequest.Builder oldBuilder) {\n        LOG.i(\"applyAllParameters:\", \"called for tag\", builder.build().getTag());\n        builder.set(CaptureRequest.CONTROL_MODE, CaptureRequest.CONTROL_MODE_AUTO);\n        applyDefaultFocus(builder);\n        applyFlash(builder, Flash.OFF);\n        applyLocation(builder, null);\n        applyWhiteBalance(builder, WhiteBalance.AUTO);\n        applyHdr(builder, Hdr.OFF);\n        applyZoom(builder, 0F);\n        applyExposureCorrection(builder, 0F);\n        applyPreviewFrameRate(builder, 0F);\n\n        if (oldBuilder != null) {\n            // We might be in a metering operation, or the old builder might have some special\n            // metering parameters. Copy these special keys over to the new builder.\n            // These are the keys changed by metering.Parameters, or by us in applyFocusForMetering.\n            builder.set(CaptureRequest.CONTROL_AF_REGIONS,\n                    oldBuilder.get(CaptureRequest.CONTROL_AF_REGIONS));\n            builder.set(CaptureRequest.CONTROL_AE_REGIONS,\n                    oldBuilder.get(CaptureRequest.CONTROL_AE_REGIONS));\n            builder.set(CaptureRequest.CONTROL_AWB_REGIONS,\n                    oldBuilder.get(CaptureRequest.CONTROL_AWB_REGIONS));\n            builder.set(CaptureRequest.CONTROL_AF_MODE,\n                    oldBuilder.get(CaptureRequest.CONTROL_AF_MODE));\n            // Do NOT copy exposure or focus triggers!\n        }\n    }\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected void applyDefaultFocus(@NonNull CaptureRequest.Builder builder) {\n        int[] modesArray = readCharacteristic(CameraCharacteristics.CONTROL_AF_AVAILABLE_MODES,\n                new int[]{});\n        List<Integer> modes = new ArrayList<>();\n        for (int mode : modesArray) { modes.add(mode); }\n        if (getMode() == Mode.VIDEO &&\n                modes.contains(CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_VIDEO)) {\n            builder.set(CaptureRequest.CONTROL_AF_MODE,\n                    CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_VIDEO);\n            return;\n        }\n\n        if (modes.contains(CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_PICTURE)) {\n            builder.set(CaptureRequest.CONTROL_AF_MODE,\n                    CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_PICTURE);\n            return;\n        }\n\n        if (modes.contains(CaptureRequest.CONTROL_AF_MODE_AUTO)) {\n            builder.set(CaptureRequest.CONTROL_AF_MODE, CaptureRequest.CONTROL_AF_MODE_AUTO);\n            return;\n        }\n\n        if (modes.contains(CaptureRequest.CONTROL_AF_MODE_OFF)) {\n            builder.set(CaptureRequest.CONTROL_AF_MODE, CaptureRequest.CONTROL_AF_MODE_OFF);\n            builder.set(CaptureRequest.LENS_FOCUS_DISTANCE, 0F);\n            //noinspection UnnecessaryReturnStatement\n            return;\n        }\n    }\n\n    /**\n     * All focus modes support the AF trigger, except OFF and EDOF.\n     * However, unlike the preview, we'd prefer AUTO to any CONTINUOUS value.\n     * An AUTO value means that focus is locked unless we run the focus trigger,\n     * which is what metering does.\n     *\n     * @param builder builder\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    protected void applyFocusForMetering(@NonNull CaptureRequest.Builder builder) {\n        int[] modesArray = readCharacteristic(CameraCharacteristics.CONTROL_AF_AVAILABLE_MODES,\n                new int[]{});\n        List<Integer> modes = new ArrayList<>();\n        for (int mode : modesArray) { modes.add(mode); }\n        if (modes.contains(CaptureRequest.CONTROL_AF_MODE_AUTO)) {\n            builder.set(CaptureRequest.CONTROL_AF_MODE, CaptureRequest.CONTROL_AF_MODE_AUTO);\n            return;\n        }\n        if (getMode() == Mode.VIDEO &&\n                modes.contains(CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_VIDEO)) {\n            builder.set(CaptureRequest.CONTROL_AF_MODE,\n                    CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_VIDEO);\n            return;\n        }\n\n        if (modes.contains(CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_PICTURE)) {\n            builder.set(CaptureRequest.CONTROL_AF_MODE,\n                    CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_PICTURE);\n            //noinspection UnnecessaryReturnStatement\n            return;\n        }\n    }\n\n    @Override\n    public void setFlash(@NonNull final Flash flash) {\n        final Flash old = mFlash;\n        mFlash = flash;\n        mFlashTask = getOrchestrator().scheduleStateful(\"flash (\" + flash + \")\",\n                CameraState.ENGINE,\n                new Runnable() {\n            @Override\n            public void run() {\n                boolean shouldApply = applyFlash(mRepeatingRequestBuilder, old);\n                boolean needsWorkaround = getState() == CameraState.PREVIEW;\n                if (needsWorkaround) {\n                    // Runtime changes to the flash value are not correctly handled by the\n                    // driver. See https://stackoverflow.com/q/53003383/4288782 for example.\n                    // For this reason, we go back to OFF, capture once, then go to the new one.\n                    mFlash = Flash.OFF;\n                    applyFlash(mRepeatingRequestBuilder, old);\n                    try {\n                        mSession.capture(mRepeatingRequestBuilder.build(), null,\n                                null);\n                    } catch (CameraAccessException e) {\n                        throw createCameraException(e);\n                    }\n                    mFlash = flash;\n                    applyFlash(mRepeatingRequestBuilder, old);\n                    applyRepeatingRequestBuilder();\n\n                } else if (shouldApply) {\n                    applyRepeatingRequestBuilder();\n                }\n            }\n        });\n    }\n\n    /**\n     * This sets the CONTROL_AE_MODE to either:\n     * - {@link CaptureRequest#CONTROL_AE_MODE_ON}\n     * - {@link CaptureRequest#CONTROL_AE_MODE_ON_AUTO_FLASH}\n     * - {@link CaptureRequest#CONTROL_AE_MODE_ON_ALWAYS_FLASH}\n     *\n     * The API offers a high level control through {@link CaptureRequest#CONTROL_AE_MODE},\n     * which is what the mapper looks at. It will trigger (if specified) flash only for\n     * still captures which is exactly what we want.\n     *\n     * However, we set CONTROL_AE_MODE to ON/OFF (depending\n     * on which is available) with both {@link Flash#OFF} and {@link Flash#TORCH}.\n     *\n     * When CONTROL_AE_MODE is ON or OFF, the low level control, called\n     * {@link CaptureRequest#FLASH_MODE}, becomes effective, and that's where we can actually\n     * distinguish between a turned off flash and a torch flash.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    protected boolean applyFlash(@NonNull CaptureRequest.Builder builder,\n                                 @NonNull Flash oldFlash) {\n        if (mCameraOptions.supports(mFlash)) {\n            int[] availableAeModesArray = readCharacteristic(\n                    CameraCharacteristics.CONTROL_AE_AVAILABLE_MODES, new int[]{});\n            List<Integer> availableAeModes = new ArrayList<>();\n            for (int mode : availableAeModesArray) { availableAeModes.add(mode); }\n\n            List<Pair<Integer, Integer>> pairs = mMapper.mapFlash(mFlash);\n            for (Pair<Integer, Integer> pair : pairs) {\n                if (availableAeModes.contains(pair.first)) {\n                    LOG.i(\"applyFlash: setting CONTROL_AE_MODE to\", pair.first);\n                    LOG.i(\"applyFlash: setting FLASH_MODE to\", pair.second);\n                    builder.set(CaptureRequest.CONTROL_AE_MODE, pair.first);\n                    builder.set(CaptureRequest.FLASH_MODE, pair.second);\n                    return true;\n                }\n            }\n        }\n        mFlash = oldFlash;\n        return false;\n    }\n\n    @Override\n    public void setLocation(@Nullable Location location) {\n        final Location old = mLocation;\n        mLocation = location;\n        mLocationTask = getOrchestrator().scheduleStateful(\"location\",\n                CameraState.ENGINE,\n                new Runnable() {\n            @Override\n            public void run() {\n                if (applyLocation(mRepeatingRequestBuilder, old)) {\n                    applyRepeatingRequestBuilder();\n                }\n            }\n        });\n    }\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected boolean applyLocation(@NonNull CaptureRequest.Builder builder,\n                                    @SuppressWarnings(\"unused\") @Nullable Location oldLocation) {\n        if (mLocation != null) {\n            builder.set(CaptureRequest.JPEG_GPS_LOCATION, mLocation);\n        }\n        return true;\n    }\n\n    @Override\n    public void setWhiteBalance(@NonNull WhiteBalance whiteBalance) {\n        final WhiteBalance old = mWhiteBalance;\n        mWhiteBalance = whiteBalance;\n        mWhiteBalanceTask = getOrchestrator().scheduleStateful(\n                \"white balance (\" + whiteBalance + \")\",\n                CameraState.ENGINE,\n                new Runnable() {\n            @Override\n            public void run() {\n                if (applyWhiteBalance(mRepeatingRequestBuilder, old)) {\n                    applyRepeatingRequestBuilder();\n                }\n            }\n        });\n    }\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected boolean applyWhiteBalance(@NonNull CaptureRequest.Builder builder,\n                                        @NonNull WhiteBalance oldWhiteBalance) {\n        if (mCameraOptions.supports(mWhiteBalance)) {\n            int whiteBalance = mMapper.mapWhiteBalance(mWhiteBalance);\n            builder.set(CaptureRequest.CONTROL_AWB_MODE, whiteBalance);\n            return true;\n        }\n        mWhiteBalance = oldWhiteBalance;\n        return false;\n    }\n\n    @Override\n    public void setHdr(@NonNull Hdr hdr) {\n        final Hdr old = mHdr;\n        mHdr = hdr;\n        mHdrTask = getOrchestrator().scheduleStateful(\"hdr (\" + hdr + \")\",\n                CameraState.ENGINE,\n                new Runnable() {\n            @Override\n            public void run() {\n                if (applyHdr(mRepeatingRequestBuilder, old)) {\n                    applyRepeatingRequestBuilder();\n                }\n            }\n        });\n    }\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected boolean applyHdr(@NonNull CaptureRequest.Builder builder, @NonNull Hdr oldHdr) {\n        if (mCameraOptions.supports(mHdr)) {\n            int hdr = mMapper.mapHdr(mHdr);\n            builder.set(CaptureRequest.CONTROL_SCENE_MODE, hdr);\n            return true;\n        }\n        mHdr = oldHdr;\n        return false;\n    }\n\n    @Override\n    public void setZoom(final float zoom, final @Nullable PointF[] points, final boolean notify) {\n        final float old = mZoomValue;\n        mZoomValue = zoom;\n        // Zoom requests can be high frequency (e.g. linked to touch events), let's trim the oldest.\n        getOrchestrator().trim(\"zoom\", ALLOWED_ZOOM_OPS);\n        mZoomTask = getOrchestrator().scheduleStateful(\n                \"zoom\",\n                CameraState.ENGINE,\n                new Runnable() {\n            @Override\n            public void run() {\n                if (applyZoom(mRepeatingRequestBuilder, old)) {\n                    applyRepeatingRequestBuilder();\n                    if (notify) {\n                        getCallback().dispatchOnZoomChanged(zoom, points);\n                    }\n                }\n            }\n        });\n    }\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected boolean applyZoom(@NonNull CaptureRequest.Builder builder, float oldZoom) {\n        if (mCameraOptions.isZoomSupported()) {\n            float maxZoom = readCharacteristic(\n                    CameraCharacteristics.SCALER_AVAILABLE_MAX_DIGITAL_ZOOM, 1F);\n            // converting 0.0f-1.0f zoom scale to the actual camera digital zoom scale\n            // (which will be for example, 1.0-10.0)\n            float calculatedZoom = (mZoomValue * (maxZoom - 1.0f)) + 1.0f;\n            Rect newRect = getZoomRect(calculatedZoom, maxZoom);\n            builder.set(CaptureRequest.SCALER_CROP_REGION, newRect);\n            return true;\n        }\n        mZoomValue = oldZoom;\n        return false;\n    }\n\n    @NonNull\n    private Rect getZoomRect(float zoomLevel, float maxDigitalZoom) {\n        Rect activeRect = readCharacteristic(CameraCharacteristics.SENSOR_INFO_ACTIVE_ARRAY_SIZE,\n                new Rect());\n        int minW = (int) (activeRect.width() / maxDigitalZoom);\n        int minH = (int) (activeRect.height() / maxDigitalZoom);\n        int difW = activeRect.width() - minW;\n        int difH = activeRect.height() - minH;\n\n        // When zoom is 1, we want to return new Rect(0, 0, width, height).\n        // When zoom is maxZoom, we want to return a centered rect with minW and minH\n        int cropW = (int) (difW * (zoomLevel - 1) / (maxDigitalZoom - 1) / 2F);\n        int cropH = (int) (difH * (zoomLevel - 1) / (maxDigitalZoom - 1) / 2F);\n        return new Rect(cropW, cropH, activeRect.width() - cropW,\n                activeRect.height() - cropH);\n    }\n\n    @Override\n    public void setExposureCorrection(final float EVvalue,\n                                      @NonNull final float[] bounds,\n                                      @Nullable final PointF[] points,\n                                      final boolean notify) {\n        final float old = mExposureCorrectionValue;\n        mExposureCorrectionValue = EVvalue;\n        // EV requests can be high frequency (e.g. linked to touch events), let's trim the oldest.\n        getOrchestrator().trim(\"exposure correction\", ALLOWED_EV_OPS);\n        mExposureCorrectionTask = getOrchestrator().scheduleStateful(\n                \"exposure correction\",\n                CameraState.ENGINE,\n                new Runnable() {\n            @Override\n            public void run() {\n                if (applyExposureCorrection(mRepeatingRequestBuilder, old)) {\n                    applyRepeatingRequestBuilder();\n                    if (notify) {\n                        getCallback().dispatchOnExposureCorrectionChanged(EVvalue, bounds, points);\n                    }\n                }\n            }\n        });\n    }\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected boolean applyExposureCorrection(@NonNull CaptureRequest.Builder builder,\n                                              float oldEVvalue) {\n        if (mCameraOptions.isExposureCorrectionSupported()) {\n            Rational exposureCorrectionStep = readCharacteristic(\n                    CameraCharacteristics.CONTROL_AE_COMPENSATION_STEP,\n                    new Rational(1, 1));\n            int exposureCorrectionSteps = Math.round(mExposureCorrectionValue\n                    * exposureCorrectionStep.floatValue());\n            builder.set(CaptureRequest.CONTROL_AE_EXPOSURE_COMPENSATION, exposureCorrectionSteps);\n            return true;\n        }\n        mExposureCorrectionValue = oldEVvalue;\n        return false;\n    }\n\n    @Override\n    public void setPlaySounds(boolean playSounds) {\n        mPlaySounds = playSounds;\n        mPlaySoundsTask = Tasks.forResult(null);\n    }\n\n    @Override\n    public void setPreviewFrameRate(float previewFrameRate) {\n        final float oldPreviewFrameRate = mPreviewFrameRate;\n        mPreviewFrameRate = previewFrameRate;\n        mPreviewFrameRateTask = getOrchestrator().scheduleStateful(\n                \"preview fps (\" + previewFrameRate + \")\",\n                CameraState.ENGINE,\n                new Runnable() {\n            @Override\n            public void run() {\n                if (applyPreviewFrameRate(mRepeatingRequestBuilder, oldPreviewFrameRate)) {\n                    applyRepeatingRequestBuilder();\n                }\n            }\n        });\n    }\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected boolean applyPreviewFrameRate(@NonNull CaptureRequest.Builder builder,\n                                            float oldPreviewFrameRate) {\n        //noinspection unchecked\n        Range<Integer>[] fpsRanges = readCharacteristic(\n                CameraCharacteristics.CONTROL_AE_AVAILABLE_TARGET_FPS_RANGES,\n                new Range[]{});\n        sortFrameRateRanges(fpsRanges);\n        if (mPreviewFrameRate == 0F) {\n            // 0F is a special value. Fallback to a reasonable default.\n            for (Range<Integer> fpsRange : filterFrameRateRanges(fpsRanges)) {\n                if (fpsRange.contains(30) || fpsRange.contains(24)) {\n                    builder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, fpsRange);\n                    return true;\n                }\n            }\n        } else {\n            // If out of boundaries, adjust it.\n            mPreviewFrameRate = Math.min(mPreviewFrameRate,\n                    mCameraOptions.getPreviewFrameRateMaxValue());\n            mPreviewFrameRate = Math.max(mPreviewFrameRate,\n                    mCameraOptions.getPreviewFrameRateMinValue());\n            for (Range<Integer> fpsRange : filterFrameRateRanges(fpsRanges)) {\n                if (fpsRange.contains(Math.round(mPreviewFrameRate))) {\n                    builder.set(CaptureRequest.CONTROL_AE_TARGET_FPS_RANGE, fpsRange);\n                    return true;\n                }\n            }\n        }\n        mPreviewFrameRate = oldPreviewFrameRate;\n        return false;\n    }\n\n    private void sortFrameRateRanges(@NonNull Range<Integer>[] fpsRanges) {\n        final boolean ascending = getPreviewFrameRateExact() && mPreviewFrameRate != 0;\n        Arrays.sort(fpsRanges, new Comparator<Range<Integer>>() {\n            @Override\n            public int compare(Range<Integer> range1, Range<Integer> range2) {\n                if (ascending) {\n                    return (range1.getUpper() - range1.getLower())\n                            - (range2.getUpper() - range2.getLower());\n                } else {\n                    return (range2.getUpper() - range2.getLower())\n                            - (range1.getUpper() - range1.getLower());\n                }\n            }\n        });\n    }\n\n    @NonNull\n    protected List<Range<Integer>> filterFrameRateRanges(@NonNull Range<Integer>[] fpsRanges) {\n        List<Range<Integer>> results = new ArrayList<>();\n        int min = Math.round(mCameraOptions.getPreviewFrameRateMinValue());\n        int max = Math.round(mCameraOptions.getPreviewFrameRateMaxValue());\n        for (Range<Integer> fpsRange : fpsRanges) {\n            if (!fpsRange.contains(min) && !fpsRange.contains(max)) continue;\n            if (!FpsRangeValidator.validate(fpsRange)) continue;\n            results.add(fpsRange);\n        }\n        return results;\n    }\n\n    @Override\n    public void setPictureFormat(final @NonNull PictureFormat pictureFormat) {\n        if (pictureFormat != mPictureFormat) {\n            mPictureFormat = pictureFormat;\n            getOrchestrator().scheduleStateful(\"picture format (\" + pictureFormat + \")\",\n                    CameraState.ENGINE,\n                    new Runnable() {\n                @Override\n                public void run() {\n                    restart();\n                }\n            });\n        }\n    }\n\n    //endregion\n\n    //region Frame Processing\n\n    @NonNull\n    @Override\n    protected FrameManager instantiateFrameManager(int poolSize) {\n        return new ImageFrameManager(poolSize);\n    }\n\n    @EngineThread\n    @Override\n    public void onImageAvailable(ImageReader reader) {\n        LOG.v(\"onImageAvailable:\", \"trying to acquire Image.\");\n        Image image = null;\n        try {\n            image = reader.acquireLatestImage();\n        } catch (Exception ignore) { }\n        if (image == null) {\n            LOG.w(\"onImageAvailable:\", \"failed to acquire Image!\");\n        } else if (getState() == CameraState.PREVIEW && !isChangingState()) {\n            // After preview, the frame manager is correctly set up\n            //noinspection unchecked\n            Frame frame = getFrameManager().getFrame(image,\n                    System.currentTimeMillis());\n            if (frame != null) {\n                LOG.v(\"onImageAvailable:\", \"Image acquired, dispatching.\");\n                getCallback().dispatchFrame(frame);\n            } else {\n                LOG.i(\"onImageAvailable:\", \"Image acquired, but no free frames. DROPPING.\");\n            }\n        } else {\n            LOG.i(\"onImageAvailable:\", \"Image acquired in wrong state. Closing it now.\");\n            image.close();\n        }\n    }\n\n    @Override\n    public void setHasFrameProcessors(final boolean hasFrameProcessors) {\n        // Frame processing is set up partially when binding and partially when starting\n        // the preview. If the value is changed between the two, the preview step can crash.\n        getOrchestrator().schedule(\"has frame processors (\" + hasFrameProcessors + \")\",\n                true, new Runnable() {\n            @Override\n            public void run() {\n                if (getState().isAtLeast(CameraState.BIND) && isChangingState()) {\n                    // Extremely rare case in which this was called in between startBind and\n                    // startPreview. This can cause issues. Try later.\n                    setHasFrameProcessors(hasFrameProcessors);\n                    return;\n                }\n                // Apply and restart.\n                mHasFrameProcessors = hasFrameProcessors;\n                if (getState().isAtLeast(CameraState.BIND)) {\n                    restartBind();\n                }\n            }\n        });\n    }\n\n    @Override\n    public void setFrameProcessingFormat(final int format) {\n        // This is called during initialization. Set our default first.\n        if (mFrameProcessingFormat == 0) mFrameProcessingFormat = FRAME_PROCESSING_FORMAT;\n        // Frame processing format is used both when binding and when starting the preview.\n        // If the value is changed between the two, the preview step can crash.\n        getOrchestrator().schedule(\"frame processing format (\" + format + \")\",\n                true, new Runnable() {\n            @Override\n            public void run() {\n                if (getState().isAtLeast(CameraState.BIND) && isChangingState()) {\n                    // Extremely rare case in which this was called in between startBind and\n                    // startPreview. This can cause issues. Try later.\n                    setFrameProcessingFormat(format);\n                    return;\n                }\n                mFrameProcessingFormat = format > 0 ? format : FRAME_PROCESSING_FORMAT;\n                if (getState().isAtLeast(CameraState.BIND)) {\n                    restartBind();\n                }\n            }\n        });\n    }\n\n    //endregion\n\n    //region 3A Metering\n\n    @Override\n    public void startAutoFocus(@Nullable final Gesture gesture,\n                               @NonNull final MeteringRegions regions,\n                               @NonNull final PointF legacyPoint) {\n        // This will only work when we have a preview, since it launches the preview\n        // in the end. Even without this it would need the bind state at least,\n        // since we need the preview size.\n        getOrchestrator().scheduleStateful(\"autofocus (\" + gesture + \")\",\n                CameraState.PREVIEW,\n                new Runnable() {\n            @Override\n            public void run() {\n                // The camera options API still has the auto focus API but it really\n                // refers to \"3A metering to a specific point\". Since we have a point, check.\n                if (!mCameraOptions.isAutoFocusSupported()) return;\n\n                // Create the meter and start.\n                getCallback().dispatchOnFocusStart(gesture, legacyPoint);\n                final MeterAction action = createMeterAction(regions);\n                Action wrapper = Actions.timeout(METER_TIMEOUT, action);\n                wrapper.start(CLASSTOKEN.this);\n                wrapper.addCallback(new CompletionCallback() {\n                    @Override\n                    protected void onActionCompleted(@NonNull Action a) {\n                        getCallback().dispatchOnFocusEnd(gesture,\n                                action.isSuccessful(), legacyPoint);\n                        getOrchestrator().remove(\"reset metering\");\n                        if (shouldResetAutoFocus()) {\n                            getOrchestrator().scheduleStatefulDelayed(\"reset metering\",\n                                    CameraState.PREVIEW,\n                                    getAutoFocusResetDelay(),\n                                    new Runnable() {\n                                @Override\n                                public void run() {\n                                    unlockAndResetMetering();\n                                }\n                            });\n                        }\n                    }\n                });\n            }\n        });\n    }\n\n    @NonNull\n    private MeterAction createMeterAction(@Nullable MeteringRegions regions) {\n        // Before creating any new meter action, abort the old one.\n        if (mMeterAction != null) mMeterAction.abort(this);\n        // The meter will check the current configuration to see if AF/AE/AWB should run.\n        // - AE should be on CONTROL_AE_MODE_ON*    (this depends on setFlash())\n        // - AWB should be on CONTROL_AWB_MODE_AUTO (this depends on setWhiteBalance())\n        // - AF should be on CONTROL_AF_MODE_AUTO or others\n        // The last one is under our control because the library has no focus API.\n        // So let's set a good af mode here. This operation is reverted during onMeteringReset().\n        applyFocusForMetering(mRepeatingRequestBuilder);\n        mMeterAction = new MeterAction(CLASSTOKEN.this, regions, regions == null);\n        return mMeterAction;\n    }\n\n    @EngineThread\n    private void unlockAndResetMetering() {\n        // Needs the PREVIEW state!\n        Actions.sequence(\n                new BaseAction() {\n                    @Override\n                    protected void onStart(@NonNull ActionHolder holder) {\n                        super.onStart(holder);\n                        applyDefaultFocus(holder.getBuilder(this));\n                        holder.getBuilder(this)\n                                .set(CaptureRequest.CONTROL_AE_LOCK, false);\n                        holder.getBuilder(this)\n                                .set(CaptureRequest.CONTROL_AWB_LOCK, false);\n                        holder.applyBuilder(this);\n                        setState(STATE_COMPLETED);\n                        // TODO should wait results?\n                    }\n                },\n                new MeterResetAction()\n        ).start(CLASSTOKEN.this);\n    }\n\n    //endregion\n\n    //region Actions\n\n    @Override\n    public void addAction(final @NonNull Action action) {\n        if (!mActions.contains(action)) {\n            mActions.add(action);\n        }\n    }\n\n    @Override\n    public void removeAction(final @NonNull Action action) {\n        mActions.remove(action);\n    }\n\n    @NonNull\n    @Override\n    public CameraCharacteristics getCharacteristics(@NonNull Action action) {\n        return mCameraCharacteristics;\n    }\n\n    @Nullable\n    @Override\n    public TotalCaptureResult getLastResult(@NonNull Action action) {\n        return mLastRepeatingResult;\n    }\n\n    @NonNull\n    @Override\n    public CaptureRequest.Builder getBuilder(@NonNull Action action) {\n        return mRepeatingRequestBuilder;\n    }\n\n    @EngineThread\n    @Override\n    public void applyBuilder(@NonNull Action source) {\n        // NOTE: Should never be called on a non-engine thread!\n        // Non-engine threads are not protected by the uncaught exception handler\n        // and can make the process crash.\n        applyRepeatingRequestBuilder();\n    }\n\n    @Override\n    public void applyBuilder(@NonNull Action source, @NonNull CaptureRequest.Builder builder)\n            throws CameraAccessException {\n        // Risky - would be better to ensure that thread is the engine one.\n        if (getState() == CameraState.PREVIEW && !isChangingState()) {\n            mSession.capture(builder.build(), mRepeatingRequestCallback, null);\n        }\n    }\n\n    //endregion\n", "target": "camera 2 engine"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/internal/WorkerHandler.java:WorkerHandler:0", "source": "\n\n    private final static CameraLogger LOG\n            = CameraLogger.create(CLASSTOKEN.class.getSimpleName());\n    private final static ConcurrentHashMap<String, WeakReference<CLASSTOKEN>> sCache\n            = new ConcurrentHashMap<>(4);\n\n    private final static String FALLBACK_NAME = \"FallbackCameraThread\";\n\n    // Store a hard reference to the fallback handler. We never use this, only update it\n    // anytime get() is called. This should ensure that this instance is not collected.\n    @SuppressWarnings(\"FieldCanBeLocal\")\n    private static CLASSTOKEN sFallbackHandler;\n\n    /**\n     * Gets a possibly cached handler with the given name.\n     * @param name the handler name\n     * @return a handler\n     */\n    @NonNull\n    public static CLASSTOKEN get(@NonNull String name) {\n        if (sCache.containsKey(name)) {\n            //noinspection ConstantConditions\n            CLASSTOKEN cached = sCache.get(name).get();\n            if (cached != null) {\n                if (cached.getThread().isAlive() && !cached.getThread().isInterrupted()) {\n                    LOG.w(\"get:\", \"Reusing cached worker handler.\", name);\n                    return cached;\n                } else {\n                    // Cleanup the old thread before creating a new one\n                    cached.destroy();\n                    LOG.w(\"get:\", \"Thread reference found, but not alive or interrupted.\",\n                            \"Removing.\", name);\n                    sCache.remove(name);\n                }\n            } else {\n                LOG.w(\"get:\", \"Thread reference died. Removing.\", name);\n                sCache.remove(name);\n            }\n        }\n\n        LOG.i(\"get:\", \"Creating new handler.\", name);\n        CLASSTOKEN handler = new CLASSTOKEN(name);\n        sCache.put(name, new WeakReference<>(handler));\n        return handler;\n    }\n\n    /**\n     * Returns a fallback CLASSTOKEN.\n     * @return a fallback handler\n     */\n    @NonNull\n    public static CLASSTOKEN get() {\n        sFallbackHandler = get(FALLBACK_NAME);\n        return sFallbackHandler;\n    }\n\n    /**\n     * Handy utility to perform an action in a fallback thread.\n     * Not to be used for long-running operations since they will block\n     * the fallback thread.\n     *\n     * @param action the action\n     */\n    public static void execute(@NonNull Runnable action) {\n        get().post(action);\n    }\n\n    private String mName;\n    private HandlerThread mThread;\n    private Handler mHandler;\n    private Executor mExecutor;\n\n    private CLASSTOKEN(@NonNull String name) {\n        mName = name;\n        mThread = new HandlerThread(name) {\n            @NonNull\n            @Override\n            public String toString() {\n                return super.toString() + \"[\" + getThreadId() + \"]\";\n            }\n        };\n        mThread.setDaemon(true);\n        mThread.start();\n        mHandler = new Handler(mThread.getLooper());\n        mExecutor = new Executor() {\n            @Override\n            public void execute(@NonNull Runnable command) {\n                CLASSTOKEN.this.run(command);\n            }\n        };\n\n        // HandlerThreads/Handlers sometimes have a significant warmup time.\n        // We want to spend this time here so when this object is built, it\n        // is fully operational.\n        final CountDownLatch latch = new CountDownLatch(1);\n        post(new Runnable() {\n            @Override\n            public void run() {\n                latch.countDown();\n            }\n        });\n        try {\n            latch.await();\n        } catch (InterruptedException ignore) {}\n    }\n\n    /**\n     * Post an action on this handler.\n     * @param runnable the action\n     */\n    public void run(@NonNull Runnable runnable) {\n        if (Thread.currentThread() == getThread()) {\n            runnable.run();\n        } else {\n            post(runnable);\n        }\n    }\n\n    /**\n     * Post an action on this handler.\n     * @param callable the action\n     */\n    public <T> Task<T> run(@NonNull Callable<T> callable) {\n        if (Thread.currentThread() == getThread()) {\n            try {\n                return Tasks.forResult(callable.call());\n            } catch (Exception e) {\n                return Tasks.forException(e);\n            }\n        } else {\n            return post(callable);\n        }\n    }\n\n    /**\n     * Post an action on this handler.\n     * @param runnable the action\n     */\n    public void post(@NonNull Runnable runnable) {\n        mHandler.post(runnable);\n    }\n\n    /**\n     * Post an action on this handler.\n     * @param callable the action\n     */\n    public <T> Task<T> post(@NonNull final Callable<T> callable) {\n        final TaskCompletionSource<T> source = new TaskCompletionSource<>();\n        post(new Runnable() {\n            @Override\n            public void run() {\n                try {\n                    source.trySetResult(callable.call());\n                } catch (Exception e) {\n                    source.trySetException(e);\n                }\n            }\n        });\n        return source.getTask();\n    }\n\n    /**\n     * Post an action on this handler.\n     * @param delay the delay in millis\n     * @param runnable the action\n     */\n    public void post(long delay, @NonNull Runnable runnable) {\n        mHandler.postDelayed(runnable, delay);\n    }\n\n    /**\n     * Removes a previously added action from this handler.\n     * @param runnable the action\n     */\n    public void remove(@NonNull Runnable runnable) {\n        mHandler.removeCallbacks(runnable);\n    }\n\n    /**\n     * Returns the android backing {@link Handler}.\n     * @return the handler\n     */\n    @NonNull\n    public Handler getHandler() {\n        return mHandler;\n    }\n\n    /**\n     * Returns the android backing {@link HandlerThread}.\n     * @return the thread\n     */\n    @NonNull\n    public HandlerThread getThread() {\n        return mThread;\n    }\n\n    /**\n     * Returns the android backing {@link Looper}.\n     * @return the looper\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    @NonNull\n    public Looper getLooper() {\n        return mThread.getLooper();\n    }\n\n    /**\n     * Returns an {@link Executor}.\n     * @return the executor\n     */\n    @NonNull\n    public Executor getExecutor() {\n        return mExecutor;\n    }\n\n    /**\n     * Destroys this handler and its thread. After this method returns, the handler\n     * should be considered unusable.\n     *\n     * Internal note: this does not remove the thread from our cache, but it does\n     * interrupt it, so the next {@link #get(String)} call will remove it.\n     * In any case, we only store weak references.\n     */\n    public void destroy() {\n        HandlerThread thread = getThread();\n        if (thread.isAlive()) {\n            thread.interrupt();\n            thread.quit();\n            // after quit(), the thread will die at some point in the future. Might take some ms.\n            // try { handler.getThread().join(); } catch (InterruptedException ignore) {}\n        }\n        // This should not be needed, but just to be sure, let's remove it from cache.\n        // For example, interrupt() won't interrupt the thread if it's blocked - it will throw\n        // an exception instead.\n        sCache.remove(mName);\n    }\n\n    /**\n     * Destroys all handlers, interrupting their work and\n     * removing them from our cache.\n     */\n    public static void destroyAll() {\n        for (String key : sCache.keySet()) {\n            WeakReference<CLASSTOKEN> ref = sCache.get(key);\n            //noinspection ConstantConditions\n            CLASSTOKEN handler = ref.get();\n            if (handler != null) handler.destroy();\n            ref.clear();\n        }\n        sCache.clear();\n    }\n", "target": "worker handler"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/gesture/PinchGestureFinder.java:PinchGestureFinder:0", "source": "\n\n    private final static float ADD_SENSITIVITY = 2f;\n\n    private ScaleGestureDetector mDetector;\n    private boolean mNotify;\n    private float mFactor = 0;\n\n    public CLASSTOKEN(@NonNull Controller controller) {\n        super(controller, 2);\n        setGesture(Gesture.PINCH);\n        mDetector = new ScaleGestureDetector(controller.getContext(),\n                new ScaleGestureDetector.SimpleOnScaleGestureListener() {\n            @Override\n            public boolean onScale(ScaleGestureDetector detector) {\n                mNotify = true;\n                mFactor = ((detector.getScaleFactor() - 1) * ADD_SENSITIVITY);\n                return true;\n            }\n        });\n\n        if (Build.VERSION.SDK_INT >= 19) {\n            mDetector.setQuickScaleEnabled(false);\n        }\n    }\n\n    @Override\n    protected boolean handleTouchEvent(@NonNull MotionEvent event) {\n        // Reset the mNotify flag on a new gesture.\n        // This is to ensure that the mNotify flag stays on until the\n        // previous gesture ends.\n        if (event.getAction() == MotionEvent.ACTION_DOWN) {\n            mNotify = false;\n        }\n\n        // Let's see if we detect something. This will call onScale().\n        mDetector.onTouchEvent(event);\n\n        // Keep notifying CameraView as long as the gesture goes.\n        if (mNotify) {\n            getPoint(0).x = event.getX(0);\n            getPoint(0).y = event.getY(0);\n            if (event.getPointerCount() > 1) {\n                getPoint(1).x = event.getX(1);\n                getPoint(1).y = event.getY(1);\n            }\n            return true;\n        }\n        return false;\n    }\n\n    @Override\n    public float getValue(float currValue, float minValue, float maxValue) {\n        float add = getFactor();\n        // ^ This works well if minValue = 0, maxValue = 1.\n        // Account for the different range:\n        add *= (maxValue - minValue);\n\n        // ^ This works well if currValue = 0.\n        // Account for a different starting point:\n        /* if (add > 0) {\n            add *= (maxValue - currValue);\n        } else if (add < 0) {\n            add *= (currValue - minValue);\n        } Nope, I don't like this, it slows everything down. */\n        return currValue + add;\n    }\n\n\n    /* for tests */ protected float getFactor() {\n        return mFactor;\n    }\n", "target": "pinch gesture finder"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/Full2VideoRecorder.java:PrepareException:1", "source": "\n        private CLASSTOKEN(Throwable cause) {\n            super(cause);\n        }\n    ", "target": "prepare exception"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/picture/Full2PictureRecorder.java:Full2PictureRecorder:0", "source": "\n\n    private final ActionHolder mHolder;\n    private final Action mAction;\n    private final ImageReader mPictureReader;\n    private final CaptureRequest.Builder mPictureBuilder;\n\n    private DngCreator mDngCreator;\n\n    public CLASSTOKEN(@NonNull PictureResult.Stub stub,\n                                @NonNull Camera2Engine engine,\n                                @NonNull CaptureRequest.Builder pictureBuilder,\n                                @NonNull ImageReader pictureReader) {\n        super(stub, engine);\n        mHolder = engine;\n        mPictureBuilder = pictureBuilder;\n        mPictureReader = pictureReader;\n        mPictureReader.setOnImageAvailableListener(this, WorkerHandler.get().getHandler());\n        mAction = new BaseAction() {\n\n            @Override\n            protected void onStart(@NonNull ActionHolder holder) {\n                super.onStart(holder);\n                mPictureBuilder.addTarget(mPictureReader.getSurface());\n                if (mResult.format == PictureFormat.JPEG) {\n                    mPictureBuilder.set(CaptureRequest.JPEG_ORIENTATION, mResult.rotation);\n                }\n                mPictureBuilder.setTag(CameraDevice.TEMPLATE_STILL_CAPTURE);\n                try {\n                    holder.applyBuilder(this, mPictureBuilder);\n                } catch (CameraAccessException e) {\n                    mResult = null;\n                    mError = e;\n                    dispatchResult();\n                    setState(STATE_COMPLETED);\n                }\n            }\n\n            @Override\n            public void onCaptureStarted(@NonNull ActionHolder holder,\n                                         @NonNull CaptureRequest request) {\n                super.onCaptureStarted(holder, request);\n                if (request.getTag() == (Integer) CameraDevice.TEMPLATE_STILL_CAPTURE) {\n                    LOG.i(\"onCaptureStarted:\", \"Dispatching picture shutter.\");\n                    dispatchOnShutter(false);\n                    setState(STATE_COMPLETED);\n                }\n            }\n\n            @Override\n            public void onCaptureCompleted(@NonNull ActionHolder holder,\n                                           @NonNull CaptureRequest request,\n                                           @NonNull TotalCaptureResult result) {\n                try {\n                    super.onCaptureCompleted(holder, request, result);\n                } catch (Exception e) {\n                    mError = e;\n                    dispatchResult();\n                }\n\n                if (mResult.format == PictureFormat.DNG) {\n                    mDngCreator = new DngCreator(holder.getCharacteristics(this), result);\n                    mDngCreator.setOrientation(ExifHelper.getExifOrientation(mResult.rotation));\n                    if (mResult.location != null) {\n                        mDngCreator.setLocation(mResult.location);\n                    }\n                }\n            }\n        };\n    }\n\n    @Override\n    public void take() {\n        mAction.start(mHolder);\n    }\n\n    @Override\n    public void onImageAvailable(ImageReader reader) {\n        LOG.i(\"onImageAvailable started.\");\n        Image image = null;\n        //noinspection TryFinallyCanBeTryWithResources\n        try {\n            image = reader.acquireNextImage();\n            switch (mResult.format) {\n                case JPEG: readJpegImage(image); break;\n                case DNG: readRawImage(image); break;\n                default: throw new IllegalStateException(\"Unknown format: \" + mResult.format);\n            }\n        } catch (Exception e) {\n            mResult = null;\n            mError = e;\n            dispatchResult();\n            return;\n        } finally {\n            if (image != null) {\n                image.close();\n            }\n        }\n\n        // Leave.\n        LOG.i(\"onImageAvailable ended.\");\n        dispatchResult();\n    }\n\n    private void readJpegImage(@NonNull Image image) {\n        ByteBuffer buffer = image.getPlanes()[0].getBuffer();\n        byte[] bytes = new byte[buffer.remaining()];\n        buffer.get(bytes);\n        mResult.data = bytes;\n\n        // Just like Camera1, unfortunately, the camera might rotate the image\n        // and put EXIF=0 instead of respecting our EXIF and leave the image unaltered.\n        mResult.rotation = 0;\n        try {\n            ExifInterface exif = new ExifInterface(new ByteArrayInputStream(mResult.data));\n            int exifOrientation = exif.getAttributeInt(ExifInterface.TAG_ORIENTATION,\n                    ExifInterface.ORIENTATION_NORMAL);\n            mResult.rotation = ExifHelper.getOrientation(exifOrientation);\n        } catch (IOException ignore) {\n            // Should not happen\n        }\n    }\n\n    private void readRawImage(@NonNull Image image) {\n        ByteArrayOutputStream array = new ByteArrayOutputStream();\n        BufferedOutputStream stream = new BufferedOutputStream(array);\n        try {\n            mDngCreator.writeImage(stream, image);\n            stream.flush();\n            mResult.data = array.toByteArray();\n        } catch (IOException e) {\n            mDngCreator.close();\n            try { stream.close(); } catch (IOException ignore) {}\n            throw new RuntimeException(e);\n        }\n    }\n", "target": "full 2 picture recorder"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/picture/SnapshotPictureRecorder.java:SnapshotPictureRecorder:0", "source": "\n    private static final String TAG = CLASSTOKEN.class.getSimpleName();\n    protected static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    public CLASSTOKEN(@NonNull PictureResult.Stub stub,\n                                   @Nullable PictureResultListener listener) {\n        super(stub, listener);\n    }\n", "target": "snapshot picture recorder"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/options/Camera1Options.java:Camera1Options:0", "source": "\n\n    public CLASSTOKEN(@NonNull Camera.Parameters params, int cameraId, boolean flipSizes) {\n        List<String> strings;\n        Camera1Mapper mapper = Camera1Mapper.get();\n\n        // Facing\n        Camera.CameraInfo cameraInfo = new Camera.CameraInfo();\n        for (int i = 0, count = Camera.getNumberOfCameras(); i < count; i++) {\n            Camera.getCameraInfo(i, cameraInfo);\n            Facing value = mapper.unmapFacing(cameraInfo.facing);\n            if (value != null) supportedFacing.add(value);\n        }\n\n        // WB\n        strings = params.getSupportedWhiteBalance();\n        if (strings != null) {\n            for (String string : strings) {\n                WhiteBalance value = mapper.unmapWhiteBalance(string);\n                if (value != null) supportedWhiteBalance.add(value);\n            }\n        }\n\n        // Flash\n        supportedFlash.add(Flash.OFF);\n        strings = params.getSupportedFlashModes();\n        if (strings != null) {\n            for (String string : strings) {\n                Flash value = mapper.unmapFlash(string);\n                if (value != null) supportedFlash.add(value);\n            }\n        }\n\n        // Hdr\n        supportedHdr.add(Hdr.OFF);\n        strings = params.getSupportedSceneModes();\n        if (strings != null) {\n            for (String string : strings) {\n                Hdr value = mapper.unmapHdr(string);\n                if (value != null) supportedHdr.add(value);\n            }\n        }\n\n        // zoom\n        zoomSupported = params.isZoomSupported();\n\n        // autofocus\n        autoFocusSupported = params.getSupportedFocusModes()\n                .contains(Camera.Parameters.FOCUS_MODE_AUTO);\n\n        // Exposure correction\n        float step = params.getExposureCompensationStep();\n        exposureCorrectionMinValue = (float) params.getMinExposureCompensation() * step;\n        exposureCorrectionMaxValue = (float) params.getMaxExposureCompensation() * step;\n        exposureCorrectionSupported = params.getMinExposureCompensation() != 0\n                || params.getMaxExposureCompensation() != 0;\n\n        // Picture Sizes\n        List<Camera.Size> sizes = params.getSupportedPictureSizes();\n        for (Camera.Size size : sizes) {\n            int width = flipSizes ? size.height : size.width;\n            int height = flipSizes ? size.width : size.height;\n            supportedPictureSizes.add(new Size(width, height));\n            supportedPictureAspectRatio.add(AspectRatio.of(width, height));\n        }\n\n        // Video Sizes\n        // As a safety measure, remove Sizes bigger than CamcorderProfile.highest\n        CamcorderProfile profile = CamcorderProfiles.get(cameraId,\n                new Size(Integer.MAX_VALUE, Integer.MAX_VALUE));\n        Size videoMaxSize = new Size(profile.videoFrameWidth, profile.videoFrameHeight);\n        List<Camera.Size> vsizes = params.getSupportedVideoSizes();\n        if (vsizes != null) {\n            for (Camera.Size size : vsizes) {\n                if (size.width <= videoMaxSize.getWidth()\n                        && size.height <= videoMaxSize.getHeight()) {\n                    int width = flipSizes ? size.height : size.width;\n                    int height = flipSizes ? size.width : size.height;\n                    supportedVideoSizes.add(new Size(width, height));\n                    supportedVideoAspectRatio.add(AspectRatio.of(width, height));\n                }\n            }\n        } else {\n            // StackOverflow threads seems to agree that if getSupportedVideoSizes is null,\n            // previews can be used.\n            List<Camera.Size> fallback = params.getSupportedPreviewSizes();\n            for (Camera.Size size : fallback) {\n                if (size.width <= videoMaxSize.getWidth()\n                        && size.height <= videoMaxSize.getHeight()) {\n                    int width = flipSizes ? size.height : size.width;\n                    int height = flipSizes ? size.width : size.height;\n                    supportedVideoSizes.add(new Size(width, height));\n                    supportedVideoAspectRatio.add(AspectRatio.of(width, height));\n                }\n            }\n        }\n\n        // Preview FPS\n        previewFrameRateMinValue = Float.MAX_VALUE;\n        previewFrameRateMaxValue = -Float.MAX_VALUE;\n        List<int[]> fpsRanges = params.getSupportedPreviewFpsRange();\n        for (int[] fpsRange : fpsRanges) {\n            float lower = (float) fpsRange[0] / 1000F;\n            float upper = (float) fpsRange[1] / 1000F;\n            previewFrameRateMinValue = Math.min(previewFrameRateMinValue, lower);\n            previewFrameRateMaxValue = Math.max(previewFrameRateMaxValue, upper);\n        }\n\n        // Picture formats\n        supportedPictureFormats.add(PictureFormat.JPEG);\n\n        // Frame processing formats\n        supportedFrameProcessingFormats.add(ImageFormat.NV21);\n    }\n", "target": "camera 1 options"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filter/MultiFilter.java:MultiFilter:0", "source": "\n\n    @VisibleForTesting\n    static class State {\n        @VisibleForTesting boolean isProgramCreated = false;\n        @VisibleForTesting boolean isFramebufferCreated = false;\n        private boolean sizeChanged = false;\n        @VisibleForTesting Size size = null;\n        private int programHandle = -1;\n        private GlFramebuffer outputFramebuffer = null;\n        private GlTexture outputTexture = null;\n    }\n\n    @VisibleForTesting final List<Filter> filters = new ArrayList<>();\n    @VisibleForTesting final Map<Filter, State> states = new HashMap<>();\n    private final Object lock = new Object();\n    private Size size = null;\n    private float parameter1 = 0F;\n    private float parameter2 = 0F;\n\n    /**\n     * Creates a new group with the given filters.\n     * @param filters children\n     */\n    public CLASSTOKEN(@NonNull Filter... filters) {\n        this(Arrays.asList(filters));\n    }\n\n    /**\n     * Creates a new group with the given filters.\n     * @param filters children\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public CLASSTOKEN(@NonNull Collection<Filter> filters) {\n        for (Filter filter : filters) {\n            addFilter(filter);\n        }\n    }\n\n    /**\n     * Adds a new filter. It will be used in the next frame.\n     * If the filter is a {@link CLASSTOKEN}, we'll use its children instead.\n     *\n     * @param filter a new filter\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public void addFilter(@NonNull Filter filter) {\n        if (filter instanceof CLASSTOKEN) {\n            CLASSTOKEN multiFilter = (CLASSTOKEN) filter;\n            for (Filter multiChild : multiFilter.filters) {\n                addFilter(multiChild);\n            }\n            return;\n        }\n        synchronized (lock) {\n            if (!filters.contains(filter)) {\n                filters.add(filter);\n                states.put(filter, new State());\n            }\n        }\n    }\n\n    // We don't offer a removeFilter method since that would cause issues\n    // with cleanup. Cleanup must happen on the GL thread so we'd have to wait\n    // for new rendering call (which might not even happen).\n\n    private void maybeCreateProgram(@NonNull Filter filter, boolean isFirst, boolean isLast) {\n        State state = states.get(filter);\n        //noinspection ConstantConditions\n        if (state.isProgramCreated) return;\n        state.isProgramCreated = true;\n\n        // The first shader actually reads from a OES texture, but the others\n        // will read from the 2d framebuffer texture. This is a dirty hack.\n        String fragmentShader = isFirst\n                ? filter.getFragmentShader()\n                : filter.getFragmentShader().replace(\"samplerExternalOES \", \"sampler2D \");\n        String vertexShader = filter.getVertexShader();\n        state.programHandle = GlProgram.create(vertexShader, fragmentShader);\n        filter.onCreate(state.programHandle);\n    }\n\n    private void maybeDestroyProgram(@NonNull Filter filter) {\n        State state = states.get(filter);\n        //noinspection ConstantConditions\n        if (!state.isProgramCreated) return;\n        state.isProgramCreated = false;\n        filter.onDestroy();\n        GLES20.glDeleteProgram(state.programHandle);\n        state.programHandle = -1;\n    }\n\n    private void maybeCreateFramebuffer(@NonNull Filter filter, boolean isFirst, boolean isLast) {\n        State state = states.get(filter);\n        if (isLast) {\n            //noinspection ConstantConditions\n            state.sizeChanged = false;\n            return;\n        }\n        //noinspection ConstantConditions\n        if (state.sizeChanged) {\n            maybeDestroyFramebuffer(filter);\n            state.sizeChanged = false;\n        }\n        if (!state.isFramebufferCreated) {\n            state.isFramebufferCreated = true;\n            state.outputTexture = new GlTexture(GLES20.GL_TEXTURE0,\n                    GLES20.GL_TEXTURE_2D,\n                    state.size.getWidth(),\n                    state.size.getHeight());\n            state.outputFramebuffer = new GlFramebuffer();\n            state.outputFramebuffer.attach(state.outputTexture);\n        }\n    }\n\n    private void maybeDestroyFramebuffer(@NonNull Filter filter) {\n        State state = states.get(filter);\n        //noinspection ConstantConditions\n        if (!state.isFramebufferCreated) return;\n        state.isFramebufferCreated = false;\n        state.outputFramebuffer.release();\n        state.outputFramebuffer = null;\n        state.outputTexture.release();\n        state.outputTexture = null;\n    }\n\n    // Any thread...\n    private void maybeSetSize(@NonNull Filter filter) {\n        State state = states.get(filter);\n        //noinspection ConstantConditions\n        if (size != null && !size.equals(state.size)) {\n            state.size = size;\n            state.sizeChanged = true;\n            filter.setSize(size.getWidth(), size.getHeight());\n        }\n    }\n\n    @Override\n    public void onCreate(int programHandle) {\n        // We'll create children during the draw() op, since some of them\n        // might have been added after this onCreate() is called.\n    }\n\n    @NonNull\n    @Override\n    public String getVertexShader() {\n        // Whatever, we won't be using this.\n        return GlTextureProgram.SIMPLE_VERTEX_SHADER;\n    }\n\n    @NonNull\n    @Override\n    public String getFragmentShader() {\n        // Whatever, we won't be using this.\n        return GlTextureProgram.SIMPLE_FRAGMENT_SHADER;\n    }\n\n    @Override\n    public void onDestroy() {\n        synchronized (lock) {\n            for (Filter filter : filters) {\n                maybeDestroyFramebuffer(filter);\n                maybeDestroyProgram(filter);\n            }\n        }\n    }\n\n    @Override\n    public void setSize(int width, int height) {\n        size = new Size(width, height);\n        synchronized (lock) {\n            for (Filter filter : filters) {\n                maybeSetSize(filter);\n            }\n        }\n    }\n\n    @Override\n    public void draw(long timestampUs, @NonNull float[] transformMatrix) {\n        synchronized (lock) {\n            for (int i = 0; i < filters.size(); i++) {\n                boolean isFirst = i == 0;\n                boolean isLast = i == filters.size() - 1;\n                Filter filter = filters.get(i);\n                State state = states.get(filter);\n\n                maybeSetSize(filter);\n                maybeCreateProgram(filter, isFirst, isLast);\n                maybeCreateFramebuffer(filter, isFirst, isLast);\n\n                //noinspection ConstantConditions\n                GLES20.glUseProgram(state.programHandle);\n\n                // Define the output framebuffer.\n                // Each filter outputs into its own framebuffer object, except the\n                // last filter, which outputs into the default framebuffer.\n                if (!isLast) {\n                    state.outputFramebuffer.bind();\n                    GLES20.glClearColor(0, 0, 0, 0);\n                } else {\n                    GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER, 0);\n                }\n\n                // Perform the actual drawing.\n                // The first filter should apply all the transformations. Then,\n                // since they are applied, we should use a no-op matrix.\n                if (isFirst) {\n                    filter.draw(timestampUs, transformMatrix);\n                } else {\n                    filter.draw(timestampUs, Egloo.IDENTITY_MATRIX);\n                }\n\n                // Set the input for the next cycle:\n                // It is the framebuffer texture from this cycle. If this is the last\n                // filter, reset this value just to cleanup.\n                if (!isLast) {\n                    state.outputTexture.bind();\n                } else {\n                    GLES20.glBindTexture(GLES20.GL_TEXTURE_2D, 0);\n                    GLES20.glActiveTexture(GLES20.GL_TEXTURE0);\n                }\n\n                GLES20.glUseProgram(0);\n            }\n        }\n    }\n\n    @NonNull\n    @Override\n    public Filter copy() {\n        synchronized (lock) {\n            CLASSTOKEN copy = new CLASSTOKEN();\n            if (size != null) {\n                copy.setSize(size.getWidth(), size.getHeight());\n            }\n            for (Filter filter : filters) {\n                copy.addFilter(filter.copy());\n            }\n            return copy;\n        }\n    }\n\n    @Override\n    public void setParameter1(float parameter1) {\n        this.parameter1 = parameter1;\n        synchronized (lock) {\n            for (Filter filter : filters) {\n                if (filter instanceof OneParameterFilter) {\n                    ((OneParameterFilter) filter).setParameter1(parameter1);\n                }\n            }\n        }\n    }\n\n    @Override\n    public void setParameter2(float parameter2) {\n        this.parameter2 = parameter2;\n        synchronized (lock) {\n            for (Filter filter : filters) {\n                if (filter instanceof TwoParameterFilter) {\n                    ((TwoParameterFilter) filter).setParameter2(parameter2);\n                }\n            }\n        }\n    }\n\n    @Override\n    public float getParameter1() {\n        return parameter1;\n    }\n\n    @Override\n    public float getParameter2() {\n        return parameter2;\n    }\n", "target": "multi filter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filters/FillLightFilter.java:FillLightFilter:0", "source": "\n\n    private final static String FRAGMENT_SHADER = \"#extension GL_OES_EGL_image_external : require\\n\"\n            + \"precision mediump float;\\n\"\n            + \"uniform samplerExternalOES sTexture;\\n\"\n            + \"uniform float mult;\\n\"\n            + \"uniform float igamma;\\n\"\n            + \"varying vec2 \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\";\\n\"\n            + \"void main() {\\n\"\n            + \"  const vec3 color_weights = vec3(0.25, 0.5, 0.25);\\n\"\n            + \"  vec4 color = texture2D(sTexture, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\");\\n\"\n            + \"  float lightmask = dot(color.rgb, color_weights);\\n\"\n            + \"  float backmask = (1.0 - lightmask);\\n\"\n            + \"  vec3 ones = vec3(1.0, 1.0, 1.0);\\n\"\n            + \"  vec3 diff = pow(mult * color.rgb, igamma * ones) - color.rgb;\\n\"\n            + \"  diff = min(diff, 1.0);\\n\"\n            + \"  vec3 new_color = min(color.rgb + diff * backmask, 1.0);\\n\"\n            + \"  gl_FragColor = vec4(new_color, color.a);\\n\"\n            + \"}\\n\";\n\n    private float strength = 0.5f;\n    private int multiplierLocation = -1;\n    private int gammaLocation = -1;\n\n    public CLASSTOKEN() { }\n\n    /**\n     * Sets the current strength.\n     * 0.0: no change.\n     * 1.0: max strength.\n     *\n     * @param strength strength\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public void setStrength(float strength) {\n        if (strength < 0.0f) strength = 0f;\n        if (strength > 1.0f) strength = 1f;\n        this.strength = strength;\n    }\n\n    /**\n     * Returns the current strength.\n     *\n     * @see #setStrength(float)\n     * @return strength\n     */\n    @SuppressWarnings({\"unused\", \"WeakerAccess\"})\n    public float getStrength() {\n        return strength;\n    }\n\n    @Override\n    public void setParameter1(float value) {\n        setStrength(value);\n    }\n\n    @Override\n    public float getParameter1() {\n        return getStrength();\n    }\n\n    @NonNull\n    @Override\n    public String getFragmentShader() {\n        return FRAGMENT_SHADER;\n    }\n\n    @Override\n    public void onCreate(int programHandle) {\n        super.onCreate(programHandle);\n        multiplierLocation = GLES20.glGetUniformLocation(programHandle, \"mult\");\n        Egloo.checkGlProgramLocation(multiplierLocation, \"mult\");\n        gammaLocation = GLES20.glGetUniformLocation(programHandle, \"igamma\");\n        Egloo.checkGlProgramLocation(gammaLocation, \"igamma\");\n    }\n\n    @Override\n    public void onDestroy() {\n        super.onDestroy();\n        multiplierLocation = -1;\n        gammaLocation = -1;\n    }\n\n    @Override\n    protected void onPreDraw(long timestampUs, @NonNull float[] transformMatrix) {\n        super.onPreDraw(timestampUs, transformMatrix);\n        float amount = 1.0f - strength;\n        float multiplier = 1.0f / (amount * 0.7f + 0.3f);\n        GLES20.glUniform1f(multiplierLocation, multiplier);\n        Egloo.checkGlError(\"glUniform1f\");\n\n        float fadeGamma = 0.3f;\n        float faded = fadeGamma + (1.0f - fadeGamma) * multiplier;\n        float gamma = 1.0f / faded;\n        GLES20.glUniform1f(gammaLocation, gamma);\n        Egloo.checkGlError(\"glUniform1f\");\n    }\n", "target": "fill light filter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/CameraOptions.java:CameraOptions:0", "source": "\n\n    protected Set<WhiteBalance> supportedWhiteBalance = new HashSet<>(5);\n    protected Set<Facing> supportedFacing = new HashSet<>(2);\n    protected Set<Flash> supportedFlash = new HashSet<>(4);\n    protected Set<Hdr> supportedHdr = new HashSet<>(2);\n    protected Set<Size> supportedPictureSizes = new HashSet<>(15);\n    protected Set<Size> supportedVideoSizes = new HashSet<>(5);\n    protected Set<AspectRatio> supportedPictureAspectRatio = new HashSet<>(4);\n    protected Set<AspectRatio> supportedVideoAspectRatio = new HashSet<>(3);\n    protected Set<PictureFormat> supportedPictureFormats = new HashSet<>(2);\n    protected Set<Integer> supportedFrameProcessingFormats = new HashSet<>(2);\n\n    protected boolean zoomSupported;\n    protected boolean exposureCorrectionSupported;\n    protected float exposureCorrectionMinValue;\n    protected float exposureCorrectionMaxValue;\n    protected boolean autoFocusSupported;\n    protected float previewFrameRateMinValue;\n    protected float previewFrameRateMaxValue;\n\n    protected CLASSTOKEN() { }\n\n    /**\n     * Shorthand for getSupported*().contains(value).\n     *\n     * @param control value to check\n     * @return whether it's supported\n     */\n    public final boolean supports(@NonNull Control control) {\n        return getSupportedControls(control.getClass()).contains(control);\n    }\n\n    /**\n     * Shorthand for other methods in this class,\n     * e.g. supports(GestureAction.ZOOM) == isZoomSupported().\n     *\n     * @param action value to be checked\n     * @return whether it's supported\n     */\n    public final boolean supports(@NonNull GestureAction action) {\n        switch (action) {\n            case AUTO_FOCUS:\n                return isAutoFocusSupported();\n            case TAKE_PICTURE:\n            case FILTER_CONTROL_1:\n            case FILTER_CONTROL_2:\n            case NONE:\n                return true;\n            case ZOOM:\n                return isZoomSupported();\n            case EXPOSURE_CORRECTION:\n                return isExposureCorrectionSupported();\n        }\n        return false;\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    @NonNull\n    public final <T extends Control> Collection<T> getSupportedControls(\n            @NonNull Class<T> controlClass) {\n        if (controlClass.equals(Audio.class)) {\n            return (Collection<T>) Arrays.asList(Audio.values());\n        } else if (controlClass.equals(Facing.class)) {\n            return (Collection<T>) getSupportedFacing();\n        } else if (controlClass.equals(Flash.class)) {\n            return (Collection<T>) getSupportedFlash();\n        } else if (controlClass.equals(Grid.class)) {\n            return (Collection<T>) Arrays.asList(Grid.values());\n        } else if (controlClass.equals(Hdr.class)) {\n            return (Collection<T>) getSupportedHdr();\n        } else if (controlClass.equals(Mode.class)) {\n            return (Collection<T>) Arrays.asList(Mode.values());\n        } else if (controlClass.equals(VideoCodec.class)) {\n            return (Collection<T>) Arrays.asList(VideoCodec.values());\n        } else if (controlClass.equals(AudioCodec.class)) {\n            return (Collection<T>) Arrays.asList(AudioCodec.values());\n        } else if (controlClass.equals(WhiteBalance.class)) {\n            return (Collection<T>) getSupportedWhiteBalance();\n        } else if (controlClass.equals(Engine.class)) {\n            return (Collection<T>) Arrays.asList(Engine.values());\n        } else if (controlClass.equals(Preview.class)) {\n            return (Collection<T>) Arrays.asList(Preview.values());\n        } else if (controlClass.equals(PictureFormat.class)) {\n            return (Collection<T>) getSupportedPictureFormats();\n        }\n        // Unrecognized control.\n        return Collections.emptyList();\n    }\n\n    /**\n     * Set of supported picture sizes for the currently opened camera.\n     *\n     * @return a collection of supported values.\n     */\n    @NonNull\n    public final Collection<Size> getSupportedPictureSizes() {\n        return Collections.unmodifiableSet(supportedPictureSizes);\n    }\n\n    /**\n     * Set of supported picture aspect ratios for the currently opened camera.\n     *\n     * @return a collection of supported values.\n     */\n    @NonNull\n    public final Collection<AspectRatio> getSupportedPictureAspectRatios() {\n        return Collections.unmodifiableSet(supportedPictureAspectRatio);\n    }\n\n    /**\n     * Set of supported video sizes for the currently opened camera.\n     *\n     * @return a collection of supported values.\n     */\n    @NonNull\n    public final Collection<Size> getSupportedVideoSizes() {\n        return Collections.unmodifiableSet(supportedVideoSizes);\n    }\n\n    /**\n     * Set of supported picture aspect ratios for the currently opened camera.\n     *\n     * @return a set of supported values.\n     */\n    @NonNull\n    public final Collection<AspectRatio> getSupportedVideoAspectRatios() {\n        return Collections.unmodifiableSet(supportedVideoAspectRatio);\n    }\n\n    /**\n     * Set of supported facing values.\n     *\n     * @see Facing#BACK\n     * @see Facing#FRONT\n     * @return a collection of supported values.\n     */\n    @NonNull\n    public final Collection<Facing> getSupportedFacing() {\n        return Collections.unmodifiableSet(supportedFacing);\n    }\n\n    /**\n     * Set of supported flash values.\n     *\n     * @see Flash#AUTO\n     * @see Flash#OFF\n     * @see Flash#ON\n     * @see Flash#TORCH\n     * @return a collection of supported values.\n     */\n    @NonNull\n    public final Collection<Flash> getSupportedFlash() {\n        return Collections.unmodifiableSet(supportedFlash);\n    }\n\n    /**\n     * Set of supported white balance values.\n     *\n     * @see WhiteBalance#AUTO\n     * @see WhiteBalance#INCANDESCENT\n     * @see WhiteBalance#FLUORESCENT\n     * @see WhiteBalance#DAYLIGHT\n     * @see WhiteBalance#CLOUDY\n     * @return a collection of supported values.\n     */\n    @NonNull\n    public final Collection<WhiteBalance> getSupportedWhiteBalance() {\n        return Collections.unmodifiableSet(supportedWhiteBalance);\n    }\n\n    /**\n     * Set of supported hdr values.\n     *\n     * @see Hdr#OFF\n     * @see Hdr#ON\n     * @return a collection of supported values.\n     */\n    @NonNull\n    public final Collection<Hdr> getSupportedHdr() {\n        return Collections.unmodifiableSet(supportedHdr);\n    }\n\n    /**\n     * Set of supported picture formats.\n     *\n     * @see PictureFormat#JPEG\n     * @see PictureFormat#DNG\n     * @return a collection of supported values.\n     */\n    @NonNull\n    public final Collection<PictureFormat> getSupportedPictureFormats() {\n        return Collections.unmodifiableSet(supportedPictureFormats);\n    }\n\n    /**\n     * Set of supported formats for frame processing,\n     * as {@link ImageFormat} constants.\n     *\n     * @see CameraView#setFrameProcessingFormat(int)\n     * @return a collection of supported values.\n     */\n    @NonNull\n    public final Collection<Integer> getSupportedFrameProcessingFormats() {\n        return Collections.unmodifiableSet(supportedFrameProcessingFormats);\n    }\n\n    /**\n     * Whether zoom is supported. If this is false, pinch-to-zoom\n     * will not work and {@link CameraView#setZoom(float)} will have no effect.\n     *\n     * @return whether zoom is supported.\n     */\n    public final boolean isZoomSupported() {\n        return zoomSupported;\n    }\n\n\n    /**\n     * Whether touch metering (metering with respect to a specific region of the screen) is\n     * supported. If it is, you can map gestures to {@link GestureAction#AUTO_FOCUS}\n     * and metering will change on tap.\n     *\n     * @return whether auto focus is supported.\n     */\n    public final boolean isAutoFocusSupported() {\n        return autoFocusSupported;\n    }\n\n    /**\n     * Whether exposure correction is supported. If this is false, calling\n     * {@link CameraView#setExposureCorrection(float)} has no effect.\n     *\n     * @see #getExposureCorrectionMinValue()\n     * @see #getExposureCorrectionMaxValue()\n     * @return whether exposure correction is supported.\n     */\n    public final boolean isExposureCorrectionSupported() {\n        return exposureCorrectionSupported;\n    }\n\n    /**\n     * The minimum value of negative exposure correction, in EV stops.\n     * This is presumably negative or 0 if not supported.\n     *\n     * @return min EV value\n     */\n    public final float getExposureCorrectionMinValue() {\n        return exposureCorrectionMinValue;\n    }\n\n\n    /**\n     * The maximum value of positive exposure correction, in EV stops.\n     * This is presumably positive or 0 if not supported.\n     *\n     * @return max EV value\n     */\n    public final float getExposureCorrectionMaxValue() {\n        return exposureCorrectionMaxValue;\n    }\n\n    /**\n     * The minimum value for the preview frame rate, in frames per second (FPS).\n     *\n     * @return the min value\n     */\n    public final float getPreviewFrameRateMinValue() {\n        return previewFrameRateMinValue;\n    }\n\n    /**\n     * The maximum value for the preview frame rate, in frames per second (FPS).\n     *\n     * @return the max value\n     */\n    public final float getPreviewFrameRateMaxValue() {\n        return previewFrameRateMaxValue;\n    }\n", "target": "camera options"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/lock/BaseLock.java:BaseLock:0", "source": "\n\n    @Override\n    protected final void onStart(@NonNull ActionHolder holder) {\n        super.onStart(holder);\n        boolean isSkipped = checkShouldSkip(holder);\n        boolean isSupported = checkIsSupported(holder);\n        if (isSupported && !isSkipped) {\n            onStarted(holder);\n        } else {\n            setState(STATE_COMPLETED);\n        }\n    }\n\n    protected abstract void onStarted(@NonNull ActionHolder holder);\n\n    protected abstract boolean checkShouldSkip(@NonNull ActionHolder holder);\n\n    protected abstract boolean checkIsSupported(@NonNull ActionHolder holder);\n", "target": "base lock"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/internal/CamcorderProfiles.java:CamcorderProfiles:0", "source": "\n\n    private static final String TAG = CLASSTOKEN.class.getSimpleName();\n    private static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    @SuppressLint(\"UseSparseArrays\")\n    private static Map<Size, Integer> sizeToProfileMap = new HashMap<>();\n\n    static {\n        sizeToProfileMap.put(new Size(176, 144), CamcorderProfile.QUALITY_QCIF);\n        sizeToProfileMap.put(new Size(320, 240), CamcorderProfile.QUALITY_QVGA);\n        sizeToProfileMap.put(new Size(352, 288), CamcorderProfile.QUALITY_CIF);\n        sizeToProfileMap.put(new Size(720, 480), CamcorderProfile.QUALITY_480P);\n        sizeToProfileMap.put(new Size(1280, 720), CamcorderProfile.QUALITY_720P);\n        sizeToProfileMap.put(new Size(1920, 1080), CamcorderProfile.QUALITY_1080P);\n        if (Build.VERSION.SDK_INT >= 21) {\n            sizeToProfileMap.put(new Size(3840, 2160),\n                    CamcorderProfile.QUALITY_2160P);\n        }\n    }\n\n\n    /**\n     * Returns a CamcorderProfile that's somewhat coherent with the target size,\n     * to ensure we get acceptable video/audio parameters for MediaRecorders\n     * (most notably the bitrate).\n     *\n     * @param cameraId the camera2 id\n     * @param targetSize the target video size\n     * @return a profile\n     */\n    @NonNull\n    public static CamcorderProfile get(@NonNull String cameraId, @NonNull Size targetSize) {\n        // It seems that the way to do this is to use Integer.parseInt().\n        try {\n            int camera1Id = Integer.parseInt(cameraId);\n            return get(camera1Id, targetSize);\n        } catch (NumberFormatException e) {\n            LOG.w(\"NumberFormatException for Camera2 id:\", cameraId);\n            return CamcorderProfile.get(CamcorderProfile.QUALITY_LOW);\n        }\n    }\n\n    /**\n     * Returns a CamcorderProfile that's somewhat coherent with the target size,\n     * to ensure we get acceptable video/audio parameters for MediaRecorders\n     * (most notably the bitrate).\n     *\n     * @param cameraId the camera id\n     * @param targetSize the target video size\n     * @return a profile\n     */\n    @NonNull\n    public static CamcorderProfile get(int cameraId, @NonNull Size targetSize) {\n        final long targetArea = (long) targetSize.getWidth() * targetSize.getHeight();\n        List<Size> sizes = new ArrayList<>(sizeToProfileMap.keySet());\n        Collections.sort(sizes, new Comparator<Size>() {\n            @Override\n            public int compare(Size s1, Size s2) {\n                long a1 = Math.abs(s1.getWidth() * s1.getHeight() - targetArea);\n                long a2 = Math.abs(s2.getWidth() * s2.getHeight() - targetArea);\n                //noinspection UseCompareMethod\n                return (a1 < a2) ? -1 : ((a1 == a2) ? 0 : 1);\n            }\n        });\n        while (sizes.size() > 0) {\n            Size candidate = sizes.remove(0);\n            //noinspection ConstantConditions\n            int quality = sizeToProfileMap.get(candidate);\n            if (CamcorderProfile.hasProfile(cameraId, quality)) {\n                return CamcorderProfile.get(cameraId, quality);\n            }\n        }\n        // Should never happen, but fallback to low.\n        return CamcorderProfile.get(cameraId, CamcorderProfile.QUALITY_LOW);\n    }\n", "target": "camcorder profiles"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/encoding/AudioMediaEncoder.java:AudioRecordingThread:1", "source": "\n\n        private AudioRecord mAudioRecord;\n        private ByteBuffer mCurrentBuffer;\n        private int mCurrentReadBytes;\n\n        private long mLastTimeUs;\n        private long mFirstTimeUs = Long.MIN_VALUE;\n\n        private CLASSTOKEN() {\n            setPriority(Thread.MAX_PRIORITY);\n            final int minBufferSize = AudioRecord.getMinBufferSize(\n                    mConfig.samplingFrequency,\n                    mConfig.audioFormatChannels(),\n                    mConfig.encoding);\n            // Make this bigger so we don't skip frames. 25: Stereo: 51200. Mono: 25600\n            // 25 is quite big already. Tried to make it bigger to solve the read() delay\n            // but it just makes things worse (ruins MONO as well).\n            // Tried to make it smaller and things change as well.\n            int bufferSize = mConfig.frameSize() * mConfig.audioRecordBufferFrames();\n            while (bufferSize < minBufferSize) {\n                bufferSize += mConfig.frameSize(); // Unlikely.\n            }\n            mAudioRecord = new AudioRecord(MediaRecorder.AudioSource.CAMCORDER,\n                    mConfig.samplingFrequency,\n                    mConfig.audioFormatChannels(),\n                    mConfig.encoding,\n                    bufferSize);\n        }\n\n        @Override\n        public void run() {\n            mAudioRecord.startRecording();\n            while (!mRequestStop) {\n                if (!hasReachedMaxLength()) {\n                    read(false);\n                } else {\n                    // We have reached the max length, so stop reading.\n                    // However, do not get out of the loop - the controller\n                    // will call stop() on us soon. It's not our responsibility\n                    // to stop ourselves.\n                    //noinspection UnnecessaryContinue\n                    continue;\n                }\n            }\n            LOG.w(\"Stop was requested. We're out of the loop. Will post an endOfStream.\");\n            // Last input with 0 length. This will signal the endOfStream.\n            // Can't use drain(true); it is only available when writing to the codec InputSurface.\n            boolean didReadEos = false;\n            while (!didReadEos) {\n                didReadEos = read(true);\n            }\n            mAudioRecord.stop();\n            mAudioRecord.release();\n            mAudioRecord = null;\n        }\n\n        /**\n         * Returns true if we found a buffer and could proceed, false if we found no buffer\n         * so the operation should be performed again by the caller.\n         * @param endOfStream true if last read\n         * @return true if proceeded\n         */\n        private boolean read(boolean endOfStream) {\n            mCurrentBuffer = mByteBufferPool.get();\n            if (mCurrentBuffer == null) {\n                // This can happen and it means that encoding is slow with respect to recording.\n                // One might be tempted to fix precisely the next frame presentation time when\n                // this happens, but this is not needed because the current increaseTime()\n                // algorithm will consider delays when they get large.\n                // Sleeping before returning is a good way of balancing the two operations.\n                // However, if endOfStream, we CAN'T lose this frame!\n                if (endOfStream) {\n                    LOG.v(\"read thread - eos: true - No buffer, retrying.\");\n                } else {\n                    LOG.w(\"read thread - eos: false - Skipping audio frame,\",\n                            \"encoding is too slow.\");\n                    skipFrames(6); // sleep a bit\n                }\n                return false;\n            } else {\n                mCurrentBuffer.clear();\n                // When stereo, we read twice the data here and AudioRecord will fill the buffer\n                // with left and right bytes. https://stackoverflow.com/q/20594750/4288782\n                if (PERFORMANCE_DEBUG) {\n                    long before = System.nanoTime();\n                    mCurrentReadBytes = mAudioRecord.read(mCurrentBuffer, mConfig.frameSize());\n                    long after = System.nanoTime();\n                    float delayMillis = (after - before) / 1000000F;\n                    float durationMillis = AudioTimestamp.bytesToMillis(mCurrentReadBytes,\n                            mConfig.byteRate());\n                    LOG.v(\"read thread - reading took:\", delayMillis,\n                            \"should be:\", durationMillis,\n                            \"delay:\", delayMillis - durationMillis);\n                } else {\n                    mCurrentReadBytes = mAudioRecord.read(mCurrentBuffer, mConfig.frameSize());\n                }\n                LOG.v(\"read thread - eos:\", endOfStream, \"- Read new audio frame. Bytes:\",\n                        mCurrentReadBytes);\n                if (mCurrentReadBytes > 0) { // Good read: increase PTS.\n                    increaseTime(mCurrentReadBytes, endOfStream);\n                    LOG.v(\"read thread - eos:\", endOfStream, \"- mLastTimeUs:\", mLastTimeUs);\n                    mCurrentBuffer.limit(mCurrentReadBytes);\n                    enqueue(mCurrentBuffer, mLastTimeUs, endOfStream);\n                } else if (mCurrentReadBytes == AudioRecord.ERROR_INVALID_OPERATION) {\n                    LOG.e(\"read thread - eos:\", endOfStream,\n                            \"- Got AudioRecord.ERROR_INVALID_OPERATION\");\n                } else if (mCurrentReadBytes == AudioRecord.ERROR_BAD_VALUE) {\n                    LOG.e(\"read thread - eos:\", endOfStream,\n                            \"- Got AudioRecord.ERROR_BAD_VALUE\");\n                }\n                return true;\n            }\n        }\n\n        /**\n         * Increases presentation time and checks for max length constraint. This is much faster\n         * then waiting for the encoder to check it during {@link #drainOutput(boolean)}. We\n         * want to catch this as soon as possible so we stop recording useless frames and bother\n         * all the threads involved.\n         * @param readBytes bytes read in last reading\n         * @param endOfStream end of stream?\n         */\n        private void increaseTime(int readBytes, boolean endOfStream) {\n            // Get the latest frame timestamp.\n            mLastTimeUs = mTimestamp.increaseUs(readBytes);\n            if (mFirstTimeUs == Long.MIN_VALUE) {\n                mFirstTimeUs = mLastTimeUs;\n                // Compute the first frame milliseconds as well.\n                notifyFirstFrameMillis(System.currentTimeMillis()\n                        - AudioTimestamp.bytesToMillis(readBytes, mConfig.byteRate()));\n            }\n\n            // See if we reached the max length value.\n            if (!hasReachedMaxLength()) {\n                boolean didReachMaxLength = (mLastTimeUs - mFirstTimeUs) > getMaxLengthUs();\n                if (didReachMaxLength && !endOfStream) {\n                    LOG.w(\"read thread - this frame reached the maxLength! deltaUs:\",\n                            mLastTimeUs - mFirstTimeUs);\n                    notifyMaxLengthReached();\n                }\n            }\n\n            // Maybe add noise.\n            maybeAddNoise();\n        }\n\n        private void enqueue(@NonNull ByteBuffer byteBuffer,\n                             long timestamp,\n                             boolean isEndOfStream) {\n            if (PERFORMANCE_DEBUG) {\n                mDebugSendStartMap.put(timestamp, System.nanoTime() / 1000000);\n            }\n            int readBytes = byteBuffer.remaining();\n            InputBuffer inputBuffer = mInputBufferPool.get();\n            //noinspection ConstantConditions\n            inputBuffer.source = byteBuffer;\n            inputBuffer.timestamp = timestamp;\n            inputBuffer.length = readBytes;\n            inputBuffer.isEndOfStream = isEndOfStream;\n            mInputBufferQueue.add(inputBuffer);\n        }\n\n        /**\n         * If our {@link AudioTimestamp} detected huge gap, and the performance flag is enabled,\n         * we can add noise to fill them.\n         *\n         * Even if we always pass the correct timestamps, if there are big gaps between the frames,\n         * the encoder implementation might shrink all timestamps to have a continuous audio.\n         * This results in a video that is fast-forwarded.\n         *\n         * Adding noise does not solve the gaps issue, we'll still have distorted audio, but\n         * at least we get a video that has the correct playback speed.\n         *\n         * NOTE: this MUST be fast!\n         * If this operation is slow, we make the {@link CLASSTOKEN} busy, so we'll\n         * read the next frame with a delay, so we'll have even more gaps at the next call\n         * and spend even more time here. The result might be recording no audio at all - just\n         * random noise.\n         * This is the reason why we have a {@link #PERFORMANCE_MAX_GAPS} number.\n         */\n        private void maybeAddNoise() {\n            if (!PERFORMANCE_FILL_GAPS) return;\n            int gaps = mTimestamp.getGapCount(mConfig.frameSize());\n            if (gaps <= 0) return;\n\n            long gapStart = mTimestamp.getGapStartUs(mLastTimeUs);\n            long frameUs = AudioTimestamp.bytesToUs(mConfig.frameSize(), mConfig.byteRate());\n            LOG.w(\"read thread - GAPS: trying to add\", gaps,\n                    \"noise buffers. PERFORMANCE_MAX_GAPS:\", PERFORMANCE_MAX_GAPS);\n            for (int i = 0; i < Math.min(gaps, PERFORMANCE_MAX_GAPS); i++) {\n                ByteBuffer noiseBuffer = mByteBufferPool.get();\n                if (noiseBuffer == null) {\n                    LOG.e(\"read thread - GAPS: aborting because we have no free buffer.\");\n                    break;\n                }\n                noiseBuffer.clear();\n                mAudioNoise.fill(noiseBuffer);\n                noiseBuffer.rewind();\n                enqueue(noiseBuffer, gapStart, false);\n                gapStart += frameUs;\n            }\n        }\n    ", "target": "audio recording thread"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/encoding/VideoMediaEncoder.java:VideoMediaEncoder:0", "source": "\n\n    private static final String TAG = CLASSTOKEN.class.getSimpleName();\n    private static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected C mConfig;\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected Surface mSurface;\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected int mFrameNumber = -1;\n\n    private boolean mSyncFrameFound = false;\n\n    CLASSTOKEN(@NonNull C config) {\n        super(\"VideoEncoder\");\n        mConfig = config;\n    }\n\n    @EncoderThread\n    @Override\n    protected void onPrepare(@NonNull MediaEncoderEngine.Controller controller, long maxLengthUs) {\n        MediaFormat format = MediaFormat.createVideoFormat(mConfig.mimeType, mConfig.width,\n                mConfig.height);\n\n        // Failing to specify some of these can cause the MediaCodec configure() call to throw an\n        // unhelpful exception. About COLOR_FormatSurface, see\n        // https://stackoverflow.com/q/28027858/4288782\n        // This just means it is an opaque, implementation-specific format that the device\n        // GPU prefers. So as long as we use the GPU to draw, the format will match what\n        // the encoder expects.\n        format.setInteger(MediaFormat.KEY_COLOR_FORMAT,\n                MediaCodecInfo.CodecCapabilities.COLOR_FormatSurface);\n        format.setInteger(MediaFormat.KEY_BIT_RATE, mConfig.bitRate);\n        format.setInteger(MediaFormat.KEY_FRAME_RATE, mConfig.frameRate);\n        format.setInteger(MediaFormat.KEY_I_FRAME_INTERVAL, 1); // seconds between key frames!\n        format.setInteger(\"rotation-degrees\", mConfig.rotation);\n\n        try {\n            if (mConfig.encoder != null) {\n                mMediaCodec = MediaCodec.createByCodecName(mConfig.encoder);\n            } else {\n                mMediaCodec = MediaCodec.createEncoderByType(mConfig.mimeType);\n            }\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n        mMediaCodec.configure(format, null, null, MediaCodec.CONFIGURE_FLAG_ENCODE);\n        mSurface = mMediaCodec.createInputSurface();\n        mMediaCodec.start();\n    }\n\n    @EncoderThread\n    @Override\n    protected void onStart() {\n        // Nothing to do here. Waiting for the first frame.\n        mFrameNumber = 0;\n    }\n\n    @EncoderThread\n    @Override\n    protected void onStop() {\n        LOG.i(\"onStop\", \"setting mFrameNumber to 1 and signaling the end of input stream.\");\n        mFrameNumber = -1;\n        // Signals the end of input stream. This is a Video only API, as in the normal case,\n        // we use input buffers to signal the end. In the video case, we don't have input buffers\n        // because we use an input surface instead.\n        mMediaCodec.signalEndOfInputStream();\n        drainOutput(true);\n    }\n\n    /**\n     * The first frame that we write MUST have the BUFFER_FLAG_SYNC_FRAME flag set.\n     * It sometimes doesn't because we might drop some frames in {@link #drainOutput(boolean)},\n     * basically if, at the time, the muxer was not started yet, due to Audio setup being slow.\n     *\n     * We can't add the BUFFER_FLAG_SYNC_FRAME flag to the first frame just because we'd like to.\n     * But we can drop frames until we get a sync one.\n     *\n     * @param pool the buffer pool\n     * @param buffer the buffer\n     */\n    @Override\n    protected void onWriteOutput(@NonNull OutputBufferPool pool, @NonNull OutputBuffer buffer) {\n        if (!mSyncFrameFound) {\n            LOG.w(\"onWriteOutput:\", \"sync frame not found yet. Checking.\");\n            int flag = MediaCodec.BUFFER_FLAG_SYNC_FRAME;\n            boolean hasFlag = (buffer.info.flags & flag) == flag;\n            if (hasFlag) {\n                LOG.w(\"onWriteOutput:\", \"SYNC FRAME FOUND!\");\n                mSyncFrameFound = true;\n                super.onWriteOutput(pool, buffer);\n            } else {\n                LOG.w(\"onWriteOutput:\", \"DROPPING FRAME and requesting a sync frame soon.\");\n                if (Build.VERSION.SDK_INT >= 19) {\n                    Bundle params = new Bundle();\n                    params.putInt(MediaCodec.PARAMETER_KEY_REQUEST_SYNC_FRAME, 0);\n                    mMediaCodec.setParameters(params);\n                }\n                pool.recycle(buffer);\n            }\n        } else {\n            super.onWriteOutput(pool, buffer);\n        }\n    }\n\n    @Override\n    protected int getEncodedBitRate() {\n        return mConfig.bitRate;\n    }\n\n    @SuppressWarnings(\"BooleanMethodIsAlwaysInverted\")\n    protected boolean shouldRenderFrame(long timestampUs) {\n        if (timestampUs == 0) return false; // grafika said so\n        if (mFrameNumber < 0) return false; // We were asked to stop.\n        if (hasReachedMaxLength()) return false; // We were not asked yet, but we'll be soon.\n        mFrameNumber++;\n        return true;\n    }\n", "target": "video media encoder"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/CameraEngine.java:NoOpExceptionHandler:2", "source": "\n        @Override\n        public void uncaughtException(@NonNull Thread thread, @NonNull Throwable throwable) {\n            LOG.w(\"EXCEPTION:\", \"In the CLASSTOKEN, probably while destroying.\",\n                    \"Thread:\", thread, \"Error:\", throwable);\n        }\n    ", "target": "no op exception handler"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/CameraUtils.java:CameraUtils:0", "source": "\n\n    private final static String TAG = CLASSTOKEN.class.getSimpleName();\n    private final static CameraLogger LOG = CameraLogger.create(TAG);\n\n    /**\n     * Determines whether the device has valid camera sensors, so the library\n     * can be used.\n     *\n     * @param context a valid Context\n     * @return whether device has cameras\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static boolean hasCameras(@NonNull Context context) {\n        PackageManager manager = context.getPackageManager();\n        // There's also FEATURE_CAMERA_EXTERNAL , should we support it?\n        return manager.hasSystemFeature(PackageManager.FEATURE_CAMERA)\n                || manager.hasSystemFeature(PackageManager.FEATURE_CAMERA_FRONT);\n    }\n\n\n    /**\n     * Determines whether the device has a valid camera sensor with the given\n     * Facing value, so that a session can be started.\n     *\n     * @param context a valid context\n     * @param facing either {@link Facing#BACK} or {@link Facing#FRONT}\n     * @return true if such sensor exists\n     */\n    public static boolean hasCameraFacing(@SuppressWarnings(\"unused\") @NonNull Context context,\n                                          @NonNull Facing facing) {\n        int internal = Camera1Mapper.get().mapFacing(facing);\n        Camera.CameraInfo cameraInfo = new Camera.CameraInfo();\n        for (int i = 0, count = Camera.getNumberOfCameras(); i < count; i++) {\n            Camera.getCameraInfo(i, cameraInfo);\n            if (cameraInfo.facing == internal) return true;\n        }\n        return false;\n    }\n\n\n    /**\n     * Simply writes the given data to the given file. It is done synchronously. If you are\n     * running on the UI thread, please use {@link #writeToFile(byte[], File, FileCallback)}\n     * and pass a file callback.\n     *\n     * If any error is encountered, this returns null.\n     *\n     * @param data the data to be written\n     * @param file the file to write into\n     * @return the source file, or null if error\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    @Nullable\n    @WorkerThread\n    @SuppressLint(\"NewApi\")\n    public static File writeToFile(@NonNull final byte[] data, @NonNull File file) {\n        if (file.exists() && !file.delete()) return null;\n        try (OutputStream stream = new BufferedOutputStream(new FileOutputStream(file))) {\n            stream.write(data);\n            stream.flush();\n            return file;\n        } catch (IOException e) {\n            LOG.e(\"writeToFile:\", \"could not write file.\", e);\n            return null;\n        }\n    }\n\n\n    /**\n     * Writes the given data to the given file in a background thread, returning on the\n     * original thread (typically the UI thread) once writing is done.\n     * If some error is encountered, the {@link FileCallback} will return null instead of the\n     * original file.\n     *\n     * @param data the data to be written\n     * @param file the file to write into\n     * @param callback a callback\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static void writeToFile(@NonNull final byte[] data,\n                                   @NonNull final File file,\n                                   @NonNull final FileCallback callback) {\n        final Handler ui = new Handler();\n        WorkerHandler.execute(new Runnable() {\n            @Override\n            public void run() {\n                final File result = writeToFile(data, file);\n                ui.post(new Runnable() {\n                    @Override\n                    public void run() {\n                        callback.onFileReady(result);\n                    }\n                });\n            }\n        });\n    }\n\n    /**\n     * Decodes an input byte array and outputs a Bitmap that is ready to be displayed.\n     * The difference with {@link android.graphics.BitmapFactory#decodeByteArray(byte[], int, int)}\n     * is that this cares about orientation, reading it from the EXIF header.\n     *\n     * @param source a JPEG byte array\n     * @return decoded bitmap or null if error is encountered\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    @Nullable\n    @WorkerThread\n    public static Bitmap decodeBitmap(@NonNull final byte[] source) {\n        return decodeBitmap(source, Integer.MAX_VALUE, Integer.MAX_VALUE);\n    }\n\n    /**\n     * Decodes an input byte array and outputs a Bitmap that is ready to be displayed.\n     * The difference with {@link android.graphics.BitmapFactory#decodeByteArray(byte[], int, int)}\n     * is that this cares about orientation, reading it from the EXIF header.\n     * This is executed in a background thread, and returns the result to the original thread.\n     *\n     * @param source a JPEG byte array\n     * @param callback a callback to be notified\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static void decodeBitmap(@NonNull final byte[] source,\n                                    @NonNull final BitmapCallback callback) {\n        decodeBitmap(source, Integer.MAX_VALUE, Integer.MAX_VALUE, callback);\n    }\n\n    /**\n     * Decodes an input byte array and outputs a Bitmap that is ready to be displayed.\n     * The difference with {@link android.graphics.BitmapFactory#decodeByteArray(byte[], int, int)}\n     * is that this cares about orientation, reading it from the EXIF header.\n     * This is executed in a background thread, and returns the result to the original thread.\n     *\n     * The image is also downscaled taking care of the maxWidth and maxHeight arguments.\n     *\n     * @param source a JPEG byte array\n     * @param maxWidth the max allowed width\n     * @param maxHeight the max allowed height\n     * @param callback a callback to be notified\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static void decodeBitmap(@NonNull final byte[] source,\n                                    final int maxWidth,\n                                    final int maxHeight,\n                                    @NonNull final BitmapCallback callback) {\n        decodeBitmap(source, maxWidth, maxHeight, new BitmapFactory.Options(), callback);\n    }\n\n    /**\n     * Decodes an input byte array and outputs a Bitmap that is ready to be displayed.\n     * The difference with {@link android.graphics.BitmapFactory#decodeByteArray(byte[], int, int)}\n     * is that this cares about orientation, reading it from the EXIF header.\n     * This is executed in a background thread, and returns the result to the original thread.\n     *\n     * The image is also downscaled taking care of the maxWidth and maxHeight arguments.\n     *\n     * @param source a JPEG byte array\n     * @param maxWidth the max allowed width\n     * @param maxHeight the max allowed height\n     * @param options the options to be passed to decodeByteArray\n     * @param callback a callback to be notified\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public static void decodeBitmap(@NonNull final byte[] source,\n                                    final int maxWidth,\n                                    final int maxHeight,\n                                    @NonNull final BitmapFactory.Options options,\n                                    @NonNull final BitmapCallback callback) {\n        decodeBitmap(source, maxWidth, maxHeight, options, -1, callback);\n    }\n\n    static void decodeBitmap(@NonNull final byte[] source,\n                             final int maxWidth,\n                             final int maxHeight,\n                             @NonNull final BitmapFactory.Options options,\n                             final int rotation,\n                             @NonNull final BitmapCallback callback) {\n        final Handler ui = new Handler();\n        WorkerHandler.execute(new Runnable() {\n            @Override\n            public void run() {\n                final Bitmap bitmap = decodeBitmap(source, maxWidth, maxHeight, options, rotation);\n                ui.post(new Runnable() {\n                    @Override\n                    public void run() {\n                        callback.onBitmapReady(bitmap);\n                    }\n                });\n            }\n        });\n    }\n\n    /**\n     * Decodes an input byte array and outputs a Bitmap that is ready to be displayed.\n     * The difference with {@link android.graphics.BitmapFactory#decodeByteArray(byte[], int, int)}\n     * is that this cares about orientation, reading it from the EXIF header.\n     *\n     * The image is also downscaled taking care of the maxWidth and maxHeight arguments.\n     *\n     * @param source a JPEG byte array\n     * @param maxWidth the max allowed width\n     * @param maxHeight the max allowed height\n     * @return decoded bitmap or null if error is encountered\n     */\n    @SuppressWarnings(\"SameParameterValue\")\n    @Nullable\n    @WorkerThread\n    public static Bitmap decodeBitmap(@NonNull byte[] source, int maxWidth, int maxHeight) {\n        return decodeBitmap(source, maxWidth, maxHeight, new BitmapFactory.Options());\n    }\n\n    /**\n     * Decodes an input byte array and outputs a Bitmap that is ready to be displayed.\n     * The difference with {@link android.graphics.BitmapFactory#decodeByteArray(byte[], int, int)}\n     * is that this cares about orientation, reading it from the EXIF header.\n     *\n     * The image is also downscaled taking care of the maxWidth and maxHeight arguments.\n     *\n     * @param source a JPEG byte array\n     * @param maxWidth the max allowed width\n     * @param maxHeight the max allowed height\n     * @param options the options to be passed to decodeByteArray\n     * @return decoded bitmap or null if error is encountered\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    @Nullable\n    @WorkerThread\n    public static Bitmap decodeBitmap(@NonNull byte[] source,\n                                      int maxWidth,\n                                      int maxHeight,\n                                      @NonNull BitmapFactory.Options options) {\n        return decodeBitmap(source, maxWidth, maxHeight, options, -1);\n    }\n\n    // Null means we got OOM\n    // Ignores flipping, but it should be super rare.\n    @SuppressWarnings(\"TryFinallyCanBeTryWithResources\")\n    @Nullable\n    private static Bitmap decodeBitmap(@NonNull byte[] source,\n                                       int maxWidth,\n                                       int maxHeight,\n                                       @NonNull BitmapFactory.Options options,\n                                       int rotation) {\n        if (maxWidth <= 0) maxWidth = Integer.MAX_VALUE;\n        if (maxHeight <= 0) maxHeight = Integer.MAX_VALUE;\n        int orientation;\n        boolean flip;\n        if (rotation == -1) {\n            InputStream stream = null;\n            try {\n                // http://sylvana.net/jpegcrop/exif_orientation.html\n                stream = new ByteArrayInputStream(source);\n                ExifInterface exif = new ExifInterface(stream);\n                int exifOrientation = exif.getAttributeInt(ExifInterface.TAG_ORIENTATION,\n                        ExifInterface.ORIENTATION_NORMAL);\n                orientation = ExifHelper.getOrientation(exifOrientation);\n                flip = exifOrientation == ExifInterface.ORIENTATION_FLIP_HORIZONTAL ||\n                        exifOrientation == ExifInterface.ORIENTATION_FLIP_VERTICAL ||\n                        exifOrientation == ExifInterface.ORIENTATION_TRANSPOSE ||\n                        exifOrientation == ExifInterface.ORIENTATION_TRANSVERSE;\n                LOG.i(\"decodeBitmap:\", \"got orientation from EXIF.\", orientation);\n            } catch (IOException e) {\n                LOG.e(\"decodeBitmap:\", \"could not get orientation from EXIF.\", e);\n                orientation = 0;\n                flip = false;\n            } finally {\n                if (stream != null) {\n                    try {\n                        stream.close();\n                    } catch (Exception ignored) { }\n                }\n            }\n        } else {\n            orientation = rotation;\n            flip = false;\n            LOG.i(\"decodeBitmap:\", \"got orientation from constructor.\", orientation);\n        }\n\n        Bitmap bitmap;\n        try {\n            if (maxWidth < Integer.MAX_VALUE || maxHeight < Integer.MAX_VALUE) {\n                options.inJustDecodeBounds = true;\n                BitmapFactory.decodeByteArray(source, 0, source.length, options);\n\n                int outHeight = options.outHeight;\n                int outWidth = options.outWidth;\n                if (orientation % 180 != 0) {\n                    //noinspection SuspiciousNameCombination\n                    outHeight = options.outWidth;\n                    //noinspection SuspiciousNameCombination\n                    outWidth = options.outHeight;\n                }\n\n                options.inSampleSize = computeSampleSize(outWidth, outHeight, maxWidth, maxHeight);\n                options.inJustDecodeBounds = false;\n                bitmap = BitmapFactory.decodeByteArray(source, 0, source.length, options);\n            } else {\n                bitmap = BitmapFactory.decodeByteArray(source, 0, source.length);\n            }\n\n            if (orientation != 0 || flip) {\n                Matrix matrix = new Matrix();\n                matrix.setRotate(orientation);\n                Bitmap temp = bitmap;\n                bitmap = Bitmap.createBitmap(bitmap, 0, 0, bitmap.getWidth(),\n                        bitmap.getHeight(), matrix, true);\n                temp.recycle();\n            }\n        } catch (OutOfMemoryError e) {\n            bitmap = null;\n        }\n        return bitmap;\n    }\n\n    private static int computeSampleSize(int width, int height, int maxWidth, int maxHeight) {\n        // https://developer.android.com/topic/performance/graphics/load-bitmap.html\n        int inSampleSize = 1;\n        if (height > maxHeight || width > maxWidth) {\n            while ((height / inSampleSize) >= maxHeight\n                    || (width / inSampleSize) >= maxWidth) {\n                inSampleSize *= 2;\n            }\n        }\n        return inSampleSize;\n    }\n\n\n", "target": "camera utils"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/offset/Angles.java:Angles:0", "source": "\n\n    private final static String TAG = CLASSTOKEN.class.getSimpleName();\n    private final static CameraLogger LOG = CameraLogger.create(TAG);\n\n    private Facing mSensorFacing;\n    @VisibleForTesting int mSensorOffset = 0;\n    @VisibleForTesting int mDisplayOffset = 0;\n    @VisibleForTesting int mDeviceOrientation = 0;\n\n    /**\n     * We want to keep everything in the {@link Axis#ABSOLUTE} reference,\n     * so a front facing sensor offset must be inverted.\n     *\n     * @param sensorFacing sensor facing value\n     * @param sensorOffset sensor offset\n     */\n    public void setSensorOffset(@NonNull Facing sensorFacing, int sensorOffset) {\n        sanitizeInput(sensorOffset);\n        mSensorFacing = sensorFacing;\n        mSensorOffset = sensorOffset;\n        if (mSensorFacing == Facing.FRONT) {\n            mSensorOffset = sanitizeOutput(360 - mSensorOffset);\n        }\n        print();\n    }\n\n    /**\n     * Sets the display offset.\n     * @param displayOffset the display offset\n     */\n    public void setDisplayOffset(int displayOffset) {\n        sanitizeInput(displayOffset);\n        mDisplayOffset = displayOffset;\n        print();\n    }\n\n    /**\n     * Sets the device orientation.\n     * @param deviceOrientation the device orientation\n     */\n    public void setDeviceOrientation(int deviceOrientation) {\n        sanitizeInput(deviceOrientation);\n        mDeviceOrientation = deviceOrientation;\n        print();\n    }\n\n    private void print() {\n        LOG.i(\"CLASSTOKEN changed:\",\n                \"sensorOffset:\", mSensorOffset,\n                \"displayOffset:\", mDisplayOffset,\n                \"deviceOrientation:\", mDeviceOrientation);\n    }\n\n    /**\n     * Returns the offset between two reference systems, computed along the given axis.\n     * @param from the source reference system\n     * @param to the destination reference system\n     * @param axis the axis\n     * @return the offset\n     */\n    public int offset(@NonNull Reference from, @NonNull Reference to, @NonNull Axis axis) {\n        int offset = absoluteOffset(from, to);\n        if (axis == Axis.RELATIVE_TO_SENSOR) {\n            if (mSensorFacing == Facing.FRONT) {\n                offset = sanitizeOutput(360 - offset);\n            }\n        }\n        return offset;\n    }\n\n    private int absoluteOffset(@NonNull Reference from, @NonNull Reference to) {\n        if (from == to) {\n            return 0;\n        } else if (to == Reference.BASE) {\n            return sanitizeOutput(360 - absoluteOffset(to, from));\n        } else if (from == Reference.BASE) {\n            switch (to) {\n                case VIEW: return sanitizeOutput(360 - mDisplayOffset);\n                case OUTPUT: return sanitizeOutput(mDeviceOrientation);\n                case SENSOR: return sanitizeOutput(360 - mSensorOffset);\n                default: throw new RuntimeException(\"Unknown reference: \" + to);\n            }\n        } else {\n            return sanitizeOutput(\n                    absoluteOffset(Reference.BASE, to)\n                    - absoluteOffset(Reference.BASE, from));\n        }\n    }\n\n    /**\n     * Whether the two references systems are flipped.\n     * @param from source\n     * @param to destination\n     * @return true if flipped\n     */\n    public boolean flip(@NonNull Reference from, @NonNull Reference to) {\n        return offset(from, to, Axis.ABSOLUTE) % 180 != 0;\n    }\n\n    private void sanitizeInput(int value) {\n        if (value != 0\n                && value != 90\n                && value != 180\n                && value != 270) {\n            throw new IllegalStateException(\"This value is not sanitized: \" + value);\n        }\n    }\n\n    private int sanitizeOutput(int value) {\n        return (value + 360) % 360;\n    }\n", "target": "angles"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/metering/MeteringRegion.java:MeteringRegion:0", "source": "\n\n    final static int MAX_WEIGHT = 1000;\n\n    final RectF mRegion;\n    final int mWeight;\n\n    CLASSTOKEN(@NonNull RectF region, int weight) {\n        mRegion = region;\n        mWeight = weight;\n    }\n\n    @NonNull\n    CLASSTOKEN transform(@NonNull MeteringTransform transform) {\n        RectF result = new RectF(Float.MAX_VALUE, Float.MAX_VALUE,\n                -Float.MAX_VALUE, -Float.MAX_VALUE);\n        PointF point = new PointF();\n        // top-left\n        point.set(mRegion.left, mRegion.top);\n        point = transform.transformMeteringPoint(point);\n        updateRect(result, point);\n        // top-right\n        point.set(mRegion.right, mRegion.top);\n        point = transform.transformMeteringPoint(point);\n        updateRect(result, point);\n        // bottom-right\n        point.set(mRegion.right, mRegion.bottom);\n        point = transform.transformMeteringPoint(point);\n        updateRect(result, point);\n        // bottom-left\n        point.set(mRegion.left, mRegion.bottom);\n        point = transform.transformMeteringPoint(point);\n        updateRect(result, point);\n        return new CLASSTOKEN(result, mWeight);\n    }\n\n    private void updateRect(@NonNull RectF rect, @NonNull PointF point) {\n        rect.left = Math.min(rect.left, point.x);\n        rect.top = Math.min(rect.top, point.y);\n        rect.right = Math.max(rect.right, point.x);\n        rect.bottom = Math.max(rect.bottom, point.y);\n    }\n\n    @NonNull\n    CLASSTOKEN clip(@NonNull Size bounds) {\n        return clip(new RectF(0, 0, bounds.getWidth(), bounds.getHeight()));\n    }\n\n    @SuppressWarnings(\"WeakerAccess\")\n    @NonNull\n    CLASSTOKEN clip(@NonNull RectF bounds) {\n        RectF region = new RectF();\n        region.set(\n                Math.max(bounds.left, mRegion.left),\n                Math.max(bounds.top, mRegion.top),\n                Math.min(bounds.right, mRegion.right),\n                Math.min(bounds.bottom, mRegion.bottom)\n        );\n        return new CLASSTOKEN(region, mWeight);\n    }\n\n    @Override\n    public int compareTo(@NonNull CLASSTOKEN o) {\n        //noinspection UseCompareMethod\n        return -Integer.valueOf(mWeight).compareTo(o.mWeight);\n    }\n", "target": "metering region"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/overlay/OverlayLayout.java:LayoutParams:1", "source": "\n\n        @SuppressWarnings(\"unused\")\n        public boolean drawOnPreview = false;\n        public boolean drawOnPictureSnapshot = false;\n        public boolean drawOnVideoSnapshot = false;\n\n        public CLASSTOKEN(int width, int height) {\n            super(width, height);\n        }\n\n        public CLASSTOKEN(@NonNull Context context, @NonNull AttributeSet attrs) {\n            super(context, attrs);\n            TypedArray a = context.obtainStyledAttributes(attrs, R.styleable.CameraView_Layout);\n            try {\n                drawOnPreview = a.getBoolean(R.styleable.CameraView_Layout_layout_drawOnPreview,\n                        false);\n                drawOnPictureSnapshot = a.getBoolean(\n                        R.styleable.CameraView_Layout_layout_drawOnPictureSnapshot, false);\n                drawOnVideoSnapshot = a.getBoolean(\n                        R.styleable.CameraView_Layout_layout_drawOnVideoSnapshot, false);\n            } finally {\n                a.recycle();\n            }\n        }\n\n        @VisibleForTesting\n        boolean drawsOn(@NonNull Target target) {\n            return ((target == Target.PREVIEW && drawOnPreview)\n                    || (target == Target.VIDEO_SNAPSHOT && drawOnVideoSnapshot)\n                    || (target == Target.PICTURE_SNAPSHOT && drawOnPictureSnapshot));\n        }\n\n        @NonNull\n        @Override\n        public String toString() {\n            return getClass().getName() + \"[\"\n                    + \"drawOnPreview:\" + drawOnPreview\n                    + \",drawOnPictureSnapshot:\" + drawOnPictureSnapshot\n                    + \",drawOnVideoSnapshot:\" + drawOnVideoSnapshot\n                    + \"]\";\n        }\n    ", "target": "layout params"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/frame/ByteBufferFrameManager.java:ByteBufferFrameManager:0", "source": "\n\n    /**\n     * Receives callbacks on buffer availability\n     * (when a Frame is released, we reuse its buffer).\n     */\n    public interface BufferCallback {\n        void onBufferAvailable(@NonNull byte[] buffer);\n    }\n\n    /**\n     * In this mode, we have a {@link #mBufferCallback} and dispatch\n     * new buffers to the callback.\n     */\n    private final static int BUFFER_MODE_DISPATCH = 0;\n\n    /**\n     * In this mode, we have a {@link #mBufferQueue} where we store\n     * buffers and only dispatch when requested.\n     */\n    private final static int BUFFER_MODE_ENQUEUE = 1;\n\n    private LinkedBlockingQueue<byte[]> mBufferQueue;\n    private BufferCallback mBufferCallback;\n    private final int mBufferMode;\n\n    /**\n     * Construct a new frame manager.\n     * The construction must be followed by an {@link FrameManager#setUp(int, Size, Angles)} call\n     * as soon as the parameters are known.\n     *\n     * @param poolSize the size of the backing pool.\n     * @param callback a callback\n     */\n    public CLASSTOKEN(int poolSize, @Nullable BufferCallback callback) {\n        super(poolSize, byte[].class);\n        if (callback != null) {\n            mBufferCallback = callback;\n            mBufferMode = BUFFER_MODE_DISPATCH;\n        } else {\n            mBufferQueue = new LinkedBlockingQueue<>(poolSize);\n            mBufferMode = BUFFER_MODE_ENQUEUE;\n        }\n    }\n\n\n    @Override\n    public void setUp(int format, @NonNull Size size, @NonNull Angles angles) {\n        super.setUp(format, size, angles);\n        int bytes = getFrameBytes();\n        for (int i = 0; i < getPoolSize(); i++) {\n            if (mBufferMode == BUFFER_MODE_DISPATCH) {\n                mBufferCallback.onBufferAvailable(new byte[bytes]);\n            } else {\n                mBufferQueue.offer(new byte[bytes]);\n            }\n        }\n    }\n\n    /**\n     * Returns a new byte buffer than can be filled.\n     * This can only be called in {@link #BUFFER_MODE_ENQUEUE} mode! Where the frame\n     * manager also holds a queue of the byte buffers.\n     *\n     * If not null, the buffer returned by this method can be filled and used to get\n     * a new frame through {@link FrameManager#getFrame(Object, long)}.\n     *\n     * @return a buffer, or null\n     */\n    @Nullable\n    public byte[] getBuffer() {\n        if (mBufferMode != BUFFER_MODE_ENQUEUE) {\n            throw new IllegalStateException(\"Can't call getBuffer() \" +\n                    \"when not in BUFFER_MODE_ENQUEUE.\");\n        }\n        return mBufferQueue.poll();\n    }\n\n    /**\n     * Can be called if the buffer obtained by {@link #getBuffer()}\n     * was not used to construct a frame, so it can be put back into the queue.\n     * @param buffer a buffer\n     */\n    public void onBufferUnused(@NonNull byte[] buffer) {\n        if (mBufferMode != BUFFER_MODE_ENQUEUE) {\n            throw new IllegalStateException(\"Can't call onBufferUnused() \" +\n                    \"when not in BUFFER_MODE_ENQUEUE.\");\n        }\n\n        if (isSetUp()) {\n            mBufferQueue.offer(buffer);\n        } else {\n            LOG.w(\"onBufferUnused: buffer was returned but we're not set up anymore.\");\n        }\n    }\n\n    @Override\n    protected void onFrameDataReleased(@NonNull byte[] data, boolean recycled) {\n        if (recycled && data.length == getFrameBytes()) {\n            if (mBufferMode == BUFFER_MODE_DISPATCH) {\n                mBufferCallback.onBufferAvailable(data);\n            } else {\n                mBufferQueue.offer(data);\n            }\n        }\n    }\n\n    @NonNull\n    @Override\n    protected byte[] onCloneFrameData(@NonNull byte[] data) {\n        byte[] clone = new byte[data.length];\n        System.arraycopy(data, 0, clone, 0, data.length);\n        return clone;\n    }\n\n    /**\n     * Releases all frames controlled by this manager and\n     * clears the pool.\n     * In BUFFER_MODE_ENQUEUE, releases also all the buffers.\n     */\n    @Override\n    public void release() {\n        super.release();\n        if (mBufferMode == BUFFER_MODE_ENQUEUE) {\n            mBufferQueue.clear();\n        }\n    }\n", "target": "byte buffer frame manager"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/preview/CameraPreview.java:CameraPreview:0", "source": "\n\n    protected final static CameraLogger LOG\n            = CameraLogger.create(CLASSTOKEN.class.getSimpleName());\n\n    /**\n     * This is used to notify CameraEngine to recompute its camera Preview size.\n     * After that, CameraView will need a new layout pass to adapt to the Preview size.\n     */\n    public interface SurfaceCallback {\n\n        /**\n         * Called when the surface is available.\n         */\n        void onSurfaceAvailable();\n\n        /**\n         * Called when the surface has changed.\n         */\n        void onSurfaceChanged();\n\n        /**\n         * Called when the surface was destroyed.\n         */\n        void onSurfaceDestroyed();\n    }\n\n    protected interface CropCallback {\n        void onCrop();\n    }\n\n    @VisibleForTesting CropCallback mCropCallback;\n    private SurfaceCallback mSurfaceCallback;\n    private T mView;\n    @SuppressWarnings(\"WeakerAccess\")\n    protected boolean mCropping;\n\n    // These are the surface dimensions in REF_VIEW.\n    @SuppressWarnings(\"WeakerAccess\")\n    protected int mOutputSurfaceWidth;\n    @SuppressWarnings(\"WeakerAccess\")\n    protected int mOutputSurfaceHeight;\n\n    // These are the preview stream dimensions, in REF_VIEW.\n    @SuppressWarnings(\"WeakerAccess\")\n    protected int mInputStreamWidth;\n    @SuppressWarnings(\"WeakerAccess\")\n    protected int mInputStreamHeight;\n\n    // The rotation, if any, to be applied when drawing.\n    @SuppressWarnings(\"WeakerAccess\")\n    protected int mDrawRotation;\n\n    /**\n     * Creates a new preview.\n     * @param context a context\n     * @param parent where to inflate our view\n     */\n    public CLASSTOKEN(@NonNull Context context, @NonNull ViewGroup parent) {\n        mView = onCreateView(context, parent);\n    }\n\n    /**\n     * Sets a callback to be notified of surface events (creation, change, destruction)\n     * @param callback a callback\n     */\n    public void setSurfaceCallback(@Nullable SurfaceCallback callback) {\n        if (hasSurface() && mSurfaceCallback != null) {\n            mSurfaceCallback.onSurfaceDestroyed();\n        }\n        mSurfaceCallback = callback;\n        if (hasSurface() && mSurfaceCallback != null) {\n            mSurfaceCallback.onSurfaceAvailable();\n        }\n    }\n\n    /**\n     * Called at creation time. Implementors should inflate the hierarchy into the\n     * parent ViewGroup, and return the View that actually hosts the surface.\n     *\n     * @param context a context\n     * @param parent where to inflate\n     * @return the view hosting the Surface\n     */\n    @NonNull\n    protected abstract T onCreateView(@NonNull Context context, @NonNull ViewGroup parent);\n\n    /**\n     * Returns the view hosting the Surface.\n     * @return the view\n     */\n    @NonNull\n    public final T getView() {\n        return mView;\n    }\n\n    /**\n     * For testing purposes, should return the root view that was inflated into the\n     * parent during {@link #onCreateView(Context, ViewGroup)}.\n     * @return the root view\n     */\n    @SuppressWarnings(\"unused\")\n    @NonNull\n    public abstract View getRootView();\n\n    /**\n     * Returns the output surface object (for example a SurfaceHolder\n     * or a SurfaceTexture).\n     * @return the surface object\n     */\n    @NonNull\n    public abstract Output getOutput();\n\n    /**\n     * Returns the type of the output returned by {@link #getOutput()}.\n     * @return the output type\n     */\n    @NonNull\n    public abstract Class<Output> getOutputClass();\n\n    /**\n     * Called to notify the preview of the input stream size. The width and height must be\n     * rotated before calling this, if needed, to be consistent with the VIEW reference.\n     * @param width width of the preview stream, in view coordinates\n     * @param height height of the preview stream, in view coordinates\n     */\n    public void setStreamSize(int width, int height) {\n        LOG.i(\"setStreamSize:\", \"desiredW=\", width, \"desiredH=\", height);\n        mInputStreamWidth = width;\n        mInputStreamHeight = height;\n        if (mInputStreamWidth > 0 && mInputStreamHeight > 0) {\n            crop(mCropCallback);\n        }\n    }\n\n    /**\n     * Returns the current input stream size, in view coordinates.\n     * @return the current input stream size\n     */\n    @VisibleForTesting\n    @NonNull\n    final Size getStreamSize() {\n        return new Size(mInputStreamWidth, mInputStreamHeight);\n    }\n\n    /**\n     * Returns the current output surface size, in view coordinates.\n     * @return the current output surface size.\n     */\n    @NonNull\n    public final Size getSurfaceSize() {\n        return new Size(mOutputSurfaceWidth, mOutputSurfaceHeight);\n    }\n\n    /**\n     * Whether we have a valid surface already.\n     * @return whether we have a surface\n     */\n    public final boolean hasSurface() {\n        return mOutputSurfaceWidth > 0 && mOutputSurfaceHeight > 0;\n    }\n\n    /**\n     * Subclasses can call this to notify that the surface is available.\n     * @param width surface width\n     * @param height surface height\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    protected final void dispatchOnSurfaceAvailable(int width, int height) {\n        LOG.i(\"dispatchOnSurfaceAvailable:\", \"w=\", width, \"h=\", height);\n        mOutputSurfaceWidth = width;\n        mOutputSurfaceHeight = height;\n        if (mOutputSurfaceWidth > 0 && mOutputSurfaceHeight > 0) {\n            crop(mCropCallback);\n        }\n        if (mSurfaceCallback != null) {\n            mSurfaceCallback.onSurfaceAvailable();\n        }\n    }\n\n    /**\n     * Subclasses can call this to notify that the surface has changed.\n     * @param width surface width\n     * @param height surface height\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    protected final void dispatchOnSurfaceSizeChanged(int width, int height) {\n        LOG.i(\"dispatchOnSurfaceSizeChanged:\", \"w=\", width, \"h=\", height);\n        if (width != mOutputSurfaceWidth || height != mOutputSurfaceHeight) {\n            mOutputSurfaceWidth = width;\n            mOutputSurfaceHeight = height;\n            if (width > 0 && height > 0) {\n                crop(mCropCallback);\n            }\n            if (mSurfaceCallback != null) {\n                mSurfaceCallback.onSurfaceChanged();\n            }\n        }\n    }\n\n    /**\n     * Subclasses can call this to notify that the surface has been destroyed.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    protected final void dispatchOnSurfaceDestroyed() {\n        mOutputSurfaceWidth = 0;\n        mOutputSurfaceHeight = 0;\n        if (mSurfaceCallback != null) {\n            mSurfaceCallback.onSurfaceDestroyed();\n        }\n    }\n\n    /**\n     * Called by the hosting {@link com.otaliastudios.cameraview.CameraView},\n     * this is a lifecycle event.\n     */\n    public void onResume() {}\n\n    /**\n     * Called by the hosting {@link com.otaliastudios.cameraview.CameraView},\n     * this is a lifecycle event.\n     */\n    public void onPause() {}\n\n    /**\n     * Called by the hosting {@link com.otaliastudios.cameraview.CameraView},\n     * this is a lifecycle event.\n     */\n    @CallSuper\n    public void onDestroy() {\n        if (Thread.currentThread() == Looper.getMainLooper().getThread()) {\n            onDestroyView();\n        } else {\n            // Do this on the UI thread and wait.\n            Handler ui = new Handler(Looper.getMainLooper());\n            final TaskCompletionSource<Void> task = new TaskCompletionSource<>();\n            ui.post(new Runnable() {\n                @Override\n                public void run() {\n                    onDestroyView();\n                    task.setResult(null);\n                }\n            });\n            try { Tasks.await(task.getTask()); } catch (Exception ignore) {}\n        }\n    }\n\n    /**\n     * At this point we undo the work that was done during\n     * {@link #onCreateView(Context, ViewGroup)}, which basically means removing the root view\n     * from the hierarchy.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    @UiThread\n    protected void onDestroyView() {\n        View root = getRootView();\n        ViewParent parent = root.getParent();\n        if (parent instanceof ViewGroup) {\n            ((ViewGroup) parent).removeView(root);\n        }\n    }\n\n    /**\n     * Here we must crop the visible part by applying a scale greater than 1 to one of our\n     * dimensions. This way our internal aspect ratio (mOutputSurfaceWidth / mOutputSurfaceHeight)\n     * will match the preview size aspect ratio (mInputStreamWidth / mInputStreamHeight).\n     *\n     * There might still be some absolute difference (e.g. same ratio but bigger / smaller).\n     * However that should be already managed by the framework.\n     *\n     * @param callback the callback\n     */\n    protected void crop(@Nullable CropCallback callback) {\n        // The base implementation does not support cropping.\n        if (callback != null) callback.onCrop();\n    }\n\n    /**\n     * Whether this preview implementation supports cropping.\n     * The base implementation does not, but it is strongly recommended to do so.\n     * @return true if cropping is supported\n     */\n    public boolean supportsCropping() {\n        return false;\n    }\n\n    /**\n     * Whether we are currently cropping the output.\n     * If false, this means that the output image will match the visible bounds.\n     * @return true if cropping\n     */\n    public boolean isCropping() {\n        return mCropping;\n    }\n\n\n    /**\n     * Should be called after {@link #setStreamSize(int, int)}!\n     *\n     * Sets the rotation, if any, to be applied when drawing.\n     * Sometimes we don't need this:\n     * - In Camera1, the buffer producer sets our Surface size and rotates it based on the value\n     *   that we pass to {@link android.hardware.Camera.Parameters#setDisplayOrientation(int)},\n     *   so the stream that comes in is already rotated (if we apply SurfaceTexture transform).\n     * - In Camera2, for {@link android.view.SurfaceView} based previews, apparently it just works\n     *   out of the box. The producer might be doing something similar.\n     *\n     * But in all the other Camera2 cases, we need to apply this rotation when drawing the surface.\n     * Seems that Camera1 can correctly rotate the stream/transform to {@link Reference#VIEW},\n     * while Camera2, that does not have any rotation API, will only rotate to {@link Reference#BASE}.\n     * That's why in Camera2 this angle is set as the offset between BASE and VIEW.\n     *\n     * @param drawRotation the rotation in degrees\n     */\n    public void setDrawRotation(int drawRotation) {\n        mDrawRotation = drawRotation;\n    }\n", "target": "camera preview"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/Full2VideoRecorder.java:Full2VideoRecorder:0", "source": "\n\n    private ActionHolder mHolder;\n    private final String mCameraId;\n    private Surface mInputSurface;\n\n    public CLASSTOKEN(@NonNull Camera2Engine engine, @NonNull String cameraId) {\n        super(engine);\n        mHolder = engine;\n        mCameraId = cameraId;\n    }\n\n    @Override\n    protected void onStart() {\n        // Do not start now. Instead, wait for the first frame.\n        // Check that the request is the correct one, using the request tag.\n        // The engine might have been changing the request to add our surface lately,\n        // and we don't want to start on an old frame.\n        Action action = new BaseAction() {\n            @Override\n            public void onCaptureStarted(@NonNull ActionHolder holder, @NonNull CaptureRequest request) {\n                super.onCaptureStarted(holder, request);\n                Object tag = holder.getBuilder(this).build().getTag();\n                Object currentTag = request.getTag();\n                if (tag == null ? currentTag == null : tag.equals(currentTag)) {\n                    setState(STATE_COMPLETED);\n                }\n            }\n        };\n        action.addCallback(new CompletionCallback() {\n            @Override\n            protected void onActionCompleted(@NonNull Action action) {\n                CLASSTOKEN.super.onStart();\n            }\n        });\n        action.start(mHolder);\n    }\n\n    @Override\n    protected void applyVideoSource(@NonNull VideoResult.Stub stub,\n                                    @NonNull MediaRecorder mediaRecorder) {\n        mediaRecorder.setVideoSource(MediaRecorder.VideoSource.SURFACE);\n    }\n\n    @NonNull\n    @Override\n    protected CamcorderProfile getCamcorderProfile(@NonNull VideoResult.Stub stub) {\n        // This was an option: get the surface from outside this class, using\n        // MediaCodec.createPersistentInputSurface(). But it doesn't really help since the\n        // Camera2 engine refuses a surface that has not been configured, so even with that trick\n        // we would have to attach the surface to this recorder before creating the CameraSession.\n        // mediaRecorder.setInputSurface(mInputSurface);\n        Size size = stub.rotation % 180 != 0 ? stub.size.flip() : stub.size;\n        return CamcorderProfiles.get(mCameraId, size);\n    }\n\n    /**\n     * This method should be called just once.\n     *\n     * @param stub the video stub\n     * @return a surface\n     * @throws PrepareException if prepare went wrong\n     */\n    @NonNull\n    public Surface createInputSurface(@NonNull VideoResult.Stub stub) throws PrepareException {\n        if (!prepareMediaRecorder(stub)) {\n            throw new PrepareException(mError);\n        }\n        mInputSurface = mMediaRecorder.getSurface();\n        return mInputSurface;\n    }\n\n    @Nullable\n    public Surface getInputSurface() {\n        return mInputSurface;\n    }\n\n    public class PrepareException extends Exception {\n        private PrepareException(Throwable cause) {\n            super(cause);\n        }\n    }\n\n", "target": "full 2 video recorder"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/internal/ExifHelper.java:ExifHelper:0", "source": "\n\n    /**\n     * Maps an {@link ExifInterface} orientation value\n     * to the actual degrees.\n     */\n    public static int getOrientation(int exifOrientation) {\n        int orientation;\n        switch (exifOrientation) {\n            case ExifInterface.ORIENTATION_NORMAL:\n            case ExifInterface.ORIENTATION_FLIP_HORIZONTAL:\n                orientation = 0; break;\n\n            case ExifInterface.ORIENTATION_ROTATE_180:\n            case ExifInterface.ORIENTATION_FLIP_VERTICAL:\n                orientation = 180; break;\n\n            case ExifInterface.ORIENTATION_ROTATE_90:\n            case ExifInterface.ORIENTATION_TRANSPOSE:\n                orientation = 90; break;\n\n            case ExifInterface.ORIENTATION_ROTATE_270:\n            case ExifInterface.ORIENTATION_TRANSVERSE:\n                orientation = 270; break;\n\n            default: orientation = 0;\n        }\n        return orientation;\n    }\n\n    /**\n     * Maps a degree value to {@link ExifInterface} constant.\n     */\n    public static int getExifOrientation(int orientation) {\n        switch ((orientation + 360) % 360) {\n            case 0: return ExifInterface.ORIENTATION_NORMAL;\n            case 90: return ExifInterface.ORIENTATION_ROTATE_90;\n            case 180: return ExifInterface.ORIENTATION_ROTATE_180;\n            case 270: return ExifInterface.ORIENTATION_ROTATE_270;\n            default: throw new IllegalArgumentException(\"Invalid orientation: \" + orientation);\n        }\n    }\n", "target": "exif helper"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/SnapshotVideoRecorder.java:SnapshotVideoRecorder:0", "source": "\n\n    private static final String TAG = CLASSTOKEN.class.getSimpleName();\n    private static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    private static final int DEFAULT_VIDEO_FRAMERATE = 30;\n    private static final int DEFAULT_AUDIO_BITRATE = 64000;\n\n    // https://stackoverflow.com/a/5220554/4288782\n    // Assuming low motion, we don't want to put this too high for default usage,\n    // advanced users are still free to change this for each video.\n    private static int estimateVideoBitRate(@NonNull Size size, int frameRate) {\n        return (int) (0.07F * 1F * size.getWidth() * size.getHeight() * frameRate);\n    }\n\n    private static final int STATE_RECORDING = 0;\n    private static final int STATE_NOT_RECORDING = 1;\n\n    private MediaEncoderEngine mEncoderEngine;\n    private final Object mEncoderEngineLock = new Object();\n    private RendererCameraPreview mPreview;\n\n    private int mCurrentState = STATE_NOT_RECORDING;\n    private int mDesiredState = STATE_NOT_RECORDING;\n    private int mTextureId = 0;\n\n    private Overlay mOverlay;\n    private OverlayDrawer mOverlayDrawer;\n    private boolean mHasOverlay;\n\n    private Filter mCurrentFilter;\n\n    public CLASSTOKEN(@NonNull CameraEngine engine,\n                                 @NonNull RendererCameraPreview preview,\n                                 @Nullable Overlay overlay) {\n        super(engine);\n        mPreview = preview;\n        mOverlay = overlay;\n        mHasOverlay = overlay != null && overlay.drawsOn(Overlay.Target.VIDEO_SNAPSHOT);\n    }\n\n    @Override\n    protected void onStart() {\n        mPreview.addRendererFrameCallback(this);\n        mDesiredState = STATE_RECORDING;\n        dispatchVideoRecordingStart();\n    }\n\n    // Can be called different threads\n    @Override\n    protected void onStop(boolean isCameraShutdown) {\n        if (isCameraShutdown) {\n            // The renderer callback might never be called. From my tests, it's not,\n            // so we can't wait for that callback to stop the encoder engine.\n            LOG.i(\"Stopping the encoder engine from isCameraShutdown.\");\n            mDesiredState = STATE_NOT_RECORDING;\n            mCurrentState = STATE_NOT_RECORDING;\n            synchronized (mEncoderEngineLock) {\n                if (mEncoderEngine != null) {\n                    mEncoderEngine.stop();\n                    mEncoderEngine = null;\n                }\n            }\n        } else {\n            mDesiredState = STATE_NOT_RECORDING;\n        }\n    }\n\n    @RendererThread\n    @Override\n    public void onRendererTextureCreated(int textureId) {\n        mTextureId = textureId;\n        if (mHasOverlay) {\n            mOverlayDrawer = new OverlayDrawer(mOverlay, mResult.size);\n        }\n    }\n\n    @RendererThread\n    @Override\n    public void onRendererFilterChanged(@NonNull Filter filter) {\n        mCurrentFilter = filter.copy();\n        mCurrentFilter.setSize(mResult.size.getWidth(), mResult.size.getHeight());\n        synchronized (mEncoderEngineLock) {\n            if (mEncoderEngine != null) {\n                mEncoderEngine.notify(TextureMediaEncoder.FILTER_EVENT, mCurrentFilter);\n            }\n        }\n    }\n\n    @RendererThread\n    @Override\n    public void onRendererFrame(@NonNull SurfaceTexture surfaceTexture, int rotation,\n                                float scaleX, float scaleY) {\n        if (mCurrentState == STATE_NOT_RECORDING && mDesiredState == STATE_RECORDING) {\n            LOG.i(\"Starting the encoder engine.\");\n\n            // Set default options\n            if (mResult.videoFrameRate <= 0) mResult.videoFrameRate = DEFAULT_VIDEO_FRAMERATE;\n            if (mResult.videoBitRate <= 0) mResult.videoBitRate\n                    = estimateVideoBitRate(mResult.size, mResult.videoFrameRate);\n            if (mResult.audioBitRate <= 0) mResult.audioBitRate = DEFAULT_AUDIO_BITRATE;\n\n            // Define mime types\n            String videoType = \"\";\n            switch (mResult.videoCodec) {\n                case H_263: videoType = \"video/3gpp\"; break; // MediaFormat.MIMETYPE_VIDEO_H263;\n                case H_264: videoType = \"video/avc\"; break; // MediaFormat.MIMETYPE_VIDEO_AVC:\n                case DEVICE_DEFAULT: videoType = \"video/avc\"; break;\n            }\n            String audioType = \"\";\n            switch (mResult.audioCodec) {\n                case AAC:\n                case HE_AAC:\n                case AAC_ELD: audioType = \"audio/mp4a-latm\"; break; // MediaFormat.MIMETYPE_AUDIO_AAC:\n                case DEVICE_DEFAULT: audioType = \"audio/mp4a-latm\"; break;\n\n            }\n            TextureConfig videoConfig = new TextureConfig();\n            AudioConfig audioConfig = new AudioConfig();\n\n            // See if we have audio\n            int audioChannels = 0;\n            if (mResult.audio == Audio.ON) {\n                audioChannels = audioConfig.channels;\n            } else if (mResult.audio == Audio.MONO) {\n                audioChannels = 1;\n            } else if (mResult.audio == Audio.STEREO) {\n                audioChannels = 2;\n            }\n            boolean hasAudio = audioChannels > 0;\n\n            // Check the availability of values\n            Size newVideoSize = null;\n            int newVideoBitRate = 0;\n            int newAudioBitRate = 0;\n            int newVideoFrameRate = 0;\n            int videoEncoderOffset = 0;\n            int audioEncoderOffset = 0;\n            boolean encodersFound = false;\n            DeviceEncoders deviceEncoders = null;\n            while (!encodersFound) {\n                LOG.i(\"Checking DeviceEncoders...\",\n                        \"videoOffset:\", videoEncoderOffset,\n                        \"audioOffset:\", audioEncoderOffset);\n                try {\n                    deviceEncoders = new DeviceEncoders(DeviceEncoders.MODE_RESPECT_ORDER,\n                            videoType, audioType, videoEncoderOffset, audioEncoderOffset);\n                } catch (RuntimeException e) {\n                    LOG.w(\"Could not respect encoders parameters.\",\n                            \"Going on again without checking encoders, possibly failing.\");\n                    newVideoSize = mResult.size;\n                    newVideoBitRate = mResult.videoBitRate;\n                    newVideoFrameRate = mResult.videoFrameRate;\n                    newAudioBitRate = mResult.audioBitRate;\n                    break;\n                }\n                deviceEncoders = new DeviceEncoders(DeviceEncoders.MODE_PREFER_HARDWARE,\n                        videoType, audioType, videoEncoderOffset, audioEncoderOffset);\n                try {\n                    newVideoSize = deviceEncoders.getSupportedVideoSize(mResult.size);\n                    newVideoBitRate = deviceEncoders.getSupportedVideoBitRate(mResult.videoBitRate);\n                    newVideoFrameRate = deviceEncoders.getSupportedVideoFrameRate(newVideoSize,\n                            mResult.videoFrameRate);\n                    deviceEncoders.tryConfigureVideo(videoType, newVideoSize, newVideoFrameRate,\n                            newVideoBitRate);\n                    if (hasAudio) {\n                        newAudioBitRate = deviceEncoders\n                                .getSupportedAudioBitRate(mResult.audioBitRate);\n                        deviceEncoders.tryConfigureAudio(audioType, newAudioBitRate,\n                                audioConfig.samplingFrequency, audioChannels);\n                    }\n                    encodersFound = true;\n                } catch (DeviceEncoders.VideoException videoException) {\n                    LOG.i(\"Got VideoException:\", videoException.getMessage());\n                    videoEncoderOffset++;\n                } catch (DeviceEncoders.AudioException audioException) {\n                    LOG.i(\"Got AudioException:\", audioException.getMessage());\n                    audioEncoderOffset++;\n                }\n            }\n            mResult.size = newVideoSize;\n            mResult.videoBitRate = newVideoBitRate;\n            mResult.audioBitRate = newAudioBitRate;\n            mResult.videoFrameRate = newVideoFrameRate;\n\n            // Video\n            videoConfig.width = mResult.size.getWidth();\n            videoConfig.height = mResult.size.getHeight();\n            videoConfig.bitRate = mResult.videoBitRate;\n            videoConfig.frameRate = mResult.videoFrameRate;\n            videoConfig.rotation = rotation + mResult.rotation;\n            videoConfig.mimeType = videoType;\n            videoConfig.encoder = deviceEncoders.getVideoEncoder();\n            videoConfig.textureId = mTextureId;\n            videoConfig.scaleX = scaleX;\n            videoConfig.scaleY = scaleY;\n            // Get egl context from the RendererThread, which is the one in which we have created\n            // the textureId and the overlayTextureId, managed by the GlSurfaceView.\n            // Next operations can then be performed on different threads using this handle.\n            videoConfig.eglContext = EGL14.eglGetCurrentContext();\n            if (mHasOverlay) {\n                videoConfig.overlayTarget = Overlay.Target.VIDEO_SNAPSHOT;\n                videoConfig.overlayDrawer = mOverlayDrawer;\n                videoConfig.overlayRotation = mResult.rotation;\n                // ^ no \"rotation\" here! Overlays are already in VIEW ref.\n            }\n            TextureMediaEncoder videoEncoder = new TextureMediaEncoder(videoConfig);\n\n            // Adjustment\n            mResult.rotation = 0; // We will rotate the result instead.\n            mCurrentFilter.setSize(mResult.size.getWidth(), mResult.size.getHeight());\n\n            // Audio\n            AudioMediaEncoder audioEncoder = null;\n            if (hasAudio) {\n                audioConfig.bitRate = mResult.audioBitRate;\n                audioConfig.channels = audioChannels;\n                audioConfig.encoder = deviceEncoders.getAudioEncoder();\n                audioEncoder = new AudioMediaEncoder(audioConfig);\n            }\n\n            // Engine\n            synchronized (mEncoderEngineLock) {\n                mEncoderEngine = new MediaEncoderEngine(mResult.file,\n                        videoEncoder,\n                        audioEncoder,\n                        mResult.maxDuration,\n                        mResult.maxSize,\n                        CLASSTOKEN.this);\n                mEncoderEngine.notify(TextureMediaEncoder.FILTER_EVENT, mCurrentFilter);\n                mEncoderEngine.start();\n            }\n            mCurrentState = STATE_RECORDING;\n        }\n\n        if (mCurrentState == STATE_RECORDING) {\n            LOG.i(\"scheduling frame.\");\n            synchronized (mEncoderEngineLock) {\n                if (mEncoderEngine != null) { // Can be null on teardown.\n                    LOG.i(\"dispatching frame.\");\n                    TextureMediaEncoder textureEncoder\n                            = (TextureMediaEncoder) mEncoderEngine.getVideoEncoder();\n                    TextureMediaEncoder.Frame frame = textureEncoder.acquireFrame();\n                    frame.timestampNanos = surfaceTexture.getTimestamp();\n                    // NOTE: this is an approximation but it seems to work:\n                    frame.timestampMillis = System.currentTimeMillis();\n                    surfaceTexture.getTransformMatrix(frame.transform);\n                    mEncoderEngine.notify(TextureMediaEncoder.FRAME_EVENT, frame);\n                }\n            }\n        }\n\n        if (mCurrentState == STATE_RECORDING && mDesiredState == STATE_NOT_RECORDING) {\n            LOG.i(\"Stopping the encoder engine.\");\n            mCurrentState = STATE_NOT_RECORDING;\n            synchronized (mEncoderEngineLock) {\n                if (mEncoderEngine != null) {\n                    mEncoderEngine.stop();\n                    mEncoderEngine = null;\n                }\n            }\n        }\n\n    }\n\n    @Override\n    public void onEncodingStart() {\n        // This would be the most correct place to call dispatchVideoRecordingStart. However,\n        // after this we'll post the call on the UI thread which can take some time. To compensate\n        // this, we call dispatchVideoRecordingStart() a bit earlier in this class (onStart()).\n    }\n\n    @Override\n    public void onEncodingStop() {\n        dispatchVideoRecordingEnd();\n    }\n\n    @EncoderThread\n    @Override\n    public void onEncodingEnd(int stopReason, @Nullable Exception e) {\n        // If something failed, undo the result, since this is the mechanism\n        // to notify Camera1Engine about this.\n        if (e != null) {\n            LOG.e(\"Error onEncodingEnd\", e);\n            mResult = null;\n            mError = e;\n        } else {\n            if (stopReason == MediaEncoderEngine.END_BY_MAX_DURATION) {\n                LOG.i(\"onEncodingEnd because of max duration.\");\n                mResult.endReason = VideoResult.REASON_MAX_DURATION_REACHED;\n            } else if (stopReason == MediaEncoderEngine.END_BY_MAX_SIZE) {\n                LOG.i(\"onEncodingEnd because of max size.\");\n                mResult.endReason = VideoResult.REASON_MAX_SIZE_REACHED;\n            } else {\n                LOG.i(\"onEncodingEnd because of user.\");\n            }\n        }\n        // Cleanup\n        mCurrentState = STATE_NOT_RECORDING;\n        mDesiredState = STATE_NOT_RECORDING;\n        mPreview.removeRendererFrameCallback(CLASSTOKEN.this);\n        mPreview = null;\n        if (mOverlayDrawer != null) {\n            mOverlayDrawer.release();\n            mOverlayDrawer = null;\n        }\n        synchronized (mEncoderEngineLock) {\n            mEncoderEngine = null;\n        }\n        dispatchResult();\n    }\n", "target": "snapshot video recorder"}
