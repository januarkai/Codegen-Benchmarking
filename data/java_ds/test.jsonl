{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filters/SepiaFilter.java:SepiaFilter:0", "source": "\n\n    private final static String FRAGMENT_SHADER = \"#extension GL_OES_EGL_image_external : require\\n\"\n            + \"precision mediump float;\\n\"\n            + \"uniform samplerExternalOES sTexture;\\n\"\n            + \"mat3 matrix;\\n\"\n            + \"varying vec2 \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\";\\n\"\n            + \"void main() {\\n\"\n            + \"  matrix[0][0]=\" + 805.0f / 2048.0f + \";\\n\"\n            + \"  matrix[0][1]=\" + 715.0f / 2048.0f + \";\\n\"\n            + \"  matrix[0][2]=\" + 557.0f / 2048.0f + \";\\n\"\n            + \"  matrix[1][0]=\" + 1575.0f / 2048.0f + \";\\n\"\n            + \"  matrix[1][1]=\" + 1405.0f / 2048.0f + \";\\n\"\n            + \"  matrix[1][2]=\" + 1097.0f / 2048.0f + \";\\n\"\n            + \"  matrix[2][0]=\" + 387.0f / 2048.0f + \";\\n\"\n            + \"  matrix[2][1]=\" + 344.0f / 2048.0f + \";\\n\"\n            + \"  matrix[2][2]=\" + 268.0f / 2048.0f + \";\\n\"\n            + \"  vec4 color = texture2D(sTexture, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\");\\n\"\n            + \"  vec3 new_color = min(matrix * color.rgb, 1.0);\\n\"\n            + \"  gl_FragColor = vec4(new_color.rgb, color.a);\\n\"\n            + \"}\\n\";\n\n    public CLASSTOKEN() { }\n\n    @NonNull\n    @Override\n    public String getFragmentShader() {\n        return FRAGMENT_SHADER;\n    }\n", "target": "sepia filter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/preview/GlCameraPreview.java:Renderer:1", "source": "\n\n        @RendererThread\n        @Override\n        public void onSurfaceCreated(GL10 gl, EGLConfig config) {\n            if (mCurrentFilter == null) {\n                mCurrentFilter = new NoFilter();\n            }\n            mOutputTextureDrawer = new GlTextureDrawer();\n            mOutputTextureDrawer.setFilter(mCurrentFilter);\n            final int textureId = mOutputTextureDrawer.getTexture().getId();\n            mInputSurfaceTexture = new SurfaceTexture(textureId);\n            getView().queueEvent(new Runnable() {\n                @Override\n                public void run() {\n                    for (RendererFrameCallback callback : mRendererFrameCallbacks) {\n                        callback.onRendererTextureCreated(textureId);\n                    }\n                }\n            });\n\n            // Since we are using GLSurfaceView.RENDERMODE_WHEN_DIRTY, we must notify\n            // the SurfaceView of dirtyness, so that it draws again. This is how it's done.\n            mInputSurfaceTexture.setOnFrameAvailableListener(new SurfaceTexture.OnFrameAvailableListener() {\n                @Override\n                public void onFrameAvailable(SurfaceTexture surfaceTexture) {\n                    getView().requestRender(); // requestRender is thread-safe.\n                }\n            });\n        }\n\n        @SuppressWarnings(\"WeakerAccess\")\n        @RendererThread\n        public void onSurfaceDestroyed() {\n            if (mInputSurfaceTexture != null) {\n                mInputSurfaceTexture.setOnFrameAvailableListener(null);\n                mInputSurfaceTexture.release();\n                mInputSurfaceTexture = null;\n            }\n            if (mOutputTextureDrawer != null) {\n                mOutputTextureDrawer.release();\n                mOutputTextureDrawer = null;\n            }\n        }\n\n        @RendererThread\n        @Override\n        public void onSurfaceChanged(GL10 gl, final int width, final int height) {\n            gl.glViewport(0, 0, width, height);\n            mCurrentFilter.setSize(width, height);\n            if (!mDispatched) {\n                dispatchOnSurfaceAvailable(width, height);\n                mDispatched = true;\n            } else if (width != mOutputSurfaceWidth || height != mOutputSurfaceHeight) {\n                dispatchOnSurfaceSizeChanged(width, height);\n            }\n        }\n\n        @RendererThread\n        @Override\n        public void onDrawFrame(GL10 gl) {\n            if (mInputSurfaceTexture == null) return;\n            if (mInputStreamWidth <= 0 || mInputStreamHeight <= 0) {\n                // Skip drawing. Camera was not opened.\n                return;\n            }\n\n            // Latch the latest frame. If there isn't anything new,\n            // we'll just re-use whatever was there before.\n            final float[] transform = mOutputTextureDrawer.getTextureTransform();\n            mInputSurfaceTexture.updateTexImage();\n            mInputSurfaceTexture.getTransformMatrix(transform);\n            // LOG.v(\"onDrawFrame:\", \"timestamp:\", mInputSurfaceTexture.getTimestamp());\n\n            // For Camera2, apply the draw rotation.\n            // See TextureCameraPreview.setDrawRotation() for info.\n            if (mDrawRotation != 0) {\n                Matrix.translateM(transform, 0, 0.5F, 0.5F, 0);\n                Matrix.rotateM(transform, 0, mDrawRotation, 0, 0, 1);\n                Matrix.translateM(transform, 0, -0.5F, -0.5F, 0);\n            }\n\n            if (isCropping()) {\n                // Scaling is easy, but we must also translate before:\n                // If the view is 10x1000 (very tall), it will show only the left strip\n                // of the preview (not the center one).\n                // If the view is 1000x10 (very large), it will show only the bottom strip\n                // of the preview (not the center one).\n                float translX = (1F - mCropScaleX) / 2F;\n                float translY = (1F - mCropScaleY) / 2F;\n                Matrix.translateM(transform, 0, translX, translY, 0);\n                Matrix.scaleM(transform, 0, mCropScaleX, mCropScaleY, 1);\n            }\n\n            mOutputTextureDrawer.draw(mInputSurfaceTexture.getTimestamp() / 1000L);\n            for (RendererFrameCallback callback : mRendererFrameCallbacks) {\n                callback.onRendererFrame(mInputSurfaceTexture, mDrawRotation, mCropScaleX, mCropScaleY);\n            }\n        }\n    ", "target": "renderer"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/encoding/TextureConfig.java:TextureConfig:0", "source": "\n\n    public int textureId;\n    public Overlay.Target overlayTarget;\n    public OverlayDrawer overlayDrawer;\n    public int overlayRotation;\n    public float scaleX;\n    public float scaleY;\n    public EGLContext eglContext;\n\n    @NonNull\n    CLASSTOKEN copy() {\n        CLASSTOKEN copy = new CLASSTOKEN();\n        copy(copy);\n        copy.textureId = this.textureId;\n        copy.overlayDrawer = this.overlayDrawer;\n        copy.overlayTarget = this.overlayTarget;\n        copy.overlayRotation = this.overlayRotation;\n        copy.scaleX = this.scaleX;\n        copy.scaleY = this.scaleY;\n        copy.eglContext = this.eglContext;\n        return copy;\n    }\n\n    boolean hasOverlay() {\n        return overlayDrawer != null;\n    }\n", "target": "texture config"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/encoding/MediaEncoderEngine.java:MediaEncoderEngine:0", "source": "\n\n    /**\n     * Receives the stop event callback to know when the video\n     * was written (or what went wrong).\n     */\n    public interface Listener {\n\n        /**\n         * Called when encoding started.\n         */\n        @EncoderThread\n        void onEncodingStart();\n\n        /**\n         * Called when encoding stopped. At this point the muxer or the encoders might still be\n         * processing data, but we have stopped receiving input (recording video and audio frames).\n         * Actually, we will stop very soon.\n         *\n         * The {@link #onEncodingEnd(int, Exception)} callback will soon be called\n         * with the results.\n         */\n        @EncoderThread\n        void onEncodingStop();\n\n        /**\n         * Called when encoding ended for some reason.\n         * If there's an exception, it failed.\n         * @param reason the reason\n         * @param e the error, if present\n         */\n        @EncoderThread\n        void onEncodingEnd(int reason, @Nullable Exception e);\n    }\n\n    private final static String TAG = CLASSTOKEN.class.getSimpleName();\n    private final static CameraLogger LOG = CameraLogger.create(TAG);\n    private static final boolean DEBUG_PERFORMANCE = true;\n\n    @SuppressWarnings(\"WeakerAccess\")\n    public final static int END_BY_USER = 0;\n    public final static int END_BY_MAX_DURATION = 1;\n    public final static int END_BY_MAX_SIZE = 2;\n\n    private final List<MediaEncoder> mEncoders = new ArrayList<>();\n    private MediaMuxer mMediaMuxer;\n    private int mStartedEncodersCount = 0;\n    private int mStoppedEncodersCount = 0;\n    private boolean mMediaMuxerStarted = false;\n    @SuppressWarnings(\"FieldCanBeLocal\")\n    private final Controller mController = new Controller();\n    private final WorkerHandler mControllerThread = WorkerHandler.get(\"EncoderEngine\");\n    private final Object mControllerLock = new Object();\n    private Listener mListener;\n    private int mEndReason = END_BY_USER;\n    private int mPossibleEndReason;\n\n    /**\n     * Creates a new engine for the given file, with the given encoders and max limits,\n     * and listener to receive events.\n     *\n     * @param file output file\n     * @param videoEncoder video encoder to use\n     * @param audioEncoder audio encoder to use\n     * @param maxDuration max duration in millis\n     * @param maxSize max size\n     * @param listener a listener\n     */\n    public CLASSTOKEN(@NonNull File file,\n                              @NonNull VideoMediaEncoder videoEncoder,\n                              @Nullable AudioMediaEncoder audioEncoder,\n                              final int maxDuration,\n                              final long maxSize,\n                              @Nullable Listener listener) {\n        mListener = listener;\n        mEncoders.add(videoEncoder);\n        if (audioEncoder != null) {\n            mEncoders.add(audioEncoder);\n        }\n        try {\n            mMediaMuxer = new MediaMuxer(file.toString(),\n                    MediaMuxer.OutputFormat.MUXER_OUTPUT_MPEG_4);\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        }\n\n        // Trying to convert the size constraints to duration constraints,\n        // because they are super easy to check.\n        // This is really naive & probably not accurate, but...\n        int bitRate = 0;\n        for (MediaEncoder encoder : mEncoders) {\n            bitRate += encoder.getEncodedBitRate();\n        }\n        int byteRate = bitRate / 8;\n        long sizeMaxDurationUs = (maxSize / byteRate) * 1000L * 1000L;\n        long maxDurationUs = maxDuration * 1000L;\n        long finalMaxDurationUs = Long.MAX_VALUE;\n        if (maxSize > 0 && maxDuration > 0) {\n            mPossibleEndReason = sizeMaxDurationUs < maxDurationUs ? END_BY_MAX_SIZE\n                    : END_BY_MAX_DURATION;\n            finalMaxDurationUs = Math.min(sizeMaxDurationUs, maxDurationUs);\n        } else if (maxSize > 0) {\n            mPossibleEndReason = END_BY_MAX_SIZE;\n            finalMaxDurationUs = sizeMaxDurationUs;\n        } else if (maxDuration > 0) {\n            mPossibleEndReason = END_BY_MAX_DURATION;\n            finalMaxDurationUs = maxDurationUs;\n        }\n        LOG.w(\"Computed a max duration of\", (finalMaxDurationUs / 1000000F));\n        for (MediaEncoder encoder : mEncoders) {\n            encoder.prepare(mController, finalMaxDurationUs);\n        }\n    }\n\n    /**\n     * Asks encoders to start (each one on its own track).\n     */\n    public final void start() {\n        LOG.i(\"Passing event to encoders:\", \"START\");\n        for (MediaEncoder encoder : mEncoders) {\n            encoder.start();\n        }\n    }\n\n    /**\n     * Notifies encoders of some event with the given payload.\n     * Can be used for example to notify the video encoder of new frame available.\n     * @param event an event string\n     * @param data an event payload\n     */\n    @SuppressWarnings(\"SameParameterValue\")\n    public final void notify(final String event, final Object data) {\n        LOG.v(\"Passing event to encoders:\", event);\n        for (MediaEncoder encoder : mEncoders) {\n            encoder.notify(event, data);\n        }\n    }\n\n    /**\n     * Asks encoders to stop. This is not sync, of course we will ask for encoders\n     * to call {@link Controller#notifyStopped(int)} before actually stop the muxer.\n     * When all encoders request a release, {@link #end()} is called to do cleanup\n     * and notify the listener.\n     */\n    public final void stop() {\n        LOG.i(\"Passing event to encoders:\", \"STOP\");\n        for (MediaEncoder encoder : mEncoders) {\n            encoder.stop();\n        }\n        if (mListener != null) {\n            mListener.onEncodingStop();\n        }\n    }\n\n    /**\n     * Called after all encoders have requested a release using\n     * {@link Controller#notifyStopped(int)}. At this point we will do cleanup and notify\n     * the listener.\n     */\n    private void end() {\n        LOG.i(\"end:\", \"Releasing muxer after all encoders have been released.\");\n        Exception error = null;\n        if (mMediaMuxer != null) {\n            // stop() throws an exception if you haven't fed it any data.\n            // But also in other occasions. So this is a signal that something\n            // went wrong, and we propagate that to the listener.\n            try {\n                mMediaMuxer.stop();\n            } catch (Exception e) {\n                error = e;\n            }\n            try {\n                mMediaMuxer.release();\n            } catch (Exception e) {\n                if (error == null) error = e;\n            }\n            mMediaMuxer = null;\n        }\n        LOG.w(\"end:\", \"Dispatching end to listener - reason:\", mEndReason, \"error:\", error);\n        if (mListener != null) {\n            mListener.onEncodingEnd(mEndReason, error);\n            mListener = null;\n        }\n        mEndReason = END_BY_USER;\n        mStartedEncodersCount = 0;\n        mStoppedEncodersCount = 0;\n        mMediaMuxerStarted = false;\n        mControllerThread.destroy();\n        LOG.i(\"end:\", \"Completed.\");\n    }\n\n    /**\n     * Returns the current video encoder.\n     * @return the current video encoder\n     */\n    @NonNull\n    public VideoMediaEncoder getVideoEncoder() {\n        return (VideoMediaEncoder) mEncoders.get(0);\n    }\n\n    /**\n     * Returns the current audio encoder.\n     * @return the current audio encoder\n     */\n    @SuppressWarnings(\"unused\")\n    @Nullable\n    public AudioMediaEncoder getAudioEncoder() {\n        if (mEncoders.size() > 1) {\n            return (AudioMediaEncoder) mEncoders.get(1);\n        } else {\n            return null;\n        }\n    }\n\n    /**\n     * A handle for {@link MediaEncoder}s to pass information to this engine.\n     * All methods here can be called for multiple threads.\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public class Controller {\n\n        /**\n         * Request that the muxer should start. This is not guaranteed to be executed:\n         * we wait for all encoders to call this method, and only then, start the muxer.\n         * @param format the media format\n         * @return the encoder track index\n         */\n        public int notifyStarted(@NonNull MediaFormat format) {\n            synchronized (mControllerLock) {\n                if (mMediaMuxerStarted) {\n                    throw new IllegalStateException(\"Trying to start but muxer started already\");\n                }\n                int track = mMediaMuxer.addTrack(format);\n                LOG.w(\"notifyStarted:\", \"Assigned track\", track, \"to format\",\n                        format.getString(MediaFormat.KEY_MIME));\n                if (++mStartedEncodersCount == mEncoders.size()) {\n                    LOG.w(\"notifyStarted:\", \"All encoders have started.\",\n                            \"Starting muxer and dispatching onEncodingStart().\");\n                    // Go out of this thread since it might be very important for the\n                    // encoders and we don't want to perform expensive operations here.\n                    mControllerThread.run(new Runnable() {\n                        @Override\n                        public void run() {\n                            mMediaMuxer.start();\n                            mMediaMuxerStarted = true;\n                            if (mListener != null) {\n                                mListener.onEncodingStart();\n                            }\n                        }\n                    });\n                }\n                return track;\n            }\n        }\n\n        /**\n         * Whether the muxer is started. MediaEncoders are required to avoid\n         * calling {@link #write(OutputBufferPool, OutputBuffer)} until this method returns true.\n         *\n         * @return true if muxer was started\n         */\n        public boolean isStarted() {\n            synchronized (mControllerLock) {\n                return mMediaMuxerStarted;\n            }\n        }\n\n        @SuppressLint(\"UseSparseArrays\")\n        private Map<Integer, Integer> mDebugCount = new HashMap<>();\n\n        /**\n         * Writes the given data to the muxer. Should be called after {@link #isStarted()}\n         * returns true. Note: this seems to be thread safe, no lock.\n         *\n         * TODO: Skip first frames from encoder A when encoder B reported a firstTimeMillis\n         * time that is significantly later. This can happen even if we wait for both to start,\n         * because {@link MediaEncoder#notifyFirstFrameMillis(long)} can be called while the\n         * muxer is still closed.\n         *\n         * The firstFrameMillis still has a value in computing the absolute times, but it is meant\n         * to be the time of the first frame read, not necessarily a frame that will be written.\n         *\n         * This controller should coordinate between firstFrameMillis and skip frames that have\n         * large differences.\n         *\n         * @param pool pool\n         * @param buffer buffer\n         */\n        public void write(@NonNull OutputBufferPool pool, @NonNull OutputBuffer buffer) {\n            if (DEBUG_PERFORMANCE) {\n                // When AUDIO = mono, this is called about twice the time. (200 vs 100 for 5 sec).\n                Integer count = mDebugCount.get(buffer.trackIndex);\n                mDebugCount.put(buffer.trackIndex, count == null ? 1 : ++count);\n                Calendar calendar = Calendar.getInstance();\n                calendar.setTimeInMillis(buffer.info.presentationTimeUs / 1000);\n                LOG.v(\"write:\", \"Writing into muxer -\",\n                                \"track:\", buffer.trackIndex,\n                        \"presentation:\", buffer.info.presentationTimeUs,\n                        \"readable:\", calendar.get(Calendar.SECOND) + \":\"\n                                + calendar.get(Calendar.MILLISECOND),\n                        \"count:\", count);\n            } else {\n                LOG.v(\"write:\", \"Writing into muxer -\",\n                        \"track:\", buffer.trackIndex,\n                        \"presentation:\", buffer.info.presentationTimeUs);\n            }\n            mMediaMuxer.writeSampleData(buffer.trackIndex, buffer.data, buffer.info);\n            pool.recycle(buffer);\n        }\n\n        /**\n         * Requests that the engine stops. This is not executed until all encoders call\n         * this method, so it is a kind of soft request, just like\n         * {@link #notifyStarted(MediaFormat)}. To be used when maxLength / maxSize constraints\n         * are reached, for example.\n         * When this succeeds, {@link MediaEncoder#stop()} is called.\n         *\n         * @param track track\n         */\n        public void requestStop(int track) {\n            synchronized (mControllerLock) {\n                LOG.w(\"requestStop:\", \"Called for track\", track);\n                if (--mStartedEncodersCount == 0) {\n                    LOG.w(\"requestStop:\", \"All encoders have requested a stop.\",\n                            \"Stopping them.\");\n                    mEndReason = mPossibleEndReason;\n                    // Go out of this thread since it might be very important for the\n                    // encoders and we don't want to perform expensive operations here.\n                    mControllerThread.run(new Runnable() {\n                        @Override\n                        public void run() {\n                            stop();\n                        }\n                    });\n                }\n            }\n        }\n\n        /**\n         * Notifies that the encoder was stopped. After this is called by all encoders,\n         * we will actually stop the muxer.\n         *\n         * @param track track\n         */\n        public void notifyStopped(int track) {\n            synchronized (mControllerLock) {\n                LOG.w(\"notifyStopped:\", \"Called for track\", track);\n                if (++mStoppedEncodersCount == mEncoders.size()) {\n                    LOG.w(\"requestStop:\", \"All encoders have been stopped.\",\n                            \"Stopping the muxer.\");\n                    // Go out of this thread since it might be very important for the\n                    // encoders and we don't want to perform expensive operations here.\n                    mControllerThread.run(new Runnable() {\n                        @Override\n                        public void run() {\n                            end();\n                        }\n                    });\n                }\n            }\n        }\n    }\n", "target": "media encoder engine"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/internal/Pool.java:Pool:0", "source": "\n\n    private static final String TAG = CLASSTOKEN.class.getSimpleName();\n    private static final CameraLogger LOG = CameraLogger.create(TAG);\n\n    private int maxPoolSize;\n    private int activeCount;\n    private LinkedBlockingQueue<T> queue;\n    private Factory<T> factory;\n    private final Object lock = new Object();\n\n    /**\n     * Used to create new instances of objects when needed.\n     * @param <T> object type\n     */\n    public interface Factory<T> {\n        T create();\n    }\n\n    /**\n     * Creates a new pool with the given pool size and factory.\n     * @param maxPoolSize the max pool size\n     * @param factory the factory\n     */\n    public CLASSTOKEN(int maxPoolSize, @NonNull Factory<T> factory) {\n        this.maxPoolSize = maxPoolSize;\n        this.queue = new LinkedBlockingQueue<>(maxPoolSize);\n        this.factory = factory;\n    }\n\n    /**\n     * Whether the pool is empty. This means that {@link #get()} will return\n     * a null item, because all objects were reclaimed and not recycled yet.\n     *\n     * @return whether the pool is empty\n     */\n    public boolean isEmpty() {\n        synchronized (lock) {\n            return count() >= maxPoolSize;\n        }\n    }\n\n    /**\n     * Returns a new item, from the recycled pool if possible (if there are recycled items),\n     * or instantiating one through the factory (if we can respect the pool size).\n     * If these conditions are not met, this returns null.\n     *\n     * @return an item or null\n     */\n    @Nullable\n    public T get() {\n        synchronized (lock) {\n            T item = queue.poll();\n            if (item != null) {\n                activeCount++; // poll decreases, this fixes\n                LOG.v(\"GET - Reusing recycled item.\", this);\n                return item;\n            }\n\n            if (isEmpty()) {\n                LOG.v(\"GET - Returning null. Too much items requested.\", this);\n                return null;\n            }\n\n            activeCount++;\n            LOG.v(\"GET - Creating a new item.\", this);\n            return factory.create();\n        }\n    }\n\n    /**\n     * Recycles an item after it has been used. The item should come from a previous\n     * {@link #get()} call.\n     *\n     * @param item used item\n     */\n    public void recycle(@NonNull T item) {\n        synchronized (lock) {\n            LOG.v(\"RECYCLE - Recycling item.\", this);\n            if (--activeCount < 0) {\n                throw new IllegalStateException(\"Trying to recycle an item which makes \" +\n                        \"activeCount < 0. This means that this or some previous items being \" +\n                        \"recycled were not coming from this pool, or some item was recycled \" +\n                        \"more than once. \" + this);\n            }\n            if (!queue.offer(item)) {\n                throw new IllegalStateException(\"Trying to recycle an item while the queue \" +\n                        \"is full. This means that this or some previous items being recycled \" +\n                        \"were not coming from this pool, or some item was recycled \" +\n                        \"more than once. \" + this);\n            }\n        }\n    }\n\n    /**\n     * Clears the pool of recycled items.\n     */\n    @CallSuper\n    public void clear() {\n        synchronized (lock) {\n            queue.clear();\n        }\n    }\n\n    /**\n     * Returns the count of all items managed by this pool. Includes\n     * - active items: currently being used\n     * - recycled items: used and recycled, available for second use\n     *\n     * @return count\n     */\n    public final int count() {\n        synchronized (lock) {\n            return activeCount() + recycledCount();\n        }\n    }\n\n    /**\n     * Returns the active items managed by this pools, which means, items\n     * currently being used.\n     *\n     * @return active count\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public final int activeCount() {\n        synchronized (lock) {\n            return activeCount;\n        }\n    }\n\n    /**\n     * Returns the recycled items managed by this pool, which means, items\n     * that were used and later recycled, and are currently available for\n     * second use.\n     *\n     * @return recycled count\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public final int recycledCount() {\n        synchronized (lock) {\n            return queue.size();\n        }\n    }\n\n    @NonNull\n    @Override\n    public String toString() {\n        return getClass().getSimpleName() + \" - count:\" + count() + \", active:\" + activeCount() + \", recycled:\" + recycledCount();\n    }\n", "target": "pool"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/PictureResult.java:Stub:1", "source": "\n\n        CLASSTOKEN() {}\n\n        public boolean isSnapshot;\n        public Location location;\n        public int rotation;\n        public Size size;\n        public Facing facing;\n        public byte[] data;\n        public PictureFormat format;\n    ", "target": "stub"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/VideoResult.java:VideoResult:0", "source": "\n\n    /**\n     * A result stub, for internal use only.\n     */\n    public static class Stub {\n\n        Stub() {}\n\n        public boolean isSnapshot;\n        public Location location;\n        public int rotation;\n        public Size size;\n        public File file;\n        public FileDescriptor fileDescriptor;\n        public Facing facing;\n        public VideoCodec videoCodec;\n        public AudioCodec audioCodec;\n        public Audio audio;\n        public long maxSize;\n        public int maxDuration;\n        public int endReason;\n        public int videoBitRate;\n        public int videoFrameRate;\n        public int audioBitRate;\n    }\n\n    @SuppressWarnings({\"WeakerAccess\", \"unused\"})\n    public static final int REASON_USER = 0;\n\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final int REASON_MAX_SIZE_REACHED = 1;\n\n    @SuppressWarnings(\"WeakerAccess\")\n    public static final int REASON_MAX_DURATION_REACHED = 2;\n\n    private final boolean isSnapshot;\n    private final Location location;\n    private final int rotation;\n    private final Size size;\n    private final File file;\n    private final FileDescriptor fileDescriptor;\n    private final Facing facing;\n    private final VideoCodec videoCodec;\n    private final AudioCodec audioCodec;\n    private final Audio audio;\n    private final long maxSize;\n    private final int maxDuration;\n    private final int endReason;\n    private final int videoBitRate;\n    private final int videoFrameRate;\n    private final int audioBitRate;\n\n    CLASSTOKEN(@NonNull Stub builder) {\n        isSnapshot = builder.isSnapshot;\n        location = builder.location;\n        rotation = builder.rotation;\n        size = builder.size;\n        file = builder.file;\n        fileDescriptor = builder.fileDescriptor;\n        facing = builder.facing;\n        videoCodec = builder.videoCodec;\n        audioCodec = builder.audioCodec;\n        audio = builder.audio;\n        maxSize = builder.maxSize;\n        maxDuration = builder.maxDuration;\n        endReason = builder.endReason;\n        videoBitRate = builder.videoBitRate;\n        videoFrameRate = builder.videoFrameRate;\n        audioBitRate = builder.audioBitRate;\n    }\n\n    /**\n     * Returns whether this result comes from a snapshot.\n     *\n     * @return whether this is a snapshot\n     */\n    public boolean isSnapshot() {\n        return isSnapshot;\n    }\n\n    /**\n     * Returns geographic information for this video, if any.\n     * If it was set, it is also present in the file metadata.\n     *\n     * @return a nullable Location\n     */\n    @Nullable\n    public Location getLocation() {\n        return location;\n    }\n\n    /**\n     * Returns the clock-wise rotation that should be applied to the\n     * video frames before displaying. If it is non-zero, it is also present\n     * in the video metadata, so most reader will take care of it.\n     *\n     * @return the clock-wise rotation\n     */\n    public int getRotation() {\n        return rotation;\n    }\n\n    /**\n     * Returns the size of the frames after the rotation is applied.\n     *\n     * @return the Size of this video\n     */\n    @NonNull\n    public Size getSize() {\n        return size;\n    }\n\n    /**\n     * Returns the file where the video was saved.\n     *\n     * @return the File of this video\n     */\n    @NonNull\n    public File getFile() {\n        if (file == null) {\n            throw new RuntimeException(\"File is only available when takeVideo(File) is used.\");\n        }\n        return file;\n    }\n\n    /**\n     * Returns the file descriptor where the video was saved.\n     *\n     * @return the File Descriptor of this video\n     */\n    @NonNull\n    public FileDescriptor getFileDescriptor() {\n        if (fileDescriptor == null) {\n            throw new RuntimeException(\"FileDescriptor is only available when takeVideo(FileDescriptor) is used.\");\n        }\n        return fileDescriptor;\n    }\n\n    /**\n     * Returns the facing value with which this video was recorded.\n     *\n     * @return the Facing of this video\n     */\n    @NonNull\n    public Facing getFacing() {\n        return facing;\n    }\n\n    /**\n     * Returns the codec that was used to encode the video frames.\n     *\n     * @return the video codec\n     */\n    @NonNull\n    public VideoCodec getVideoCodec() {\n        return videoCodec;\n    }\n\n    /**\n     * Returns the codec that was used to encode the audio frames.\n     *\n     * @return the audio codec\n     */\n    @NonNull\n    public AudioCodec getAudioCodec() {\n        return audioCodec;\n    }\n\n    /**\n     * Returns the max file size in bytes that was set before recording,\n     * or 0 if no constraint was set.\n     *\n     * @return the max file size in bytes\n     */\n    public long getMaxSize() {\n        return maxSize;\n    }\n\n    /**\n     * Returns the max video duration in milliseconds that was set before recording,\n     * or 0 if no constraint was set.\n     *\n     * @return the max duration in milliseconds\n     */\n    public int getMaxDuration() {\n        return maxDuration;\n    }\n\n    /**\n     * Returns the {@link Audio} setting for this video.\n     *\n     * @return the audio setting for this video\n     */\n    @NonNull\n    public Audio getAudio() {\n        return audio;\n    }\n\n    /**\n     * Returns the reason why the recording was stopped.\n     * @return one of {@link #REASON_USER}, {@link #REASON_MAX_DURATION_REACHED}\n     *         or {@link #REASON_MAX_SIZE_REACHED}.\n     */\n    public int getTerminationReason() {\n        return endReason;\n    }\n\n    /**\n     * Returns the bit rate used for video encoding.\n     *\n     * @return the video bit rate\n     */\n    public int getVideoBitRate() {\n        return videoBitRate;\n    }\n\n    /**\n     * Returns the frame rate used for video encoding\n     * in frames per second.\n     *\n     * @return the video frame rate\n     */\n    public int getVideoFrameRate() {\n        return videoFrameRate;\n    }\n\n    /**\n     * Returns the bit rate used for audio encoding.\n     *\n     * @return the audio bit rate\n     */\n    public int getAudioBitRate() {\n        return audioBitRate;\n    }\n", "target": "video result"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/engine/mappers/Camera2Mapper.java:Camera2Mapper:0", "source": "\n\n    private static CLASSTOKEN sInstance;\n\n    public static CLASSTOKEN get() {\n        if (sInstance == null) {\n            sInstance = new CLASSTOKEN();\n        }\n        return sInstance;\n    }\n\n    private static final Map<Facing, Integer> FACING = new HashMap<>();\n    private static final Map<WhiteBalance, Integer> WB = new HashMap<>();\n    private static final Map<Hdr, Integer> HDR = new HashMap<>();\n\n    static {\n        FACING.put(Facing.BACK, CameraCharacteristics.LENS_FACING_BACK);\n        FACING.put(Facing.FRONT, CameraCharacteristics.LENS_FACING_FRONT);\n        WB.put(WhiteBalance.AUTO, CameraCharacteristics.CONTROL_AWB_MODE_AUTO);\n        WB.put(WhiteBalance.CLOUDY, CameraCharacteristics.CONTROL_AWB_MODE_CLOUDY_DAYLIGHT);\n        WB.put(WhiteBalance.DAYLIGHT, CameraCharacteristics.CONTROL_AWB_MODE_DAYLIGHT);\n        WB.put(WhiteBalance.FLUORESCENT, CameraCharacteristics.CONTROL_AWB_MODE_FLUORESCENT);\n        WB.put(WhiteBalance.INCANDESCENT, CameraCharacteristics.CONTROL_AWB_MODE_INCANDESCENT);\n        HDR.put(Hdr.OFF, CameraCharacteristics.CONTROL_SCENE_MODE_DISABLED);\n        HDR.put(Hdr.ON, 18 /* CameraCharacteristics.CONTROL_SCENE_MODE_HDR */);\n    }\n\n    private CLASSTOKEN() {}\n\n    @NonNull\n    public List<Pair<Integer, Integer>> mapFlash(@NonNull Flash flash) {\n        List<Pair<Integer, Integer>> result = new ArrayList<>();\n        switch (flash) {\n            case ON: {\n                result.add(new Pair<>(\n                        CameraCharacteristics.CONTROL_AE_MODE_ON_ALWAYS_FLASH,\n                        CameraCharacteristics.FLASH_MODE_OFF));\n                break;\n            }\n            case AUTO: {\n                result.add(new Pair<>(\n                        CameraCharacteristics.CONTROL_AE_MODE_ON_AUTO_FLASH,\n                        CameraCharacteristics.FLASH_MODE_OFF));\n                result.add(new Pair<>(\n                        CameraCharacteristics.CONTROL_AE_MODE_ON_AUTO_FLASH_REDEYE,\n                        CameraCharacteristics.FLASH_MODE_OFF));\n                break;\n            }\n            case OFF: {\n                result.add(new Pair<>(\n                        CameraCharacteristics.CONTROL_AE_MODE_ON,\n                        CameraCharacteristics.FLASH_MODE_OFF));\n                result.add(new Pair<>(\n                        CameraCharacteristics.CONTROL_AE_MODE_OFF,\n                        CameraCharacteristics.FLASH_MODE_OFF));\n                break;\n            }\n            case TORCH: {\n                // When AE_MODE is ON or OFF, we can finally use the flash mode\n                // low level control to either turn flash off or open the torch\n                result.add(new Pair<>(\n                        CameraCharacteristics.CONTROL_AE_MODE_ON,\n                        CameraCharacteristics.FLASH_MODE_TORCH));\n                result.add(new Pair<>(\n                        CameraCharacteristics.CONTROL_AE_MODE_OFF,\n                        CameraCharacteristics.FLASH_MODE_TORCH));\n                break;\n            }\n        }\n        return  result;\n    }\n\n    public int mapFacing(@NonNull Facing facing) {\n        //noinspection ConstantConditions\n        return FACING.get(facing);\n    }\n\n    public int mapWhiteBalance(@NonNull WhiteBalance whiteBalance) {\n        //noinspection ConstantConditions\n        return WB.get(whiteBalance);\n    }\n\n    public int mapHdr(@NonNull Hdr hdr) {\n        //noinspection ConstantConditions\n        return HDR.get(hdr);\n    }\n\n    @NonNull\n    public Set<Flash> unmapFlash(int cameraConstant) {\n        Set<Flash> result = new HashSet<>();\n        switch (cameraConstant) {\n            case CameraCharacteristics.CONTROL_AE_MODE_OFF:\n            case CameraCharacteristics.CONTROL_AE_MODE_ON: {\n                result.add(Flash.OFF);\n                result.add(Flash.TORCH);\n                break;\n            }\n            case CameraCharacteristics.CONTROL_AE_MODE_ON_ALWAYS_FLASH: {\n                result.add(Flash.ON);\n                break;\n            }\n            case CameraCharacteristics.CONTROL_AE_MODE_ON_AUTO_FLASH:\n            case CameraCharacteristics.CONTROL_AE_MODE_ON_AUTO_FLASH_REDEYE: {\n                result.add(Flash.AUTO);\n                break;\n            }\n            case CameraCharacteristics.CONTROL_AE_MODE_ON_EXTERNAL_FLASH:\n            default: break; // we don't support external flash\n        }\n        return result;\n    }\n\n    @Nullable\n    public Facing unmapFacing(int cameraConstant) {\n        return reverseLookup(FACING, cameraConstant);\n    }\n\n    @Nullable\n    public WhiteBalance unmapWhiteBalance(int cameraConstant) {\n        return reverseLookup(WB, cameraConstant);\n    }\n\n    @Nullable\n    public Hdr unmapHdr(int cameraConstant) {\n        return reverseLookup(HDR, cameraConstant);\n    }\n\n    @Nullable\n    private <C extends Control, T> C reverseLookup(@NonNull Map<C, T> map, @NonNull T object) {\n        for (C value : map.keySet()) {\n            if (object.equals(map.get(value))) {\n                return value;\n            }\n        }\n        return null;\n    }\n", "target": "camera 2 mapper"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/internal/DeviceEncoders.java:AudioException:2", "source": "\n        private CLASSTOKEN(@NonNull String message) {\n            super(message);\n        }\n    ", "target": "audio exception"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filters/ContrastFilter.java:ContrastFilter:0", "source": "\n\n    private final static String FRAGMENT_SHADER = \"#extension GL_OES_EGL_image_external : require\\n\"\n            + \"precision mediump float;\\n\"\n            + \"uniform samplerExternalOES sTexture;\\n\"\n            + \"uniform float contrast;\\n\"\n            + \"varying vec2 \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\";\\n\"\n            + \"void main() {\\n\"\n            + \"  vec4 color = texture2D(sTexture, \"+DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME+\");\\n\"\n            + \"  color -= 0.5;\\n\"\n            + \"  color *= contrast;\\n\"\n            + \"  color += 0.5;\\n\"\n            + \"  gl_FragColor = color;\\n\"\n            + \"}\\n\";\n\n    private float contrast = 2F;\n    private int contrastLocation = -1;\n\n    public CLASSTOKEN() { }\n\n    /**\n     * Sets the current contrast adjustment.\n     * 1.0: no adjustment\n     * 2.0: increased contrast\n     *\n     * @param contrast contrast\n     */\n    @SuppressWarnings(\"WeakerAccess\")\n    public void setContrast(float contrast) {\n        if (contrast < 1.0f) contrast = 1.0f;\n        if (contrast > 2.0f) contrast = 2.0f;\n        this.contrast = contrast;\n    }\n\n    /**\n     * Returns the current contrast.\n     *\n     * @see #setContrast(float)\n     * @return contrast\n     */\n    @SuppressWarnings({\"unused\", \"WeakerAccess\"})\n    public float getContrast() {\n        return contrast;\n    }\n\n    @Override\n    public void setParameter1(float value) {\n        // parameter is 0...1, contrast is 1...2.\n        setContrast(value + 1);\n    }\n\n    @Override\n    public float getParameter1() {\n        // parameter is 0...1, contrast is 1...2.\n        return getContrast() - 1F;\n    }\n\n    @NonNull\n    @Override\n    public String getFragmentShader() {\n        return FRAGMENT_SHADER;\n    }\n\n    @Override\n    public void onCreate(int programHandle) {\n        super.onCreate(programHandle);\n        contrastLocation = GLES20.glGetUniformLocation(programHandle, \"contrast\");\n        Egloo.checkGlProgramLocation(contrastLocation, \"contrast\");\n    }\n\n    @Override\n    public void onDestroy() {\n        super.onDestroy();\n        contrastLocation = -1;\n    }\n\n    @Override\n    protected void onPreDraw(long timestampUs, @NonNull float[] transformMatrix) {\n        super.onPreDraw(timestampUs, transformMatrix);\n        GLES20.glUniform1f(contrastLocation, contrast);\n        Egloo.checkGlError(\"glUniform1f\");\n    }\n", "target": "contrast filter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/size/Size.java:Size:0", "source": "\n\n    private final int mWidth;\n    private final int mHeight;\n\n    public CLASSTOKEN(int width, int height) {\n        mWidth = width;\n        mHeight = height;\n    }\n\n    public int getWidth() {\n        return mWidth;\n    }\n\n    public int getHeight() {\n        return mHeight;\n    }\n\n    /**\n     * Returns a flipped size, with height equal to this size's width\n     * and width equal to this size's height.\n     *\n     * @return a flipped size\n     */\n    @SuppressWarnings(\"SuspiciousNameCombination\")\n    public CLASSTOKEN flip() {\n        return new CLASSTOKEN(mHeight, mWidth);\n    }\n\n    @Override\n    public boolean equals(Object o) {\n        if (o == null) {\n            return false;\n        }\n        if (this == o) {\n            return true;\n        }\n        if (o instanceof CLASSTOKEN) {\n            CLASSTOKEN size = (CLASSTOKEN) o;\n            return mWidth == size.mWidth && mHeight == size.mHeight;\n        }\n        return false;\n    }\n\n    @NonNull\n    @Override\n    public String toString() {\n        return mWidth + \"x\" + mHeight;\n    }\n\n    @Override\n    public int hashCode() {\n        return mHeight ^ ((mWidth << (Integer.SIZE / 2)) | (mWidth >>> (Integer.SIZE / 2)));\n    }\n\n    @Override\n    public int compareTo(@NonNull CLASSTOKEN another) {\n        return mWidth * mHeight - another.mWidth * another.mHeight;\n    }\n\n", "target": "size"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/filter/BaseFilter.java:BaseFilter:0", "source": "\n\n    private final static String TAG = CLASSTOKEN.class.getSimpleName();\n    private final static CameraLogger LOG = CameraLogger.create(TAG);\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected final static String DEFAULT_VERTEX_POSITION_NAME = \"aPosition\";\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected final static String DEFAULT_VERTEX_TEXTURE_COORDINATE_NAME = \"aTextureCoord\";\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected final static String DEFAULT_VERTEX_MVP_MATRIX_NAME = \"uMVPMatrix\";\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected final static String DEFAULT_VERTEX_TRANSFORM_MATRIX_NAME = \"uTexMatrix\";\n    protected final static String DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME = \"vTextureCoord\";\n\n    @NonNull\n    private static String createDefaultVertexShader(\n            @NonNull String vertexPositionName,\n            @NonNull String vertexTextureCoordinateName,\n            @NonNull String vertexModelViewProjectionMatrixName,\n            @NonNull String vertexTransformMatrixName,\n            @NonNull String fragmentTextureCoordinateName) {\n        return \"uniform mat4 \"+vertexModelViewProjectionMatrixName+\";\\n\"\n                + \"uniform mat4 \"+vertexTransformMatrixName+\";\\n\"\n                + \"attribute vec4 \"+vertexPositionName+\";\\n\"\n                + \"attribute vec4 \"+vertexTextureCoordinateName+\";\\n\"\n                + \"varying vec2 \"+fragmentTextureCoordinateName+\";\\n\"\n                + \"void main() {\\n\"\n                + \"    gl_Position = \" +vertexModelViewProjectionMatrixName+\" * \"\n                + vertexPositionName+\";\\n\"\n                + \"    \"+fragmentTextureCoordinateName+\" = (\"+vertexTransformMatrixName+\" * \"\n                + vertexTextureCoordinateName+\").xy;\\n\"\n                + \"}\\n\";\n    }\n\n    @NonNull\n    private static String createDefaultFragmentShader(\n            @NonNull String fragmentTextureCoordinateName) {\n        return \"#extension GL_OES_EGL_image_external : require\\n\"\n                + \"precision mediump float;\\n\"\n                + \"varying vec2 \"+fragmentTextureCoordinateName+\";\\n\"\n                + \"uniform samplerExternalOES sTexture;\\n\"\n                + \"void main() {\\n\"\n                + \"  gl_FragColor = texture2D(sTexture, \"+fragmentTextureCoordinateName+\");\\n\"\n                + \"}\\n\";\n    }\n\n    @VisibleForTesting GlTextureProgram program = null;\n    private GlDrawable programDrawable = null;\n    @VisibleForTesting Size size;\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected String vertexPositionName = DEFAULT_VERTEX_POSITION_NAME;\n    @SuppressWarnings(\"WeakerAccess\")\n    protected String vertexTextureCoordinateName = DEFAULT_VERTEX_TEXTURE_COORDINATE_NAME;\n    @SuppressWarnings(\"WeakerAccess\")\n    protected String vertexModelViewProjectionMatrixName = DEFAULT_VERTEX_MVP_MATRIX_NAME;\n    @SuppressWarnings(\"WeakerAccess\")\n    protected String vertexTransformMatrixName = DEFAULT_VERTEX_TRANSFORM_MATRIX_NAME;\n    @SuppressWarnings({\"unused\", \"WeakerAccess\"})\n    protected String fragmentTextureCoordinateName = DEFAULT_FRAGMENT_TEXTURE_COORDINATE_NAME;\n\n    @SuppressWarnings(\"WeakerAccess\")\n    @NonNull\n    protected String createDefaultVertexShader() {\n        return createDefaultVertexShader(vertexPositionName,\n                vertexTextureCoordinateName,\n                vertexModelViewProjectionMatrixName,\n                vertexTransformMatrixName,\n                fragmentTextureCoordinateName);\n    }\n\n    @SuppressWarnings(\"WeakerAccess\")\n    @NonNull\n    protected String createDefaultFragmentShader() {\n        return createDefaultFragmentShader(fragmentTextureCoordinateName);\n    }\n\n    @Override\n    public void onCreate(int programHandle) {\n        program = new GlTextureProgram(programHandle,\n                vertexPositionName,\n                vertexModelViewProjectionMatrixName,\n                vertexTextureCoordinateName,\n                vertexTransformMatrixName);\n        programDrawable = new GlRect();\n    }\n\n    @Override\n    public void onDestroy() {\n        // Since we used the handle constructor of GlTextureProgram, calling release here\n        // will NOT destroy the GL program. This is important because Filters are not supposed\n        // to have ownership of programs. Creation and deletion happen outside, and deleting twice\n        // would cause an error.\n        program.release();\n        program = null;\n        programDrawable = null;\n    }\n\n    @NonNull\n    @Override\n    public String getVertexShader() {\n        return createDefaultVertexShader();\n    }\n\n    @Override\n    public void setSize(int width, int height) {\n        size = new Size(width, height);\n    }\n\n    @Override\n    public void draw(long timestampUs, @NonNull float[] transformMatrix) {\n        if (program == null) {\n            LOG.w(\"Filter.draw() called after destroying the filter. \" +\n                    \"This can happen rarely because of threading.\");\n        } else {\n            onPreDraw(timestampUs, transformMatrix);\n            onDraw(timestampUs);\n            onPostDraw(timestampUs);\n        }\n    }\n\n    protected void onPreDraw(long timestampUs, @NonNull float[] transformMatrix) {\n        program.setTextureTransform(transformMatrix);\n        program.onPreDraw(programDrawable, programDrawable.getModelMatrix());\n    }\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected void onDraw(@SuppressWarnings(\"unused\") long timestampUs) {\n        program.onDraw(programDrawable);\n    }\n\n    @SuppressWarnings(\"WeakerAccess\")\n    protected void onPostDraw(@SuppressWarnings(\"unused\") long timestampUs) {\n        program.onPostDraw(programDrawable);\n    }\n\n    @NonNull\n    @Override\n    public final CLASSTOKEN copy() {\n        CLASSTOKEN copy = onCopy();\n        if (size != null) {\n            copy.setSize(size.getWidth(), size.getHeight());\n        }\n        if (this instanceof OneParameterFilter) {\n            ((OneParameterFilter) copy).setParameter1(((OneParameterFilter) this).getParameter1());\n        }\n        if (this instanceof TwoParameterFilter) {\n            ((TwoParameterFilter) copy).setParameter2(((TwoParameterFilter) this).getParameter2());\n        }\n        return copy;\n    }\n\n    @NonNull\n    protected CLASSTOKEN onCopy() {\n        try {\n            return getClass().newInstance();\n        } catch (IllegalAccessException e) {\n            throw new RuntimeException(\"Filters should have a public no-arguments constructor.\", e);\n        } catch (InstantiationException e) {\n            throw new RuntimeException(\"Filters should have a public no-arguments constructor.\", e);\n        }\n    }\n", "target": "base filter"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/encoding/InputBuffer.java:InputBuffer:0", "source": "\n    public ByteBuffer data;\n    public ByteBuffer source;\n    public int index;\n    public int length;\n    public long timestamp;\n    public boolean isEndOfStream;\n", "target": "input buffer"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/encoding/OutputBuffer.java:OutputBuffer:0", "source": "\n    public MediaCodec.BufferInfo info;\n    public int trackIndex;\n    public ByteBuffer data;\n", "target": "output buffer"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/video/encoding/AudioTimestamp.java:AudioTimestamp:0", "source": "\n\n    static long bytesToUs(long bytes, int byteRate) {\n        return (1000000L * bytes) / byteRate;\n    }\n\n    static long bytesToMillis(long bytes, int byteRate) {\n        return (1000L * bytes) / byteRate;\n    }\n\n    private int mByteRate;\n    private long mBaseTimeUs;\n    private long mBytesSinceBaseTime;\n    private long mGapUs;\n\n    CLASSTOKEN(int byteRate) {\n        mByteRate = byteRate;\n    }\n\n    /**\n     * This method accounts for the current time and proved to be the most reliable among\n     * the ones tested.\n     *\n     * This creates regular timestamps unless we accumulate a lot of delay (greater than\n     * twice the buffer duration), in which case it creates a gap and starts again trying\n     * to be regular from the new point.\n     *\n     * Returns timestamps in the {@link System#nanoTime()} reference.\n     */\n    @SuppressWarnings(\"SameParameterValue\")\n    long increaseUs(int readBytes) {\n        long bufferDurationUs = bytesToUs((long) readBytes, mByteRate);\n        long bufferEndTimeUs = System.nanoTime() / 1000; // now\n        long bufferStartTimeUs = bufferEndTimeUs - bufferDurationUs;\n\n        // If this is the first time, the base time is the buffer start time.\n        if (mBytesSinceBaseTime == 0) mBaseTimeUs = bufferStartTimeUs;\n\n        // Recompute time assuming that we are respecting the sampling frequency.\n        // This puts the time at the end of last read buffer, which means, where we\n        // should be if we had no delay / missed buffers.\n        long correctedTimeUs = mBaseTimeUs + bytesToUs(mBytesSinceBaseTime, mByteRate);\n        long correctionUs = bufferStartTimeUs - correctedTimeUs;\n\n        if (correctionUs >= 2L * bufferDurationUs) {\n            // However, if the correction is too big (> 2*bufferDurationUs), reset to this point.\n            // This is triggered if we lose buffers and are recording/encoding at a slower rate.\n            mBaseTimeUs = bufferStartTimeUs;\n            mBytesSinceBaseTime = readBytes;\n            mGapUs = correctionUs;\n            return mBaseTimeUs;\n        } else {\n            //noinspection StatementWithEmptyBody\n            if (correctionUs < 0) {\n                // This means that this method is being called too often, so that the expected start\n                // time for this buffer is BEFORE the last buffer end. So, respect the last buffer\n                // end instead.\n            }\n            mGapUs = 0;\n            mBytesSinceBaseTime += readBytes;\n            return correctedTimeUs;\n        }\n    }\n\n    /**\n     * Returns the number of gaps (meaning, missing frames) assuming that each\n     * frame has frameBytes size. Possibly 0.\n     *\n     * @param frameBytes size of standard frame\n     * @return number of gaps\n     */\n    int getGapCount(int frameBytes) {\n        if (mGapUs == 0) return 0;\n        long durationUs = bytesToUs((long) frameBytes, mByteRate);\n        return (int) (mGapUs / durationUs);\n    }\n\n    /**\n     * Returns the timestamp of the first missing frame.\n     * Should be called only after {@link #getGapCount(int)} returns something\n     * greater than zero.\n     *\n     * @param lastTimeUs the last real frame timestamp\n     * @return the first missing frame timestamp\n     */\n    long getGapStartUs(long lastTimeUs) {\n        return lastTimeUs - mGapUs;\n    }\n", "target": "audio timestamp"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/PictureResult.java:PictureResult:0", "source": "\n\n    /**\n     * A result stub, for internal use only.\n     */\n    public static class Stub {\n\n        Stub() {}\n\n        public boolean isSnapshot;\n        public Location location;\n        public int rotation;\n        public Size size;\n        public Facing facing;\n        public byte[] data;\n        public PictureFormat format;\n    }\n\n    private final boolean isSnapshot;\n    private final Location location;\n    private final int rotation;\n    private final Size size;\n    private final Facing facing;\n    private final byte[] data;\n    private final PictureFormat format;\n\n    CLASSTOKEN(@NonNull Stub builder) {\n        isSnapshot = builder.isSnapshot;\n        location = builder.location;\n        rotation = builder.rotation;\n        size = builder.size;\n        facing = builder.facing;\n        data = builder.data;\n        format = builder.format;\n    }\n\n    /**\n     * Returns whether this result comes from a snapshot.\n     *\n     * @return whether this is a snapshot\n     */\n    public boolean isSnapshot() {\n        return isSnapshot;\n    }\n\n    /**\n     * Returns geographic information for this picture, if any.\n     * If it was set, it is also present in the file metadata.\n     *\n     * @return a nullable Location\n     */\n    @Nullable\n    public Location getLocation() {\n        return location;\n    }\n\n    /**\n     * Returns the clock-wise rotation that should be applied to the\n     * picture before displaying. If it is non-zero, it is also present\n     * in the EXIF metadata.\n     *\n     * @return the clock-wise rotation\n     */\n    public int getRotation() {\n        return rotation;\n    }\n\n    /**\n     * Returns the size of the picture after the rotation is applied.\n     *\n     * @return the Size of this picture\n     */\n    @NonNull\n    public Size getSize() {\n        return size;\n    }\n\n    /**\n     * Returns the facing value with which this video was recorded.\n     *\n     * @return the Facing of this video\n     */\n    @NonNull\n    public Facing getFacing() {\n        return facing;\n    }\n\n    /**\n     * Returns the raw compressed, ready to be saved to file,\n     * in the given format.\n     *\n     * @return the compressed data stream\n     */\n    @NonNull\n    public byte[] getData() {\n        return data;\n    }\n\n    /**\n     * Returns the format for {@link #getData()}.\n     *\n     * @return the format\n     */\n    @NonNull\n    public PictureFormat getFormat() {\n        return format;\n    }\n\n    /**\n     * Shorthand for {@link CameraUtils#decodeBitmap(byte[], int, int, BitmapCallback)}.\n     * Decodes this picture on a background thread and posts the result in the UI thread using\n     * the given callback.\n     *\n     * @param maxWidth the max. width of final bitmap\n     * @param maxHeight the max. height of final bitmap\n     * @param callback a callback to be notified of image decoding\n     */\n    public void toBitmap(int maxWidth, int maxHeight, @NonNull BitmapCallback callback) {\n        if (format == PictureFormat.JPEG) {\n            CameraUtils.decodeBitmap(getData(), maxWidth, maxHeight, new BitmapFactory.Options(),\n                    rotation, callback);\n        } else if (format == PictureFormat.DNG && Build.VERSION.SDK_INT >= 24) {\n            // Apparently: BitmapFactory added DNG support in API 24.\n            // https://github.com/aosp-mirror/platform_frameworks_base/blob/nougat-mr1-release/core/jni/android/graphics/BitmapFactory.cpp\n            CameraUtils.decodeBitmap(getData(), maxWidth, maxHeight, new BitmapFactory.Options(),\n                    rotation, callback);\n        } else {\n            throw new UnsupportedOperationException(\"CLASSTOKEN.toBitmap() does not support \"\n                    + \"this picture format: \" + format);\n        }\n    }\n\n    /**\n     * Shorthand for {@link CameraUtils#decodeBitmap(byte[], BitmapCallback)}.\n     * Decodes this picture on a background thread and posts the result in the UI thread using\n     * the given callback.\n     *\n     * @param callback a callback to be notified of image decoding\n     */\n    public void toBitmap(@NonNull BitmapCallback callback) {\n        toBitmap(-1, -1, callback);\n    }\n\n    /**\n     * Shorthand for {@link CameraUtils#writeToFile(byte[], File, FileCallback)}.\n     * This writes this picture to file on a background thread and posts the result in the UI\n     * thread using the given callback.\n     *\n     * @param file the file to write into\n     * @param callback a callback\n     */\n    public void toFile(@NonNull File file, @NonNull FileCallback callback) {\n        CameraUtils.writeToFile(getData(), file, callback);\n    }\n", "target": "picture result"}
{"lang": "java", "dataset": "java_ds", "id": "java_ds:dataset/CameraView/cameraview/src/main/java/com/otaliastudios/cameraview/size/SizeSelectors.java:FilterSelector:1", "source": "\n\n        private Filter constraint;\n\n        private CLASSTOKEN(@NonNull Filter constraint) {\n            this.constraint = constraint;\n        }\n\n        @Override\n        @NonNull\n        public List<Size> select(@NonNull List<Size> source) {\n            List<Size> sizes = new ArrayList<>();\n            for (Size size : source) {\n                if (constraint.accepts(size)) {\n                    sizes.add(size);\n                }\n            }\n            return sizes;\n        }\n    ", "target": "filter selector"}
